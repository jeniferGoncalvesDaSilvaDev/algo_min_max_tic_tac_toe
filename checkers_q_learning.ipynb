{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBa49TvOWzy99ONhVbYRlx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeniferGoncalvesDaSilvaDev/algo_min_max_tic_tac_toe/blob/main/checkers_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNCPZjhE2gBN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b6bb7e3"
      },
      "source": [
        "# Task\n",
        "Develop an AI for playing checkers using Q-learning and Minimax with Alpha-Beta pruning, including environment setup, agent development, a heuristic evaluation function for Q-values, training, and performance evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7b700d7"
      },
      "source": [
        "## Configuração do Ambiente de Damas\n",
        "\n",
        "### Subtask:\n",
        "Implementar a lógica do jogo de damas, incluindo a representação do tabuleiro, regras de movimento das peças, detecção de jogadas válidas (capturas, movimentos), e identificação de estados terminais (vitória, derrota, empate).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46c1f56e"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to define a class to represent the checkers board, including its initialization with standard piece positions and how pieces are represented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ddf755d",
        "outputId": "fc0c3b87-c60f-41a4-c9d0-c0bac7c01310"
      },
      "source": [
        "class CheckersBoard:\n",
        "    def __init__(self):\n",
        "        # Initialize an 8x8 board\n",
        "        self.board = [[0 for _ in range(8)] for _ in range(8)]\n",
        "        # 0: empty, 1: player 1 piece, 2: player 2 piece\n",
        "        # 3: player 1 king, 4: player 2 king\n",
        "\n",
        "        # Setup initial pieces for Player 1 (bottom of the board)\n",
        "        for row in range(3):\n",
        "            for col in range(8):\n",
        "                if (row + col) % 2 != 0:  # Pieces are on dark squares\n",
        "                    self.board[row][col] = 1\n",
        "\n",
        "        # Setup initial pieces for Player 2 (top of the board)\n",
        "        for row in range(5, 8):\n",
        "            for col in range(8):\n",
        "                if (row + col) % 2 != 0:  # Pieces are on dark squares\n",
        "                    self.board[row][col] = 2\n",
        "\n",
        "    def __str__(self):\n",
        "        board_str = \"\"\n",
        "        for r_idx, row in enumerate(self.board):\n",
        "            board_str += f\"{8 - r_idx} |\"\n",
        "            for piece in row:\n",
        "                if piece == 0:\n",
        "                    board_str += \" . \"\n",
        "                elif piece == 1:\n",
        "                    board_str += \" x \"\n",
        "                elif piece == 2:\n",
        "                    board_str += \" o \"\n",
        "                elif piece == 3:\n",
        "                    board_str += \" X \"\n",
        "                elif piece == 4:\n",
        "                    board_str += \" O \"\n",
        "            board_str += \"\\n\"\n",
        "        board_str += \"  ------------------------\\n\"\n",
        "        board_str += \"    a  b  c  d  e  f  g  h\"\n",
        "        return board_str\n",
        "\n",
        "# Example usage:\n",
        "board = CheckersBoard()\n",
        "print(board)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  x  .  x  .  x  .  x \n",
            "5 | .  .  .  .  .  .  .  . \n",
            "4 | .  .  .  .  .  .  .  . \n",
            "3 | o  .  o  .  o  .  o  . \n",
            "2 | .  o  .  o  .  o  .  o \n",
            "1 | o  .  o  .  o  .  o  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b485caf"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully initialized and displayed the checkers board. The next logical step, as per the subtask instructions, is to implement the functionality to determine all valid moves for a given player, considering both basic moves and mandatory captures, including chaining captures. This will involve adding several helper methods to the `CheckersBoard` class to check square validity, piece ownership, and then generate possible moves and captures.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f6d87cf",
        "outputId": "1c12c044-81ed-4c3b-edf0-8080d30ee6fd"
      },
      "source": [
        "class CheckersBoard:\n",
        "    def __init__(self):\n",
        "        self.board = [[0 for _ in range(8)] for _ in range(8)]\n",
        "        # 0: empty, 1: player 1 piece, 2: player 2 piece\n",
        "        # 3: player 1 king, 4: player 2 king\n",
        "\n",
        "        # Setup initial pieces for Player 1 (bottom of the board)\n",
        "        for row in range(3):\n",
        "            for col in range(8):\n",
        "                if (row + col) % 2 != 0:\n",
        "                    self.board[row][col] = 1\n",
        "\n",
        "        # Setup initial pieces for Player 2 (top of the board)\n",
        "        for row in range(5, 8):\n",
        "            for col in range(8):\n",
        "                if (row + col) % 2 != 0:\n",
        "                    self.board[row][col] = 2\n",
        "\n",
        "    def __str__(self):\n",
        "        board_str = \"\"\n",
        "        for r_idx, row in enumerate(self.board):\n",
        "            board_str += f\"{8 - r_idx} |\"\n",
        "            for piece in row:\n",
        "                if piece == 0:\n",
        "                    board_str += \" . \"\n",
        "                elif piece == 1:\n",
        "                    board_str += \" x \"\n",
        "                elif piece == 2:\n",
        "                    board_str += \" o \"\n",
        "                elif piece == 3:\n",
        "                    board_str += \" X \"\n",
        "                elif piece == 4:\n",
        "                    board_str += \" O \"\n",
        "            board_str += \"\\n\"\n",
        "        board_str += \"  ------------------------\\n\"\n",
        "        board_str += \"    a  b  c  d  e  f  g  h\"\n",
        "        return board_str\n",
        "\n",
        "    def _is_valid_coord(self, r, c):\n",
        "        return 0 <= r < 8 and 0 <= c < 8\n",
        "\n",
        "    def _get_piece_owner(self, piece):\n",
        "        if piece == 1 or piece == 3:\n",
        "            return 1 # Player 1\n",
        "        elif piece == 2 or piece == 4:\n",
        "            return 2 # Player 2\n",
        "        return 0 # Empty or invalid\n",
        "\n",
        "    def _is_king(self, piece):\n",
        "        return piece == 3 or piece == 4\n",
        "\n",
        "    def _get_possible_moves_for_piece(self, r, c, player):\n",
        "        moves = []\n",
        "        piece = self.board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 moves up (decreasing row index)\n",
        "            directions = [(-1, -1), (-1, 1)]\n",
        "        else: # Player 2 moves down (increasing row index)\n",
        "            directions = [(1, -1), (1, 1)]\n",
        "\n",
        "        # Kings can move in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        for dr, dc in directions:\n",
        "            nr, nc = r + dr, c + dc\n",
        "            if self._is_valid_coord(nr, nc) and self.board[nr][nc] == 0:\n",
        "                moves.append(((r, c), (nr, nc)))\n",
        "        return moves\n",
        "\n",
        "    def _get_possible_captures_for_piece(self, r, c, player, current_board=None, path=None):\n",
        "        if current_board is None:\n",
        "            current_board = [row[:] for row in self.board]\n",
        "        if path is None:\n",
        "            path = [(r, c)]\n",
        "\n",
        "        captures = []\n",
        "        piece = current_board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 moves up (decreasing row index)\n",
        "            capture_directions = [(-1, -1), (-1, 1)]\n",
        "        else: # Player 2 moves down (increasing row index)\n",
        "            capture_directions = [(1, -1), (1, 1)]\n",
        "\n",
        "        # Kings can capture in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            capture_directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        found_capture_in_this_step = False\n",
        "        for dr, dc in capture_directions:\n",
        "            # Square of the opponent's piece to be jumped over\n",
        "            enemy_r, enemy_c = r + dr, c + dc\n",
        "            # Landing square after the jump\n",
        "            land_r, land_c = r + 2 * dr, c + 2 * dc\n",
        "\n",
        "            if (self._is_valid_coord(land_r, land_c) and\n",
        "                    current_board[land_r][land_c] == 0 and\n",
        "                    self._is_valid_coord(enemy_r, enemy_c) and\n",
        "                    self.board[enemy_r][enemy_c] != 0 and\n",
        "                    self._get_piece_owner(current_board[enemy_r][enemy_c]) != player):\n",
        "\n",
        "                found_capture_in_this_step = True\n",
        "                # Simulate the capture on a temporary board\n",
        "                temp_board = [row[:] for row in current_board]\n",
        "                temp_board[land_r][land_c] = temp_board[r][c]\n",
        "                temp_board[r][c] = 0\n",
        "                temp_board[enemy_r][enemy_c] = 0\n",
        "\n",
        "                # Check for king promotion after landing, only if not king yet and reaching back row\n",
        "                promoted_piece = temp_board[land_r][land_c]\n",
        "                if not self._is_king(promoted_piece):\n",
        "                    if (player == 1 and land_r == 0) or (player == 2 and land_r == 7):\n",
        "                        temp_board[land_r][land_c] += 2 # Promote to king\n",
        "\n",
        "                # Recursively look for more captures from the new position\n",
        "                chained_captures = self._get_possible_captures_for_piece(land_r, land_c, player, temp_board, path + [(land_r, land_c)])\n",
        "\n",
        "                if chained_captures: # If more captures are found, add them\n",
        "                    captures.extend(chained_captures)\n",
        "                else: # If no more captures, this is a terminal capture sequence\n",
        "                    captures.append(path + [(land_r, land_c)])\n",
        "\n",
        "        # If no capture was found in this step, but it's part of a longer path, return empty to signify end of chain\n",
        "        if not found_capture_in_this_step and len(path) > 1:\n",
        "            return []\n",
        "\n",
        "        return captures\n",
        "\n",
        "\n",
        "\n",
        "    def get_valid_moves(self, player):\n",
        "        all_possible_moves = []\n",
        "        all_possible_captures = []\n",
        "\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == player:\n",
        "                    # Get basic moves\n",
        "                    all_possible_moves.extend(self._get_possible_moves_for_piece(r, c, player))\n",
        "                    # Get captures\n",
        "                    captures_from_piece = self._get_possible_captures_for_piece(r, c, player)\n",
        "                    if captures_from_piece:\n",
        "                        all_possible_captures.extend(captures_from_piece)\n",
        "\n",
        "        if all_possible_captures:\n",
        "            # If captures are available, they are mandatory\n",
        "            # The `_get_possible_captures_for_piece` returns paths. We need to convert them to (start, end) format for consistency\n",
        "            # or adapt the rest of the code to handle paths.\n",
        "            # For now, let's return paths as they represent chained captures.\n",
        "            return all_possible_captures\n",
        "        else:\n",
        "            # Otherwise, return all basic moves\n",
        "            return all_possible_moves\n",
        "\n",
        "# Example usage:\n",
        "board = CheckersBoard()\n",
        "print(\"Initial Board:\")\n",
        "print(board)\n",
        "\n",
        "print(\"\\nValid moves for Player 1:\")\n",
        "moves_p1 = board.get_valid_moves(1)\n",
        "if moves_p1:\n",
        "    for move in moves_p1:\n",
        "        if isinstance(move[0], tuple): # If it's a move ((r1,c1), (r2,c2))\n",
        "            print(f\"  Move from {chr(move[0][1]+97)}{8-move[0][0]} to {chr(move[1][1]+97)}{8-move[1][0]}\")\n",
        "        else: # If it's a capture path [(r1,c1), (r2,c2), (r3,c3)...]\n",
        "            print(f\"  Capture path: {' -> '.join([f'{chr(pos[1]+97)}{8-pos[0]}' for pos in move])}\")\n",
        "else:\n",
        "    print(\"  No valid moves for Player 1.\")\n",
        "\n",
        "print(\"\\nValid moves for Player 2:\")\n",
        "moves_p2 = board.get_valid_moves(2)\n",
        "if moves_p2:\n",
        "    for move in moves_p2:\n",
        "        if isinstance(move[0], tuple): # If it's a move ((r1,c1), (r2,c2))\n",
        "            print(f\"  Move from {chr(move[0][1]+97)}{8-move[0][0]} to {chr(move[1][1]+97)}{8-move[1][0]}\")\n",
        "        else: # If it's a capture path [(r1,c1), (r2,c2), (r3,c3)...]\n",
        "            print(f\"  Capture path: {' -> '.join([f'{chr(pos[1]+97)}{8-pos[0]}' for pos in move])}\")\n",
        "else:\n",
        "    print(\"  No valid moves for Player 2.\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Board:\n",
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  x  .  x  .  x  .  x \n",
            "5 | .  .  .  .  .  .  .  . \n",
            "4 | .  .  .  .  .  .  .  . \n",
            "3 | o  .  o  .  o  .  o  . \n",
            "2 | .  o  .  o  .  o  .  o \n",
            "1 | o  .  o  .  o  .  o  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n",
            "\n",
            "Valid moves for Player 1:\n",
            "  No valid moves for Player 1.\n",
            "\n",
            "Valid moves for Player 2:\n",
            "  No valid moves for Player 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35183cc9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution indicated that no valid moves were found for either player on a fresh board, which is incorrect. This points to a misalignment between piece placement and movement directions, and potentially an issue in the capture logic's use of board states. This step corrects the movement directions for Player 1 and Player 2 to match their initial board positions (Player 1 at rows 0-2 moving 'down' and Player 2 at rows 5-7 moving 'up'), updates the king promotion conditions, and ensures capture logic uses the correct `current_board` for chained captures.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bed4f868",
        "outputId": "2ae931c6-6f11-44de-8dc3-30862a7d7956"
      },
      "source": [
        "class CheckersBoard:\n",
        "    def __init__(self):\n",
        "        self.board = [[0 for _ in range(8)] for _ in range(8)]\n",
        "        # 0: empty, 1: player 1 piece, 2: player 2 piece\n",
        "        # 3: player 1 king, 4: player 2 king\n",
        "\n",
        "        # Setup initial pieces for Player 1 (top of the board, rows 0-2)\n",
        "        for row in range(3):\n",
        "            for col in range(8):\n",
        "                if (row + col) % 2 != 0:\n",
        "                    self.board[row][col] = 1\n",
        "\n",
        "        # Setup initial pieces for Player 2 (bottom of the board, rows 5-7)\n",
        "        for row in range(5, 8):\n",
        "            for col in range(8):\n",
        "                if (row + col) % 2 != 0:\n",
        "                    self.board[row][col] = 2\n",
        "\n",
        "    def __str__(self):\n",
        "        board_str = \"\"\n",
        "        for r_idx, row in enumerate(self.board):\n",
        "            board_str += f\"{8 - r_idx} |\"\n",
        "            for piece in row:\n",
        "                if piece == 0:\n",
        "                    board_str += \" . \"\n",
        "                elif piece == 1:\n",
        "                    board_str += \" x \"\n",
        "                elif piece == 2:\n",
        "                    board_str += \" o \"\n",
        "                elif piece == 3:\n",
        "                    board_str += \" X \"\n",
        "                elif piece == 4:\n",
        "                    board_str += \" O \"\n",
        "            board_str += \"\\n\"\n",
        "        board_str += \"  ------------------------\\n\"\n",
        "        board_str += \"    a  b  c  d  e  f  g  h\"\n",
        "        return board_str\n",
        "\n",
        "    def _is_valid_coord(self, r, c):\n",
        "        return 0 <= r < 8 and 0 <= c < 8\n",
        "\n",
        "    def _get_piece_owner(self, piece):\n",
        "        if piece == 1 or piece == 3:\n",
        "            return 1 # Player 1\n",
        "        elif piece == 2 or piece == 4:\n",
        "            return 2 # Player 2\n",
        "        return 0 # Empty or invalid\n",
        "\n",
        "    def _is_king(self, piece):\n",
        "        return piece == 3 or piece == 4\n",
        "\n",
        "    def _get_possible_moves_for_piece(self, r, c, player):\n",
        "        moves = []\n",
        "        piece = self.board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can move in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        for dr, dc in directions:\n",
        "            nr, nc = r + dr, c + dc\n",
        "            if self._is_valid_coord(nr, nc) and self.board[nr][nc] == 0:\n",
        "                moves.append(((r, c), (nr, nc)))\n",
        "        return moves\n",
        "\n",
        "    def _get_possible_captures_for_piece(self, r, c, player, current_board=None, path=None):\n",
        "        if current_board is None:\n",
        "            current_board = [row[:] for row in self.board]\n",
        "        if path is None:\n",
        "            path = [(r, c)]\n",
        "\n",
        "        captures = []\n",
        "        piece = current_board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            capture_directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            capture_directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can capture in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            capture_directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        found_capture_in_this_step = False\n",
        "        for dr, dc in capture_directions:\n",
        "            # Square of the opponent's piece to be jumped over\n",
        "            enemy_r, enemy_c = r + dr, c + dc\n",
        "            # Landing square after the jump\n",
        "            land_r, land_c = r + 2 * dr, c + 2 * dc\n",
        "\n",
        "            if (self._is_valid_coord(land_r, land_c) and\n",
        "                    current_board[land_r][land_c] == 0 and\n",
        "                    self._is_valid_coord(enemy_r, enemy_c) and\n",
        "                    current_board[enemy_r][enemy_c] != 0 and # Changed from self.board to current_board\n",
        "                    self._get_piece_owner(current_board[enemy_r][enemy_c]) != player):\n",
        "\n",
        "                found_capture_in_this_step = True\n",
        "                # Simulate the capture on a temporary board\n",
        "                temp_board = [row[:] for row in current_board]\n",
        "                temp_board[land_r][land_c] = temp_board[r][c]\n",
        "                temp_board[r][c] = 0\n",
        "                temp_board[enemy_r][enemy_c] = 0\n",
        "\n",
        "                # Check for king promotion after landing, only if not king yet and reaching back row\n",
        "                promoted_piece = temp_board[land_r][land_c]\n",
        "                if not self._is_king(promoted_piece): # only promote if not already a king\n",
        "                    if (player == 1 and land_r == 7) or (player == 2 and land_r == 0): # Adjusted promotion rows\n",
        "                        temp_board[land_r][land_c] += 2 # Promote to king (1->3, 2->4)\n",
        "\n",
        "                # Recursively look for more captures from the new position\n",
        "                chained_captures = self._get_possible_captures_for_piece(land_r, land_c, player, temp_board, path + [(land_r, land_c)])\n",
        "\n",
        "                if chained_captures: # If more captures are found, add them\n",
        "                    captures.extend(chained_captures)\n",
        "                else: # If no more captures, this is a terminal capture sequence\n",
        "                    captures.append(path + [(land_r, land_c)])\n",
        "\n",
        "        # If no capture was found in this step, but it's part of a longer path, return empty to signify end of chain\n",
        "        if not found_capture_in_this_step and len(path) > 1:\n",
        "            return []\n",
        "\n",
        "        return captures\n",
        "\n",
        "\n",
        "\n",
        "    def get_valid_moves(self, player):\n",
        "        all_possible_moves = []\n",
        "        all_possible_captures = []\n",
        "\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == player:\n",
        "                    # Get basic moves\n",
        "                    all_possible_moves.extend(self._get_possible_moves_for_piece(r, c, player))\n",
        "                    # Get captures\n",
        "                    captures_from_piece = self._get_possible_captures_for_piece(r, c, player)\n",
        "                    if captures_from_piece:\n",
        "                        all_possible_captures.extend(captures_from_piece)\n",
        "\n",
        "        if all_possible_captures:\n",
        "            # If captures are available, they are mandatory\n",
        "            return all_possible_captures\n",
        "        else:\n",
        "            # Otherwise, return all basic moves\n",
        "            return all_possible_moves\n",
        "\n",
        "# Example usage:\n",
        "board = CheckersBoard()\n",
        "print(\"Initial Board:\")\n",
        "print(board)\n",
        "\n",
        "print(\"\\nValid moves for Player 1:\")\n",
        "moves_p1 = board.get_valid_moves(1)\n",
        "if moves_p1:\n",
        "    for move in moves_p1:\n",
        "        if isinstance(move[0], tuple): # If it's a move ((r1,c1), (r2,c2))\n",
        "            print(f\"  Move from {chr(move[0][1]+97)}{8-move[0][0]} to {chr(move[1][1]+97)}{8-move[1][0]}\")\n",
        "        else: # If it's a capture path [(r1,c1), (r2,c2), (r3,c3)...]\n",
        "            print(f\"  Capture path: {' -> '.join([f'{chr(pos[1]+97)}{8-pos[0]}' for pos in move])}\")\n",
        "else:\n",
        "    print(\"  No valid moves for Player 1.\")\n",
        "\n",
        "print(\"\\nValid moves for Player 2:\")\n",
        "moves_p2 = board.get_valid_moves(2)\n",
        "if moves_p2:\n",
        "    for move in moves_p2:\n",
        "        if isinstance(move[0], tuple): # If it's a move ((r1,c1), (r2,c2))\n",
        "            print(f\"  Move from {chr(move[0][1]+97)}{8-move[0][0]} to {chr(move[1][1]+97)}{8-move[1][0]}\")\n",
        "        else: # If it's a capture path [(r1,c1), (r2,c2), (r3,c3)...]\n",
        "            print(f\"  Capture path: {' -> '.join([f'{chr(pos[1]+97)}{8-pos[0]}' for pos in move])}\")\n",
        "else:\n",
        "    print(\"  No valid moves for Player 2.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Board:\n",
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  x  .  x  .  x  .  x \n",
            "5 | .  .  .  .  .  .  .  . \n",
            "4 | .  .  .  .  .  .  .  . \n",
            "3 | o  .  o  .  o  .  o  . \n",
            "2 | .  o  .  o  .  o  .  o \n",
            "1 | o  .  o  .  o  .  o  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n",
            "\n",
            "Valid moves for Player 1:\n",
            "  Move from b6 to a5\n",
            "  Move from b6 to c5\n",
            "  Move from d6 to c5\n",
            "  Move from d6 to e5\n",
            "  Move from f6 to e5\n",
            "  Move from f6 to g5\n",
            "  Move from h6 to g5\n",
            "\n",
            "Valid moves for Player 2:\n",
            "  Move from a3 to b4\n",
            "  Move from c3 to b4\n",
            "  Move from c3 to d4\n",
            "  Move from e3 to d4\n",
            "  Move from e3 to f4\n",
            "  Move from g3 to f4\n",
            "  Move from g3 to h4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9bc668c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully generated valid moves for both players. The next logical step, according to the subtask, is to implement a `make_move` function that applies a given move (either a simple move or a capture path) to the board, updates piece positions, removes captured pieces, and handles king promotions. This function will return a new `CheckersBoard` instance to allow for exploring game states without modifying the original.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ee1b3ca",
        "outputId": "7eefa8c1-b16a-46dc-8a65-faa7575ebd3f"
      },
      "source": [
        "class CheckersBoard:\n",
        "    def __init__(self, board=None):\n",
        "        if board is None:\n",
        "            self.board = [[0 for _ in range(8)] for _ in range(8)]\n",
        "            # 0: empty, 1: player 1 piece, 2: player 2 piece\n",
        "            # 3: player 1 king, 4: player 2 king\n",
        "\n",
        "            # Setup initial pieces for Player 1 (top of the board, rows 0-2)\n",
        "            for row in range(3):\n",
        "                for col in range(8):\n",
        "                    if (row + col) % 2 != 0:\n",
        "                        self.board[row][col] = 1\n",
        "\n",
        "            # Setup initial pieces for Player 2 (bottom of the board, rows 5-7)\n",
        "            for row in range(5, 8):\n",
        "                for col in range(8):\n",
        "                    if (row + col) % 2 != 0:\n",
        "                        self.board[row][col] = 2\n",
        "        else:\n",
        "            self.board = [row[:] for row in board] # Deep copy of the board\n",
        "\n",
        "    def __str__(self):\n",
        "        board_str = \"\"\n",
        "        for r_idx, row in enumerate(self.board):\n",
        "            board_str += f\"{8 - r_idx} |\"\n",
        "            for piece in row:\n",
        "                if piece == 0:\n",
        "                    board_str += \" . \"\n",
        "                elif piece == 1:\n",
        "                    board_str += \" x \"\n",
        "                elif piece == 2:\n",
        "                    board_str += \" o \"\n",
        "                elif piece == 3:\n",
        "                    board_str += \" X \"\n",
        "                elif piece == 4:\n",
        "                    board_str += \" O \"\n",
        "            board_str += \"\\n\"\n",
        "        board_str += \"  ------------------------\\n\"\n",
        "        board_str += \"    a  b  c  d  e  f  g  h\"\n",
        "        return board_str\n",
        "\n",
        "    def _is_valid_coord(self, r, c):\n",
        "        return 0 <= r < 8 and 0 <= c < 8\n",
        "\n",
        "    def _get_piece_owner(self, piece):\n",
        "        if piece == 1 or piece == 3:\n",
        "            return 1 # Player 1\n",
        "        elif piece == 2 or piece == 4:\n",
        "            return 2 # Player 2\n",
        "        return 0 # Empty or invalid\n",
        "\n",
        "    def _is_king(self, piece):\n",
        "        return piece == 3 or piece == 4\n",
        "\n",
        "    def _get_possible_moves_for_piece(self, r, c, player):\n",
        "        moves = []\n",
        "        piece = self.board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can move in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        for dr, dc in directions:\n",
        "            nr, nc = r + dr, c + dc\n",
        "            if self._is_valid_coord(nr, nc) and self.board[nr][nc] == 0:\n",
        "                moves.append(((r, c), (nr, nc)))\n",
        "        return moves\n",
        "\n",
        "    def _get_possible_captures_for_piece(self, r, c, player, current_board=None, path=None):\n",
        "        if current_board is None:\n",
        "            current_board = [row[:] for row in self.board]\n",
        "        if path is None:\n",
        "            path = [(r, c)]\n",
        "\n",
        "        captures = []\n",
        "        piece = current_board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            capture_directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            capture_directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can capture in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            capture_directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        found_capture_in_this_step = False\n",
        "        for dr, dc in capture_directions:\n",
        "            # Square of the opponent's piece to be jumped over\n",
        "            enemy_r, enemy_c = r + dr, c + dc\n",
        "            # Landing square after the jump\n",
        "            land_r, land_c = r + 2 * dr, c + 2 * dc\n",
        "\n",
        "            if (self._is_valid_coord(land_r, land_c) and\n",
        "                    current_board[land_r][land_c] == 0 and\n",
        "                    self._is_valid_coord(enemy_r, enemy_c) and\n",
        "                    current_board[enemy_r][enemy_c] != 0 and\n",
        "                    self._get_piece_owner(current_board[enemy_r][enemy_c]) != player):\n",
        "\n",
        "                found_capture_in_this_step = True\n",
        "                # Simulate the capture on a temporary board\n",
        "                temp_board = [row[:] for row in current_board]\n",
        "                temp_board[land_r][land_c] = temp_board[r][c]\n",
        "                temp_board[r][c] = 0\n",
        "                temp_board[enemy_r][enemy_c] = 0\n",
        "\n",
        "                # Check for king promotion after landing, only if not king yet and reaching back row\n",
        "                promoted_piece = temp_board[land_r][land_c]\n",
        "                if not self._is_king(promoted_piece): # only promote if not already a king\n",
        "                    if (player == 1 and land_r == 7) or (player == 2 and land_r == 0): # Adjusted promotion rows\n",
        "                        temp_board[land_r][land_c] += 2 # Promote to king (1->3, 2->4)\n",
        "\n",
        "                # Recursively look for more captures from the new position\n",
        "                chained_captures = self._get_possible_captures_for_piece(land_r, land_c, player, temp_board, path + [(land_r, land_c)])\n",
        "\n",
        "                if chained_captures: # If more captures are found, add them\n",
        "                    captures.extend(chained_captures)\n",
        "                else: # If no more captures, this is a terminal capture sequence\n",
        "                    captures.append(path + [(land_r, land_c)])\n",
        "\n",
        "        # If no capture was found in this step, but it's part of a longer path, return empty to signify end of chain\n",
        "        if not found_capture_in_this_step and len(path) > 1:\n",
        "            return []\n",
        "\n",
        "        return captures\n",
        "\n",
        "\n",
        "\n",
        "    def get_valid_moves(self, player):\n",
        "        all_possible_moves = []\n",
        "        all_possible_captures = []\n",
        "\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == player:\n",
        "                    # Get basic moves\n",
        "                    all_possible_moves.extend(self._get_possible_moves_for_piece(r, c, player))\n",
        "                    # Get captures\n",
        "                    captures_from_piece = self._get_possible_captures_for_piece(r, c, player)\n",
        "                    if captures_from_piece:\n",
        "                        all_possible_captures.extend(captures_from_piece)\n",
        "\n",
        "        if all_possible_captures:\n",
        "            # If captures are available, they are mandatory\n",
        "            return all_possible_captures\n",
        "        else:\n",
        "            # Otherwise, return all basic moves\n",
        "            return all_possible_moves\n",
        "\n",
        "    def make_move(self, move, player):\n",
        "        # Create a new board state for the move\n",
        "        new_board = CheckersBoard(board=self.board)\n",
        "\n",
        "        if isinstance(move[0], tuple) and len(move) == 2 and isinstance(move[1], tuple): # It's a simple move ((r1,c1), (r2,c2))\n",
        "            start_r, start_c = move[0]\n",
        "            end_r, end_c = move[1]\n",
        "            piece_to_move = new_board.board[start_r][start_c]\n",
        "\n",
        "            new_board.board[end_r][end_c] = piece_to_move\n",
        "            new_board.board[start_r][start_c] = 0\n",
        "\n",
        "            # Check for king promotion\n",
        "            if not new_board._is_king(piece_to_move):\n",
        "                if (player == 1 and end_r == 7) or (player == 2 and end_r == 0):\n",
        "                    new_board.board[end_r][end_c] += 2 # Promote to king\n",
        "\n",
        "        elif isinstance(move, list): # It's a capture path [(r1,c1), (r2,c2), ...]\n",
        "            for i in range(len(move) - 1):\n",
        "                start_r, start_c = move[i]\n",
        "                end_r, end_c = move[i+1]\n",
        "                piece_to_move = new_board.board[start_r][start_c]\n",
        "\n",
        "                new_board.board[end_r][end_c] = piece_to_move\n",
        "                new_board.board[start_r][start_c] = 0\n",
        "\n",
        "                # Remove captured piece\n",
        "                # Calculate the position of the captured piece (midpoint)\n",
        "                captured_r = (start_r + end_r) // 2\n",
        "                captured_c = (start_c + end_c) // 2\n",
        "                new_board.board[captured_r][captured_c] = 0\n",
        "\n",
        "            # After the whole capture path, check for king promotion at the final landing spot\n",
        "            final_r, final_c = move[-1]\n",
        "            promoted_piece = new_board.board[final_r][final_c]\n",
        "            if not new_board._is_king(promoted_piece):\n",
        "                if (player == 1 and final_r == 7) or (player == 2 and final_r == 0):\n",
        "                    new_board.board[final_r][final_c] += 2\n",
        "\n",
        "        return new_board\n",
        "\n",
        "# Example usage:\n",
        "board = CheckersBoard()\n",
        "print(\"Initial Board:\")\n",
        "print(board)\n",
        "\n",
        "print(\"\\nValid moves for Player 1:\")\n",
        "moves_p1 = board.get_valid_moves(1)\n",
        "if moves_p1:\n",
        "    print(f\"Found {len(moves_p1)} moves for Player 1. Applying the first move.\")\n",
        "    first_move_p1 = moves_p1[0]\n",
        "    new_board_p1 = board.make_move(first_move_p1, 1)\n",
        "    print(\"Board after Player 1's first move:\")\n",
        "    print(new_board_p1)\n",
        "else:\n",
        "    print(\"No valid moves for Player 1.\")\n",
        "\n",
        "print(\"\\nValid moves for Player 2:\")\n",
        "moves_p2 = board.get_valid_moves(2)\n",
        "if moves_p2:\n",
        "    print(f\"Found {len(moves_p2)} moves for Player 2. Applying the first move.\")\n",
        "    first_move_p2 = moves_p2[0]\n",
        "    new_board_p2 = board.make_move(first_move_p2, 2)\n",
        "    print(\"Board after Player 2's first move:\")\n",
        "    print(new_board_p2)\n",
        "else:\n",
        "    print(\"No valid moves for Player 2.\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Board:\n",
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  x  .  x  .  x  .  x \n",
            "5 | .  .  .  .  .  .  .  . \n",
            "4 | .  .  .  .  .  .  .  . \n",
            "3 | o  .  o  .  o  .  o  . \n",
            "2 | .  o  .  o  .  o  .  o \n",
            "1 | o  .  o  .  o  .  o  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n",
            "\n",
            "Valid moves for Player 1:\n",
            "Found 7 moves for Player 1. Applying the first move.\n",
            "Board after Player 1's first move:\n",
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  .  .  x  .  x  .  x \n",
            "5 | x  .  .  .  .  .  .  . \n",
            "4 | .  .  .  .  .  .  .  . \n",
            "3 | o  .  o  .  o  .  o  . \n",
            "2 | .  o  .  o  .  o  .  o \n",
            "1 | o  .  o  .  o  .  o  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n",
            "\n",
            "Valid moves for Player 2:\n",
            "Found 7 moves for Player 2. Applying the first move.\n",
            "Board after Player 2's first move:\n",
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  x  .  x  .  x  .  x \n",
            "5 | .  .  .  .  .  .  .  . \n",
            "4 | .  o  .  .  .  .  .  . \n",
            "3 | .  .  o  .  o  .  o  . \n",
            "2 | .  o  .  o  .  o  .  o \n",
            "1 | o  .  o  .  o  .  o  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64915dbc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps successfully implemented board representation, move generation, and move execution. The final part of the subtask is to implement a function to detect terminal states of the game, determining if the game has ended and who won or if it's a draw, based on piece count and available moves.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe6516ca",
        "outputId": "4ceab273-3d84-4286-b967-09b38000bf97"
      },
      "source": [
        "class CheckersBoard:\n",
        "    def __init__(self, board=None):\n",
        "        if board is None:\n",
        "            self.board = [[0 for _ in range(8)] for _ in range(8)]\n",
        "            # 0: empty, 1: player 1 piece, 2: player 2 piece\n",
        "            # 3: player 1 king, 4: player 2 king\n",
        "\n",
        "            # Setup initial pieces for Player 1 (top of the board, rows 0-2)\n",
        "            for row in range(3):\n",
        "                for col in range(8):\n",
        "                    if (row + col) % 2 != 0:\n",
        "                        self.board[row][col] = 1\n",
        "\n",
        "            # Setup initial pieces for Player 2 (bottom of the board, rows 5-7)\n",
        "            for row in range(5, 8):\n",
        "                for col in range(8):\n",
        "                    if (row + col) % 2 != 0:\n",
        "                        self.board[row][col] = 2\n",
        "        else:\n",
        "            self.board = [row[:] for row in board] # Deep copy of the board\n",
        "\n",
        "    def __str__(self):\n",
        "        board_str = \"\"\n",
        "        for r_idx, row in enumerate(self.board):\n",
        "            board_str += f\"{8 - r_idx} |\"\n",
        "            for piece in row:\n",
        "                if piece == 0:\n",
        "                    board_str += \" . \"\n",
        "                elif piece == 1:\n",
        "                    board_str += \" x \"\n",
        "                elif piece == 2:\n",
        "                    board_str += \" o \"\n",
        "                elif piece == 3:\n",
        "                    board_str += \" X \"\n",
        "                elif piece == 4:\n",
        "                    board_str += \" O \"\n",
        "            board_str += \"\\n\"\n",
        "        board_str += \"  ------------------------\\n\"\n",
        "        board_str += \"    a  b  c  d  e  f  g  h\"\n",
        "        return board_str\n",
        "\n",
        "    def _is_valid_coord(self, r, c):\n",
        "        return 0 <= r < 8 and 0 <= c < 8\n",
        "\n",
        "    def _get_piece_owner(self, piece):\n",
        "        if piece == 1 or piece == 3:\n",
        "            return 1 # Player 1\n",
        "        elif piece == 2 or piece == 4:\n",
        "            return 2 # Player 2\n",
        "        return 0 # Empty or invalid\n",
        "\n",
        "    def _is_king(self, piece):\n",
        "        return piece == 3 or piece == 4\n",
        "\n",
        "    def _get_possible_moves_for_piece(self, r, c, player):\n",
        "        moves = []\n",
        "        piece = self.board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can move in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        for dr, dc in directions:\n",
        "            nr, nc = r + dr, c + dc\n",
        "            if self._is_valid_coord(nr, nc) and self.board[nr][nc] == 0:\n",
        "                moves.append(((r, c), (nr, nc)))\n",
        "        return moves\n",
        "\n",
        "    def _get_possible_captures_for_piece(self, r, c, player, current_board=None, path=None):\n",
        "        if current_board is None:\n",
        "            current_board = [row[:] for row in self.board]\n",
        "        if path is None:\n",
        "            path = [(r, c)]\n",
        "\n",
        "        captures = []\n",
        "        piece = current_board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            capture_directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            capture_directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can capture in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            capture_directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        found_capture_in_this_step = False\n",
        "        for dr, dc in capture_directions:\n",
        "            # Square of the opponent's piece to be jumped over\n",
        "            enemy_r, enemy_c = r + dr, c + dc\n",
        "            # Landing square after the jump\n",
        "            land_r, land_c = r + 2 * dr, c + 2 * dc\n",
        "\n",
        "            if (self._is_valid_coord(land_r, land_c) and\n",
        "                    current_board[land_r][land_c] == 0 and\n",
        "                    self._is_valid_coord(enemy_r, enemy_c) and\n",
        "                    current_board[enemy_r][enemy_c] != 0 and\n",
        "                    self._get_piece_owner(current_board[enemy_r][enemy_c]) != player):\n",
        "\n",
        "                found_capture_in_this_step = True\n",
        "                # Simulate the capture on a temporary board\n",
        "                temp_board = [row[:] for row in current_board]\n",
        "                temp_board[land_r][land_c] = temp_board[r][c]\n",
        "                temp_board[r][c] = 0\n",
        "                temp_board[enemy_r][enemy_c] = 0\n",
        "\n",
        "                # Check for king promotion after landing, only if not king yet and reaching back row\n",
        "                promoted_piece = temp_board[land_r][land_c]\n",
        "                if not self._is_king(promoted_piece): # only promote if not already a king\n",
        "                    if (player == 1 and land_r == 7) or (player == 2 and land_r == 0): # Adjusted promotion rows\n",
        "                        temp_board[land_r][land_c] += 2 # Promote to king (1->3, 2->4)\n",
        "\n",
        "                # Recursively look for more captures from the new position\n",
        "                chained_captures = self._get_possible_captures_for_piece(land_r, land_c, player, temp_board, path + [(land_r, land_c)])\n",
        "\n",
        "                if chained_captures: # If more captures are found, add them\n",
        "                    captures.extend(chained_captures)\n",
        "                else: # If no more captures, this is a terminal capture sequence\n",
        "                    captures.append(path + [(land_r, land_c)])\n",
        "\n",
        "        # If no capture was found in this step, but it's part of a longer path, return empty to signify end of chain\n",
        "        if not found_capture_in_this_step and len(path) > 1:\n",
        "            return []\n",
        "\n",
        "        return captures\n",
        "\n",
        "    def get_valid_moves(self, player):\n",
        "        all_possible_moves = []\n",
        "        all_possible_captures = []\n",
        "\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == player:\n",
        "                    # Get basic moves\n",
        "                    all_possible_moves.extend(self._get_possible_moves_for_piece(r, c, player))\n",
        "                    # Get captures\n",
        "                    captures_from_piece = self._get_possible_captures_for_piece(r, c, player)\n",
        "                    if captures_from_piece:\n",
        "                        all_possible_captures.extend(captures_from_piece)\n",
        "\n",
        "        if all_possible_captures:\n",
        "            # If captures are available, they are mandatory\n",
        "            return all_possible_captures\n",
        "        else:\n",
        "            # Otherwise, return all basic moves\n",
        "            return all_possible_moves\n",
        "\n",
        "    def make_move(self, move, player):\n",
        "        # Create a new board state for the move\n",
        "        new_board = CheckersBoard(board=self.board)\n",
        "\n",
        "        if isinstance(move[0], tuple) and len(move) == 2 and isinstance(move[1], tuple): # It's a simple move ((r1,c1), (r2,c2))\n",
        "            start_r, start_c = move[0]\n",
        "            end_r, end_c = move[1]\n",
        "            piece_to_move = new_board.board[start_r][start_c]\n",
        "\n",
        "            new_board.board[end_r][end_c] = piece_to_move\n",
        "            new_board.board[start_r][start_c] = 0\n",
        "\n",
        "            # Check for king promotion\n",
        "            if not new_board._is_king(piece_to_move): # Only promote if not already a king\n",
        "                if (player == 1 and end_r == 7) or (player == 2 and end_r == 0):\n",
        "                    new_board.board[end_r][end_c] += 2 # Promote to king\n",
        "\n",
        "        elif isinstance(move, list): # It's a capture path [(r1,c1), (r2,c2), ...]\n",
        "            for i in range(len(move) - 1):\n",
        "                start_r, start_c = move[i]\n",
        "                end_r, end_c = move[i+1]\n",
        "                piece_to_move = new_board.board[start_r][start_c]\n",
        "\n",
        "                new_board.board[end_r][end_c] = piece_to_move\n",
        "                new_board.board[start_r][start_c] = 0\n",
        "\n",
        "                # Remove captured piece\n",
        "                # Calculate the position of the captured piece (midpoint)\n",
        "                captured_r = (start_r + end_r) // 2\n",
        "                captured_c = (start_c + end_c) // 2\n",
        "                new_board.board[captured_r][captured_c] = 0\n",
        "\n",
        "            # After the whole capture path, check for king promotion at the final landing spot\n",
        "            final_r, final_c = move[-1]\n",
        "            promoted_piece = new_board.board[final_r][final_c]\n",
        "            if not new_board._is_king(promoted_piece): # Only promote if not already a king\n",
        "                if (player == 1 and final_r == 7) or (player == 2 and final_r == 0):\n",
        "                    new_board.board[final_r][final_c] += 2\n",
        "\n",
        "        return new_board\n",
        "\n",
        "    def is_game_over(self):\n",
        "        player1_pieces = 0\n",
        "        player2_pieces = 0\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == 1:\n",
        "                    player1_pieces += 1\n",
        "                elif self._get_piece_owner(piece) == 2:\n",
        "                    player2_pieces += 1\n",
        "\n",
        "        # Check for no pieces left\n",
        "        if player1_pieces == 0: return True, 2 # Player 1 has no pieces, Player 2 wins\n",
        "        if player2_pieces == 0: return True, 1 # Player 2 has no pieces, Player 1 wins\n",
        "\n",
        "        # Check for no valid moves for the current player\n",
        "        # This would typically be checked by the game loop for the *current* player's turn.\n",
        "        # For this function, we check for both to determine if ANY player can make a move.\n",
        "        # However, a more accurate check for a terminal state is often for the *current player*.\n",
        "        # Let's assume a simplified check here: if either player has no moves.\n",
        "        # The game loop will determine whose turn it is. This function determines if\n",
        "        # a general end state has been reached.\n",
        "\n",
        "        # A more robust implementation would check for no valid moves for the 'current' player\n",
        "        # at the time this function is called. For now, let's just make sure there are pieces.\n",
        "        # If we reach here, and both players have pieces, we assume the game is not over\n",
        "        # unless explicitly told whose turn it is and they have no moves.\n",
        "        # For now, let's implement a simple version where if a player has no valid moves,\n",
        "        # the other player wins. This requires `is_game_over` to know whose turn it is.\n",
        "        # To avoid making `is_game_over` take `current_player` as input,\n",
        "        # let's assume this check happens before calling `is_game_over` or `is_game_over`\n",
        "        # itself checks for both players.\n",
        "\n",
        "        # Re-evaluating: The subtask specifies 'no valid moves available'.\n",
        "        # This implies checking for the current player whose turn it is.\n",
        "        # Since this method is part of the board, it should ideally be agnostic to whose turn it is,\n",
        "        # or take the current player as input.\n",
        "        # Let's make it check for both players and return a draw if neither can move.\n",
        "        # In Checkers, if a player cannot move, they lose. So, if current player has no moves,\n",
        "        # the other player wins. If both have no moves (unlikely with pieces), it's a draw.\n",
        "\n",
        "        # Let's count pieces and if either player cannot move (and the other has pieces), that's a loss.\n",
        "        # For this function to be truly 'terminal state detection', it needs the current player.\n",
        "        # For now, I'll provide an example where a player is passed to 'is_game_over'\n",
        "        # and assume it's called with `current_player` to check for no moves.\n",
        "\n",
        "        # Default return for ongoing game\n",
        "        return False, None\n",
        "\n",
        "    # Helper to check if a specific player has any valid moves\n",
        "    def _has_valid_moves(self, player):\n",
        "        return bool(self.get_valid_moves(player))\n",
        "\n",
        "    # The main game over function, now taking the current player\n",
        "    def check_terminal_state(self, current_player):\n",
        "        player1_pieces = 0\n",
        "        player2_pieces = 0\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == 1:\n",
        "                    player1_pieces += 1\n",
        "                elif self._get_piece_owner(piece) == 2:\n",
        "                    player2_pieces += 1\n",
        "\n",
        "        # Condition 1: No pieces left\n",
        "        if player1_pieces == 0: return True, 2 # Player 2 wins\n",
        "        if player2_pieces == 0: return True, 1 # Player 1 wins\n",
        "\n",
        "        # Condition 2: Current player has no valid moves\n",
        "        if not self._has_valid_moves(current_player):\n",
        "            if current_player == 1: return True, 2 # Player 1 has no moves, Player 2 wins\n",
        "            if current_player == 2: return True, 1 # Player 2 has no moves, Player 1 wins\n",
        "\n",
        "        # Default: Game is not over\n",
        "        return False, None\n",
        "\n",
        "# Example usage for terminal state detection:\n",
        "board = CheckersBoard()\n",
        "print(\"Initial Board:\")\n",
        "print(board)\n",
        "\n",
        "# Check terminal state for Player 1 at the beginning (should not be over)\n",
        "is_over, winner = board.check_terminal_state(1)\n",
        "print(f\"\\nGame over (P1's turn)? {is_over}, Winner: {winner}\")\n",
        "\n",
        "# Simulate a scenario where Player 2 has no pieces\n",
        "empty_board_p2 = CheckersBoard()\n",
        "for r in range(8):\n",
        "    for c in range(8):\n",
        "        if empty_board_p2.board[r][c] == 2 or empty_board_p2.board[r][c] == 4:\n",
        "            empty_board_p2.board[r][c] = 0\n",
        "\n",
        "print(\"\\nBoard with no P2 pieces:\")\n",
        "print(empty_board_p2)\n",
        "is_over, winner = empty_board_p2.check_terminal_state(1) # P1's turn\n",
        "print(f\"Game over (P1's turn)? {is_over}, Winner: {winner}\")\n",
        "\n",
        "# Simulate a scenario where Player 1 has no valid moves (e.g., block all pieces)\n",
        "# This is harder to construct artificially without a lot of setup.\n",
        "# For demonstration, let's assume a board state where P1 has pieces but no moves.\n",
        "# For a true no-move test, we'd need a very specific board setup.\n",
        "# Let's create a minimal board where P1 has one piece at (0,1) and cannot move.\n",
        "no_move_board_p1 = CheckersBoard([[0 for _ in range(8)] for _ in range(8)])\n",
        "no_move_board_p1.board[0][1] = 1 # P1 piece, but cannot move (top edge)\n",
        "no_move_board_p1.board[7][0] = 2 # P2 piece for game to theoretically continue\n",
        "\n",
        "print(\"\\nBoard with P1 having no valid moves:\")\n",
        "print(no_move_board_p1)\n",
        "is_over, winner = no_move_board_p1.check_terminal_state(1) # P1's turn\n",
        "print(f\"Game over (P1's turn)? {is_over}, Winner: {winner}\")\n",
        "\n",
        "is_over, winner = no_move_board_p1.check_terminal_state(2) # P2's turn\n",
        "print(f\"Game over (P2's turn)? {is_over}, Winner: {winner}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Board:\n",
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  x  .  x  .  x  .  x \n",
            "5 | .  .  .  .  .  .  .  . \n",
            "4 | .  .  .  .  .  .  .  . \n",
            "3 | o  .  o  .  o  .  o  . \n",
            "2 | .  o  .  o  .  o  .  o \n",
            "1 | o  .  o  .  o  .  o  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n",
            "\n",
            "Game over (P1's turn)? False, Winner: None\n",
            "\n",
            "Board with no P2 pieces:\n",
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  x  .  x  .  x  .  x \n",
            "5 | .  .  .  .  .  .  .  . \n",
            "4 | .  .  .  .  .  .  .  . \n",
            "3 | .  .  .  .  .  .  .  . \n",
            "2 | .  .  .  .  .  .  .  . \n",
            "1 | .  .  .  .  .  .  .  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n",
            "Game over (P1's turn)? True, Winner: 1\n",
            "\n",
            "Board with P1 having no valid moves:\n",
            "8 | .  x  .  .  .  .  .  . \n",
            "7 | .  .  .  .  .  .  .  . \n",
            "6 | .  .  .  .  .  .  .  . \n",
            "5 | .  .  .  .  .  .  .  . \n",
            "4 | .  .  .  .  .  .  .  . \n",
            "3 | .  .  .  .  .  .  .  . \n",
            "2 | .  .  .  .  .  .  .  . \n",
            "1 | o  .  .  .  .  .  .  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n",
            "Game over (P1's turn)? False, Winner: None\n",
            "Game over (P2's turn)? False, Winner: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f935cb7"
      },
      "source": [
        "## Desenvolvimento do Agente Q-Learning\n",
        "\n",
        "### Subtask:\n",
        "Criar a estrutura básica para os agentes de Q-learning, incluindo a representação de estados e ações, a tabela Q (ou rede neural), a função de recompensa, e o mecanismo de exploração/explotação (e.g., epsilon-greedy).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67ef9a97"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to define the `CheckersAgent` class, initializing it with Q-learning parameters like learning rate, discount factor, and exploration rate. Additionally, I will implement a method to convert the current board state into a hashable representation (tuple of tuples) suitable for use as keys in the Q-table.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54d5ad15"
      },
      "source": [
        "import random\n",
        "\n",
        "class CheckersAgent:\n",
        "    def __init__(self, player, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.player = player # 1 for player 1, 2 for player 2\n",
        "        self.alpha = alpha # Learning rate\n",
        "        self.gamma = gamma # Discount factor\n",
        "        self.epsilon = epsilon # Exploration rate\n",
        "        self.q_table = {}\n",
        "\n",
        "    def _get_state_representation(self, board):\n",
        "        # Convert the 2D list board into a tuple of tuples for hashability\n",
        "        return tuple(tuple(row) for row in board.board)\n",
        "\n",
        "    def _get_possible_actions(self, board):\n",
        "        # Actions are the valid moves obtained from the CheckersBoard class\n",
        "        return board.get_valid_moves(self.player)\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        # Get the Q-value for a given state-action pair, default to 0 if not seen\n",
        "        # Actions can be tuples (for simple moves) or lists of tuples (for capture paths)\n",
        "        # Convert action to a hashable format if it's a list\n",
        "        hashable_action = tuple(action) if isinstance(action, list) else action\n",
        "        return self.q_table.get((state, hashable_action), 0.0)\n",
        "\n",
        "    def choose_action(self, board):\n",
        "        state = self._get_state_representation(board)\n",
        "        possible_actions = self._get_possible_actions(board)\n",
        "\n",
        "        if not possible_actions:\n",
        "            return None # No valid moves\n",
        "\n",
        "        # Epsilon-greedy strategy\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            # Explore: choose a random action\n",
        "            return random.choice(possible_actions)\n",
        "        else:\n",
        "            # Exploit: choose the action with the highest Q-value\n",
        "            q_values = [self.get_q_value(state, action) for action in possible_actions]\n",
        "            max_q = -float('inf')\n",
        "            best_actions = []\n",
        "\n",
        "            for i, q_val in enumerate(q_values):\n",
        "                if q_val > max_q:\n",
        "                    max_q = q_val\n",
        "                    best_actions = [possible_actions[i]]\n",
        "                elif q_val == max_q:\n",
        "                    best_actions.append(possible_actions[i])\n",
        "\n",
        "            return random.choice(best_actions)\n",
        "\n",
        "    def learn(self, old_state, action, reward, new_state, new_board):\n",
        "        hashable_action = tuple(action) if isinstance(action, list) else action\n",
        "\n",
        "        old_state_rep = self._get_state_representation(old_state)\n",
        "        new_state_rep = self._get_state_representation(new_state)\n",
        "\n",
        "        current_q = self.get_q_value(old_state_rep, hashable_action)\n",
        "\n",
        "        # Calculate max Q for the new state (Q(s', a'))\n",
        "        future_possible_actions = self._get_possible_actions(new_board) # Use new_board for new_state's possible actions\n",
        "        if future_possible_actions:\n",
        "            max_future_q = max([self.get_q_value(new_state_rep, a) for a in future_possible_actions])\n",
        "        else:\n",
        "            max_future_q = 0.0 # Terminal state or no actions\n",
        "\n",
        "        # Q-learning update rule\n",
        "        self.q_table[(old_state_rep, hashable_action)] = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)\n",
        "\n",
        "# Reward function (can be a standalone function or part of the agent/game class)\n",
        "def get_reward(board, player, opponent_player):\n",
        "    is_over, winner = board.check_terminal_state(player)\n",
        "\n",
        "    if is_over:\n",
        "        if winner == player:\n",
        "            return 100 # Win\n",
        "        elif winner == opponent_player:\n",
        "            return -100 # Loss\n",
        "        else:\n",
        "            return -10 # Draw or other negative outcome for current player\n",
        "\n",
        "    # Non-terminal state rewards (adjust as needed)\n",
        "    # Example: slightly positive for capturing, slightly negative for losing pieces\n",
        "    return -1 # Small negative reward for each step to encourage faster wins\n",
        "\n",
        "# Example usage (requires CheckersBoard class from previous steps):\n",
        "# board = CheckersBoard()\n",
        "# agent_p1 = CheckersAgent(player=1)\n",
        "# agent_p2 = CheckersAgent(player=2)\n",
        "\n",
        "# # Simulate a turn for Player 1\n",
        "# current_board = CheckersBoard()\n",
        "# print(\"Initial board:\")\n",
        "# print(current_board)\n",
        "\n",
        "# old_state_p1 = current_board # Keep a reference to the old board object for state representation\n",
        "# action_p1 = agent_p1.choose_action(old_state_p1)\n",
        "\n",
        "# if action_p1:\n",
        "#     new_board_p1 = current_board.make_move(action_p1, 1)\n",
        "#     reward_p1 = get_reward(new_board_p1, 1, 2)\n",
        "#     agent_p1.learn(old_state_p1, action_p1, reward_p1, new_board_p1, new_board_p1)\n",
        "#     print(f\"\\nPlayer 1 chose action: {action_p1}\")\n",
        "#     print(\"Board after P1's move:\")\n",
        "#     print(new_board_p1)\n",
        "#     print(f\"Reward for P1: {reward_p1}\")\n",
        "# else:\n",
        "#     print(\"Player 1 has no moves.\")\n",
        "\n",
        "# # Simulate a turn for Player 2 (using the board after P1's move)\n",
        "# if 'new_board_p1' in locals():\n",
        "#     old_state_p2 = new_board_p1 # Keep a reference to the old board object for state representation\n",
        "#     action_p2 = agent_p2.choose_action(old_state_p2)\n",
        "\n",
        "#     if action_p2:\n",
        "#         new_board_p2 = old_state_p2.make_move(action_p2, 2)\n",
        "#         reward_p2 = get_reward(new_board_p2, 2, 1)\n",
        "#         agent_p2.learn(old_state_p2, action_p2, reward_p2, new_board_p2, new_board_p2)\n",
        "#         print(f\"\\nPlayer 2 chose action: {action_p2}\")\n",
        "#         print(\"Board after P2's move:\")\n",
        "#         print(new_board_p2)\n",
        "#         print(f\"Reward for P2: {reward_p2}\")\n",
        "#     else:\n",
        "#         print(\"Player 2 has no moves.\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d45d9f8"
      },
      "source": [
        "## Função de Avaliação Heurística para Q-Value\n",
        "\n",
        "### Subtask:\n",
        "Desenvolver uma função de avaliação heurística para estados do jogo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c93e40ac"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires developing a heuristic evaluation function. The first step is to implement the `evaluate_board` method within the `CheckersAgent` class, which will analyze the board state and return a score based on piece counts and their positions, differentiating between normal pieces and kings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "856520ac",
        "outputId": "90f466bb-9f80-4f9c-f0e9-7b0c1b9be5e5"
      },
      "source": [
        "import random\n",
        "\n",
        "class CheckersBoard:\n",
        "    def __init__(self, board=None):\n",
        "        if board is None:\n",
        "            self.board = [[0 for _ in range(8)] for _ in range(8)]\n",
        "            # 0: empty, 1: player 1 piece, 2: player 2 piece\n",
        "            # 3: player 1 king, 4: player 2 king\n",
        "\n",
        "            # Setup initial pieces for Player 1 (top of the board, rows 0-2)\n",
        "            for row in range(3):\n",
        "                for col in range(8):\n",
        "                    if (row + col) % 2 != 0:\n",
        "                        self.board[row][col] = 1\n",
        "\n",
        "            # Setup initial pieces for Player 2 (bottom of the board, rows 5-7)\n",
        "            for row in range(5, 8):\n",
        "                for col in range(8):\n",
        "                    if (row + col) % 2 != 0:\n",
        "                        self.board[row][col] = 2\n",
        "        else:\n",
        "            self.board = [row[:] for row in board] # Deep copy of the board\n",
        "\n",
        "    def __str__(self):\n",
        "        board_str = \"\"\n",
        "        for r_idx, row in enumerate(self.board):\n",
        "            board_str += f\"{8 - r_idx} |\"\n",
        "            for piece in row:\n",
        "                if piece == 0:\n",
        "                    board_str += \" . \"\n",
        "                elif piece == 1:\n",
        "                    board_str += \" x \"\n",
        "                elif piece == 2:\n",
        "                    board_str += \" o \"\n",
        "                elif piece == 3:\n",
        "                    board_str += \" X \"\n",
        "                elif piece == 4:\n",
        "                    board_str += \" O \"\n",
        "            board_str += \"\\n\"\n",
        "        board_str += \"  ------------------------\\n\"\n",
        "        board_str += \"    a  b  c  d  e  f  g  h\"\n",
        "        return board_str\n",
        "\n",
        "    def _is_valid_coord(self, r, c):\n",
        "        return 0 <= r < 8 and 0 <= c < 8\n",
        "\n",
        "    def _get_piece_owner(self, piece):\n",
        "        if piece == 1 or piece == 3:\n",
        "            return 1 # Player 1\n",
        "        elif piece == 2 or piece == 4:\n",
        "            return 2 # Player 2\n",
        "        return 0 # Empty or invalid\n",
        "\n",
        "    def _is_king(self, piece):\n",
        "        return piece == 3 or piece == 4\n",
        "\n",
        "    def _get_possible_moves_for_piece(self, r, c, player):\n",
        "        moves = []\n",
        "        piece = self.board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can move in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        for dr, dc in directions:\n",
        "            nr, nc = r + dr, c + dc\n",
        "            if self._is_valid_coord(nr, nc) and self.board[nr][nc] == 0:\n",
        "                moves.append(((r, c), (nr, nc)))\n",
        "        return moves\n",
        "\n",
        "    def _get_possible_captures_for_piece(self, r, c, player, current_board=None, path=None):\n",
        "        if current_board is None:\n",
        "            current_board = [row[:] for row in self.board]\n",
        "        if path is None:\n",
        "            path = [(r, c)]\n",
        "\n",
        "        captures = []\n",
        "        piece = current_board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            capture_directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            capture_directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can capture in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            capture_directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        found_capture_in_this_step = False\n",
        "        for dr, dc in capture_directions:\n",
        "            # Square of the opponent's piece to be jumped over\n",
        "            enemy_r, enemy_c = r + dr, c + dc\n",
        "            # Landing square after the jump\n",
        "            land_r, land_c = r + 2 * dr, c + 2 * dc\n",
        "\n",
        "            if (self._is_valid_coord(land_r, land_c) and\n",
        "                    current_board[land_r][land_c] == 0 and\n",
        "                    self._is_valid_coord(enemy_r, enemy_c) and\n",
        "                    current_board[enemy_r][enemy_c] != 0 and\n",
        "                    self._get_piece_owner(current_board[enemy_r][enemy_c]) != player):\n",
        "\n",
        "                found_capture_in_this_step = True\n",
        "                # Simulate the capture on a temporary board\n",
        "                temp_board = [row[:] for row in current_board]\n",
        "                temp_board[land_r][land_c] = temp_board[r][c]\n",
        "                temp_board[r][c] = 0\n",
        "                temp_board[enemy_r][enemy_c] = 0\n",
        "\n",
        "                # Check for king promotion after landing, only if not king yet and reaching back row\n",
        "                promoted_piece = temp_board[land_r][land_c]\n",
        "                if not self._is_king(promoted_piece): # only promote if not already a king\n",
        "                    if (player == 1 and land_r == 7) or (player == 2 and land_r == 0): # Adjusted promotion rows\n",
        "                        temp_board[land_r][land_c] += 2 # Promote to king (1->3, 2->4)\n",
        "\n",
        "                # Recursively look for more captures from the new position\n",
        "                chained_captures = self._get_possible_captures_for_piece(land_r, land_c, player, temp_board, path + [(land_r, land_c)])\n",
        "\n",
        "                if chained_captures: # If more captures are found, add them\n",
        "                    captures.extend(chained_captures)\n",
        "                else: # If no more captures, this is a terminal capture sequence\n",
        "                    captures.append(path + [(land_r, land_c)])\n",
        "\n",
        "        # If no capture was found in this step, but it's part of a longer path, return empty to signify end of chain\n",
        "        if not found_capture_in_this_step and len(path) > 1:\n",
        "            return []\n",
        "\n",
        "        return captures\n",
        "\n",
        "    def get_valid_moves(self, player):\n",
        "        all_possible_moves = []\n",
        "        all_possible_captures = []\n",
        "\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == player:\n",
        "                    # Get basic moves\n",
        "                    all_possible_moves.extend(self._get_possible_moves_for_piece(r, c, player))\n",
        "                    # Get captures\n",
        "                    captures_from_piece = self._get_possible_captures_for_piece(r, c, player)\n",
        "                    if captures_from_piece:\n",
        "                        all_possible_captures.extend(captures_from_piece)\n",
        "\n",
        "        if all_possible_captures:\n",
        "            # If captures are available, they are mandatory\n",
        "            return all_possible_captures\n",
        "        else:\n",
        "            # Otherwise, return all basic moves\n",
        "            return all_possible_moves\n",
        "\n",
        "    def make_move(self, move, player):\n",
        "        # Create a new board state for the move\n",
        "        new_board = CheckersBoard(board=self.board)\n",
        "\n",
        "        if isinstance(move[0], tuple) and len(move) == 2 and isinstance(move[1], tuple): # It's a simple move ((r1,c1), (r2,c2))\n",
        "            start_r, start_c = move[0]\n",
        "            end_r, end_c = move[1]\n",
        "            piece_to_move = new_board.board[start_r][start_c]\n",
        "\n",
        "            new_board.board[end_r][end_c] = piece_to_move\n",
        "            new_board.board[start_r][start_c] = 0\n",
        "\n",
        "            # Check for king promotion\n",
        "            if not new_board._is_king(piece_to_move): # Only promote if not already a king\n",
        "                if (player == 1 and end_r == 7) or (player == 2 and end_r == 0):\n",
        "                    new_board.board[end_r][end_c] += 2 # Promote to king\n",
        "\n",
        "        elif isinstance(move, list): # It's a capture path [(r1,c1), (r2,c2), ...]\n",
        "            for i in range(len(move) - 1):\n",
        "                start_r, start_c = move[i]\n",
        "                end_r, end_c = move[i+1]\n",
        "                piece_to_move = new_board.board[start_r][start_c]\n",
        "\n",
        "                new_board.board[end_r][end_c] = piece_to_move\n",
        "                new_board.board[start_r][start_c] = 0\n",
        "\n",
        "                # Remove captured piece\n",
        "                # Calculate the position of the captured piece (midpoint)\n",
        "                captured_r = (start_r + end_r) // 2\n",
        "                captured_c = (start_c + end_c) // 2\n",
        "                new_board.board[captured_r][captured_c] = 0\n",
        "\n",
        "            # After the whole capture path, check for king promotion at the final landing spot\n",
        "            final_r, final_c = move[-1]\n",
        "            promoted_piece = new_board.board[final_r][final_c]\n",
        "            if not new_board._is_king(promoted_piece): # Only promote if not already a king\n",
        "                if (player == 1 and final_r == 7) or (player == 2 and final_r == 0):\n",
        "                    new_board.board[final_r][final_c] += 2\n",
        "\n",
        "        return new_board\n",
        "\n",
        "    def is_game_over(self):\n",
        "        player1_pieces = 0\n",
        "        player2_pieces = 0\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == 1:\n",
        "                    player1_pieces += 1\n",
        "                elif self._get_piece_owner(piece) == 2:\n",
        "                    player2_pieces += 1\n",
        "\n",
        "        # Check for no pieces left\n",
        "        if player1_pieces == 0: return True, 2 # Player 1 has no pieces, Player 2 wins\n",
        "        if player2_pieces == 0: return True, 1 # Player 2 has no pieces, Player 1 wins\n",
        "\n",
        "        return False, None\n",
        "\n",
        "    def _has_valid_moves(self, player):\n",
        "        return bool(self.get_valid_moves(player))\n",
        "\n",
        "    def check_terminal_state(self, current_player):\n",
        "        player1_pieces = 0\n",
        "        player2_pieces = 0\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == 1:\n",
        "                    player1_pieces += 1\n",
        "                elif self._get_piece_owner(piece) == 2:\n",
        "                    player2_pieces += 1\n",
        "\n",
        "        # Condition 1: No pieces left\n",
        "        if player1_pieces == 0: return True, 2 # Player 2 wins\n",
        "        if player2_pieces == 0: return True, 1 # Player 1 wins\n",
        "\n",
        "        # Condition 2: Current player has no valid moves\n",
        "        if not self._has_valid_moves(current_player):\n",
        "            if current_player == 1: return True, 2 # Player 1 has no moves, Player 2 wins\n",
        "            if current_player == 2: return True, 1 # Player 2 has no moves, Player 1 wins\n",
        "\n",
        "        # Default: Game is not over\n",
        "        return False, None\n",
        "\n",
        "class CheckersAgent:\n",
        "    def __init__(self, player, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.player = player # 1 for player 1, 2 for player 2\n",
        "        self.alpha = alpha # Learning rate\n",
        "        self.gamma = gamma # Discount factor\n",
        "        self.epsilon = epsilon # Exploration rate\n",
        "        self.q_table = {}\n",
        "\n",
        "    def _get_state_representation(self, board):\n",
        "        # Convert the 2D list board into a tuple of tuples for hashability\n",
        "        return tuple(tuple(row) for row in board.board)\n",
        "\n",
        "    def _get_possible_actions(self, board):\n",
        "        # Actions are the valid moves obtained from the CheckersBoard class\n",
        "        return board.get_valid_moves(self.player)\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        # Get the Q-value for a given state-action pair, default to 0 if not seen\n",
        "        # Actions can be tuples (for simple moves) or lists of tuples (for capture paths)\n",
        "        # Convert action to a hashable format if it's a list\n",
        "        hashable_action = tuple(action) if isinstance(action, list) else action\n",
        "        return self.q_table.get((state, hashable_action), 0.0)\n",
        "\n",
        "    def choose_action(self, board):\n",
        "        state = self._get_state_representation(board)\n",
        "        possible_actions = self._get_possible_actions(board)\n",
        "\n",
        "        if not possible_actions:\n",
        "            return None # No valid moves\n",
        "\n",
        "        # Epsilon-greedy strategy\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            # Explore: choose a random action\n",
        "            return random.choice(possible_actions)\n",
        "        else:\n",
        "            # Exploit: choose the action with the highest Q-value\n",
        "            q_values = [self.get_q_value(state, action) for action in possible_actions]\n",
        "            max_q = -float('inf')\n",
        "            best_actions = []\n",
        "\n",
        "            for i, q_val in enumerate(q_values):\n",
        "                if q_val > max_q:\n",
        "                    max_q = q_val\n",
        "                    best_actions = [possible_actions[i]]\n",
        "                elif q_val == max_q:\n",
        "                    best_actions.append(possible_actions[i])\n",
        "\n",
        "            return random.choice(best_actions)\n",
        "\n",
        "    def learn(self, old_state_obj, action, reward, new_state_obj, new_board_obj):\n",
        "        hashable_action = tuple(action) if isinstance(action, list) else action\n",
        "\n",
        "        old_state_rep = self._get_state_representation(old_state_obj)\n",
        "        new_state_rep = self._get_state_representation(new_state_obj)\n",
        "\n",
        "        current_q = self.get_q_value(old_state_rep, hashable_action)\n",
        "\n",
        "        # Calculate max Q for the new state (Q(s', a'))\n",
        "        # Note: new_board_obj is passed directly to _get_possible_actions\n",
        "        future_possible_actions = new_board_obj.get_valid_moves(self.player) # Use new_board_obj for new_state's possible actions\n",
        "        if future_possible_actions:\n",
        "            max_future_q = max([self.get_q_value(new_state_rep, a) for a in future_possible_actions])\n",
        "        else:\n",
        "            max_future_q = 0.0 # Terminal state or no actions\n",
        "\n",
        "        # Q-learning update rule\n",
        "        self.q_table[(old_state_rep, hashable_action)] = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)\n",
        "\n",
        "    def evaluate_board(self, board_obj):\n",
        "        score = 0\n",
        "        opponent_player = 3 - self.player # If player is 1, opponent is 2, and vice versa\n",
        "\n",
        "        piece_weight = 1\n",
        "        king_weight = 2\n",
        "\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = board_obj.board[r][c]\n",
        "                owner = board_obj._get_piece_owner(piece)\n",
        "\n",
        "                if owner == self.player:\n",
        "                    if board_obj._is_king(piece):\n",
        "                        score += king_weight\n",
        "                    else:\n",
        "                        score += piece_weight\n",
        "\n",
        "                    # Positional bonus: pieces closer to enemy back rank\n",
        "                    if self.player == 1: # Moving down, higher row is better\n",
        "                        score += r * 0.1\n",
        "                    else: # Moving up, lower row is better\n",
        "                        score += (7 - r) * 0.1\n",
        "\n",
        "                    # Center control bonus (simplified) for rows 2-5 and cols 2-5\n",
        "                    if 2 <= r <= 5 and 2 <= c <= 5:\n",
        "                        score += 0.05\n",
        "\n",
        "                elif owner == opponent_player:\n",
        "                    if board_obj._is_king(piece):\n",
        "                        score -= king_weight\n",
        "                    else:\n",
        "                        score -= piece_weight\n",
        "\n",
        "                    # Positional penalty for opponent\n",
        "                    if self.player == 1: # Opponent moving up, lower row is worse for them\n",
        "                        score -= (7 - r) * 0.1\n",
        "                    else: # Opponent moving down, higher row is worse for them\n",
        "                        score -= r * 0.1\n",
        "\n",
        "                    # Opponent center control penalty\n",
        "                    if 2 <= r <= 5 and 2 <= c <= 5:\n",
        "                        score -= 0.05\n",
        "\n",
        "        return score\n",
        "\n",
        "# Reward function (can be a standalone function or part of the agent/game class)\n",
        "def get_reward(board, player, opponent_player):\n",
        "    is_over, winner = board.check_terminal_state(player)\n",
        "\n",
        "    if is_over:\n",
        "        if winner == player:\n",
        "            return 100 # Win\n",
        "        elif winner == opponent_player:\n",
        "            return -100 # Loss\n",
        "        else:\n",
        "            return -10 # Draw or other negative outcome for current player\n",
        "\n",
        "    # Non-terminal state rewards (adjust as needed)\n",
        "    # Example: slightly positive for capturing, slightly negative for losing pieces\n",
        "    return -1 # Small negative reward for each step to encourage faster wins\n",
        "\n",
        "# Example usage:\n",
        "board = CheckersBoard()\n",
        "agent_p1 = CheckersAgent(player=1)\n",
        "agent_p2 = CheckersAgent(player=2)\n",
        "\n",
        "print(\"Initial Board:\")\n",
        "print(board)\n",
        "\n",
        "# Evaluate initial board for Player 1\n",
        "score_p1_initial = agent_p1.evaluate_board(board)\n",
        "print(f\"\\nEvaluation for Player 1 (Initial Board): {score_p1_initial:.2f}\")\n",
        "\n",
        "# Evaluate initial board for Player 2\n",
        "score_p2_initial = agent_p2.evaluate_board(board)\n",
        "print(f\"Evaluation for Player 2 (Initial Board): {score_p2_initial:.2f}\")\n",
        "\n",
        "# Simulate a move and re-evaluate\n",
        "moves_p1 = board.get_valid_moves(1)\n",
        "if moves_p1:\n",
        "    first_move_p1 = moves_p1[0]\n",
        "    new_board_p1 = board.make_move(first_move_p1, 1)\n",
        "    print(\"\\nBoard after Player 1's first move:\")\n",
        "    print(new_board_p1)\n",
        "    score_p1_after_move = agent_p1.evaluate_board(new_board_p1)\n",
        "    print(f\"Evaluation for Player 1 (After Move): {score_p1_after_move:.2f}\")\n",
        "    score_p2_after_move = agent_p2.evaluate_board(new_board_p1)\n",
        "    print(f\"Evaluation for Player 2 (After Move): {score_p2_after_move:.2f}\")\n",
        "\n",
        "# Custom board state for testing captures and kings\n",
        "custom_board = CheckersBoard([[0 for _ in range(8)] for _ in range(8)])\n",
        "custom_board.board[2][1] = 1 # Player 1 piece\n",
        "custom_board.board[3][2] = 2 # Player 2 piece to be captured\n",
        "custom_board.board[4][3] = 1 # Player 1 piece (to test advanced position)\n",
        "custom_board.board[0][5] = 3 # Player 1 King\n",
        "custom_board.board[7][4] = 4 # Player 2 King\n",
        "\n",
        "print(\"\\nCustom Board:\")\n",
        "print(custom_board)\n",
        "score_p1_custom = agent_p1.evaluate_board(custom_board)\n",
        "print(f\"Evaluation for Player 1 (Custom Board): {score_p1_custom:.2f}\")\n",
        "score_p2_custom = agent_p2.evaluate_board(custom_board)\n",
        "print(f\"Evaluation for Player 2 (Custom Board): {score_p2_custom:.2f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Board:\n",
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  x  .  x  .  x  .  x \n",
            "5 | .  .  .  .  .  .  .  . \n",
            "4 | .  .  .  .  .  .  .  . \n",
            "3 | o  .  o  .  o  .  o  . \n",
            "2 | .  o  .  o  .  o  .  o \n",
            "1 | o  .  o  .  o  .  o  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n",
            "\n",
            "Evaluation for Player 1 (Initial Board): -0.00\n",
            "Evaluation for Player 2 (Initial Board): 0.00\n",
            "\n",
            "Board after Player 1's first move:\n",
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  .  .  x  .  x  .  x \n",
            "5 | x  .  .  .  .  .  .  . \n",
            "4 | .  .  .  .  .  .  .  . \n",
            "3 | o  .  o  .  o  .  o  . \n",
            "2 | .  o  .  o  .  o  .  o \n",
            "1 | o  .  o  .  o  .  o  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n",
            "Evaluation for Player 1 (After Move): 0.10\n",
            "Evaluation for Player 2 (After Move): -0.10\n",
            "\n",
            "Custom Board:\n",
            "8 | .  .  .  .  .  X  .  . \n",
            "7 | .  .  .  .  .  .  .  . \n",
            "6 | .  x  .  .  .  .  .  . \n",
            "5 | .  .  o  .  .  .  .  . \n",
            "4 | .  .  .  x  .  .  .  . \n",
            "3 | .  .  .  .  .  .  .  . \n",
            "2 | .  .  .  .  .  .  .  . \n",
            "1 | .  .  .  .  O  .  .  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n",
            "Evaluation for Player 1 (Custom Board): 1.20\n",
            "Evaluation for Player 2 (Custom Board): -1.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83c2ebf"
      },
      "source": [
        "## Implementação do Minimax com Poda Alpha-Beta\n",
        "\n",
        "### Subtask:\n",
        "Integrar o algoritmo Minimax com busca de corte (poda Alpha-Beta) para auxiliar na tomada de decisão dos agentes, utilizando a função de avaliação heurística desenvolvida.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b93ec1e"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing the Minimax algorithm with Alpha-Beta pruning. This will be done by adding a recursive `minimax` method to the `CheckersAgent` class, handling base cases, maximizing and minimizing player logic, and alpha-beta pruning. Then, a `choose_action_minimax` method will be added to leverage this minimax function for selecting the optimal move. The `CheckersBoard` class needs to be included for context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b378c5ba",
        "outputId": "ed3c434e-c895-47ec-ab17-51871cb6e15f"
      },
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "class CheckersBoard:\n",
        "    def __init__(self, board=None):\n",
        "        if board is None:\n",
        "            self.board = [[0 for _ in range(8)] for _ in range(8)]\n",
        "            # 0: empty, 1: player 1 piece, 2: player 2 piece\n",
        "            # 3: player 1 king, 4: player 2 king\n",
        "\n",
        "            # Setup initial pieces for Player 1 (top of the board, rows 0-2)\n",
        "            for row in range(3):\n",
        "                for col in range(8):\n",
        "                    if (row + col) % 2 != 0:\n",
        "                        self.board[row][col] = 1\n",
        "\n",
        "            # Setup initial pieces for Player 2 (bottom of the board, rows 5-7)\n",
        "            for row in range(5, 8):\n",
        "                for col in range(8):\n",
        "                    if (row + col) % 2 != 0:\n",
        "                        self.board[row][col] = 2\n",
        "        else:\n",
        "            self.board = [row[:] for row in board] # Deep copy of the board\n",
        "\n",
        "    def __str__(self):\n",
        "        board_str = \"\"\n",
        "        for r_idx, row in enumerate(self.board):\n",
        "            board_str += f\"{8 - r_idx} |\"\n",
        "            for piece in row:\n",
        "                if piece == 0:\n",
        "                    board_str += \" . \"\n",
        "                elif piece == 1:\n",
        "                    board_str += \" x \"\n",
        "                elif piece == 2:\n",
        "                    board_str += \" o \"\n",
        "                elif piece == 3:\n",
        "                    board_str += \" X \"\n",
        "                elif piece == 4:\n",
        "                    board_str += \" O \"\n",
        "            board_str += \"\\n\"\n",
        "        board_str += \"  ------------------------\\n\"\n",
        "        board_str += \"    a  b  c  d  e  f  g  h\"\n",
        "        return board_str\n",
        "\n",
        "    def _is_valid_coord(self, r, c):\n",
        "        return 0 <= r < 8 and 0 <= c < 8\n",
        "\n",
        "    def _get_piece_owner(self, piece):\n",
        "        if piece == 1 or piece == 3:\n",
        "            return 1 # Player 1\n",
        "        elif piece == 2 or piece == 4:\n",
        "            return 2 # Player 2\n",
        "        return 0 # Empty or invalid\n",
        "\n",
        "    def _is_king(self, piece):\n",
        "        return piece == 3 or piece == 4\n",
        "\n",
        "    def _get_possible_moves_for_piece(self, r, c, player):\n",
        "        moves = []\n",
        "        piece = self.board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can move in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        for dr, dc in directions:\n",
        "            nr, nc = r + dr, c + dc\n",
        "            if self._is_valid_coord(nr, nc) and self.board[nr][nc] == 0:\n",
        "                moves.append(((r, c), (nr, nc)))\n",
        "        return moves\n",
        "\n",
        "    def _get_possible_captures_for_piece(self, r, c, player, current_board=None, path=None):\n",
        "        if current_board is None:\n",
        "            current_board = [row[:] for row in self.board]\n",
        "        if path is None:\n",
        "            path = [(r, c)]\n",
        "\n",
        "        captures = []\n",
        "        piece = current_board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            capture_directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            capture_directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can capture in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            capture_directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        found_capture_in_this_step = False\n",
        "        for dr, dc in capture_directions:\n",
        "            # Square of the opponent's piece to be jumped over\n",
        "            enemy_r, enemy_c = r + dr, c + dc\n",
        "            # Landing square after the jump\n",
        "            land_r, land_c = r + 2 * dr, c + 2 * dc\n",
        "\n",
        "            if (self._is_valid_coord(land_r, land_c) and\n",
        "                    current_board[land_r][land_c] == 0 and\n",
        "                    self._is_valid_coord(enemy_r, enemy_c) and\n",
        "                    current_board[enemy_r][enemy_c] != 0 and\n",
        "                    self._get_piece_owner(current_board[enemy_r][enemy_c]) != player):\n",
        "\n",
        "                found_capture_in_this_step = True\n",
        "                # Simulate the capture on a temporary board\n",
        "                temp_board = [row[:] for row in current_board]\n",
        "                temp_board[land_r][land_c] = temp_board[r][c]\n",
        "                temp_board[r][c] = 0\n",
        "                temp_board[enemy_r][enemy_c] = 0\n",
        "\n",
        "                # Check for king promotion after landing, only if not king yet and reaching back row\n",
        "                promoted_piece = temp_board[land_r][land_c]\n",
        "                if not self._is_king(promoted_piece): # only promote if not already a king\n",
        "                    if (player == 1 and land_r == 7) or (player == 2 and land_r == 0): # Adjusted promotion rows\n",
        "                        temp_board[land_r][land_c] += 2 # Promote to king (1->3, 2->4)\n",
        "\n",
        "                # Recursively look for more captures from the new position\n",
        "                chained_captures = self._get_possible_captures_for_piece(land_r, land_c, player, temp_board, path + [(land_r, land_c)])\n",
        "\n",
        "                if chained_captures: # If more captures are found, add them\n",
        "                    captures.extend(chained_captures)\n",
        "                else: # If no more captures, this is a terminal capture sequence\n",
        "                    captures.append(path + [(land_r, land_c)])\n",
        "\n",
        "        # If no capture was found in this step, but it's part of a longer path, return empty to signify end of chain\n",
        "        if not found_capture_in_this_step and len(path) > 1:\n",
        "            return []\n",
        "\n",
        "        return captures\n",
        "\n",
        "    def get_valid_moves(self, player):\n",
        "        all_possible_moves = []\n",
        "        all_possible_captures = []\n",
        "\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == player:\n",
        "                    # Get basic moves\n",
        "                    all_possible_moves.extend(self._get_possible_moves_for_piece(r, c, player))\n",
        "                    # Get captures\n",
        "                    captures_from_piece = self._get_possible_captures_for_piece(r, c, player)\n",
        "                    if captures_from_piece:\n",
        "                        all_possible_captures.extend(captures_from_piece)\n",
        "\n",
        "        if all_possible_captures:\n",
        "            # If captures are available, they are mandatory\n",
        "            return all_possible_captures\n",
        "        else:\n",
        "            # Otherwise, return all basic moves\n",
        "            return all_possible_moves\n",
        "\n",
        "    def make_move(self, move, player):\n",
        "        # Create a new board state for the move\n",
        "        new_board = CheckersBoard(board=self.board)\n",
        "\n",
        "        if isinstance(move[0], tuple) and len(move) == 2 and isinstance(move[1], tuple): # It's a simple move ((r1,c1), (r2,c2))\n",
        "            start_r, start_c = move[0]\n",
        "            end_r, end_c = move[1]\n",
        "            piece_to_move = new_board.board[start_r][start_c]\n",
        "\n",
        "            new_board.board[end_r][end_c] = piece_to_move\n",
        "            new_board.board[start_r][start_c] = 0\n",
        "\n",
        "            # Check for king promotion\n",
        "            if not new_board._is_king(piece_to_move): # Only promote if not already a king\n",
        "                if (player == 1 and end_r == 7) or (player == 2 and end_r == 0):\n",
        "                    new_board.board[end_r][end_c] += 2 # Promote to king\n",
        "\n",
        "        elif isinstance(move, list): # It's a capture path [(r1,c1), (r2,c2), ...]\n",
        "            for i in range(len(move) - 1):\n",
        "                start_r, start_c = move[i]\n",
        "                end_r, end_c = move[i+1]\n",
        "                piece_to_move = new_board.board[start_r][start_c]\n",
        "\n",
        "                new_board.board[end_r][end_c] = piece_to_move\n",
        "                new_board.board[start_r][start_c] = 0\n",
        "\n",
        "                # Remove captured piece\n",
        "                # Calculate the position of the captured piece (midpoint)\n",
        "                captured_r = (start_r + end_r) // 2\n",
        "                captured_c = (start_c + end_c) // 2\n",
        "                new_board.board[captured_r][captured_c] = 0\n",
        "\n",
        "            # After the whole capture path, check for king promotion at the final landing spot\n",
        "            final_r, final_c = move[-1]\n",
        "            promoted_piece = new_board.board[final_r][final_c]\n",
        "            if not new_board._is_king(promoted_piece): # Only promote if not already a king\n",
        "                if (player == 1 and final_r == 7) or (player == 2 and final_r == 0):\n",
        "                    new_board.board[final_r][final_c] += 2\n",
        "\n",
        "        return new_board\n",
        "\n",
        "    def is_game_over(self):\n",
        "        player1_pieces = 0\n",
        "        player2_pieces = 0\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == 1:\n",
        "                    player1_pieces += 1\n",
        "                elif self._get_piece_owner(piece) == 2:\n",
        "                    player2_pieces += 1\n",
        "\n",
        "        # Check for no pieces left\n",
        "        if player1_pieces == 0: return True, 2 # Player 1 has no pieces, Player 2 wins\n",
        "        if player2_pieces == 0: return True, 1 # Player 2 has no pieces, Player 1 wins\n",
        "\n",
        "        return False, None\n",
        "\n",
        "    def _has_valid_moves(self, player):\n",
        "        return bool(self.get_valid_moves(player))\n",
        "\n",
        "    def check_terminal_state(self, current_player):\n",
        "        player1_pieces = 0\n",
        "        player2_pieces = 0\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == 1:\n",
        "                    player1_pieces += 1\n",
        "                elif self._get_piece_owner(piece) == 2:\n",
        "                    player2_pieces += 1\n",
        "\n",
        "        # Condition 1: No pieces left\n",
        "        if player1_pieces == 0: return True, 2 # Player 2 wins\n",
        "        if player2_pieces == 0: return True, 1 # Player 1 wins\n",
        "\n",
        "        # Condition 2: Current player has no valid moves\n",
        "        if not self._has_valid_moves(current_player):\n",
        "            if current_player == 1: return True, 2 # Player 1 has no moves, Player 2 wins\n",
        "            if current_player == 2: return True, 1 # Player 2 has no moves, Player 1 wins\n",
        "\n",
        "        # Default: Game is not over\n",
        "        return False, None\n",
        "\n",
        "class CheckersAgent:\n",
        "    def __init__(self, player, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.player = player # 1 for player 1, 2 for player 2\n",
        "        self.alpha = alpha # Learning rate\n",
        "        self.gamma = gamma # Discount factor\n",
        "        self.epsilon = epsilon # Exploration rate\n",
        "        self.q_table = {}\n",
        "\n",
        "    def _get_state_representation(self, board):\n",
        "        # Convert the 2D list board into a tuple of tuples for hashability\n",
        "        return tuple(tuple(row) for row in board.board)\n",
        "\n",
        "    def _get_possible_actions(self, board):\n",
        "        # Actions are the valid moves obtained from the CheckersBoard class\n",
        "        return board.get_valid_moves(self.player)\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        # Get the Q-value for a given state-action pair, default to 0 if not seen\n",
        "        # Actions can be tuples (for simple moves) or lists of tuples (for capture paths)\n",
        "        # Convert action to a hashable format if it's a list\n",
        "        hashable_action = tuple(action) if isinstance(action, list) else action\n",
        "        return self.q_table.get((state, hashable_action), 0.0)\n",
        "\n",
        "    def choose_action(self, board):\n",
        "        state = self._get_state_representation(board)\n",
        "        possible_actions = self._get_possible_actions(board)\n",
        "\n",
        "        if not possible_actions:\n",
        "            return None # No valid moves\n",
        "\n",
        "        # Epsilon-greedy strategy\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            # Explore: choose a random action\n",
        "            return random.choice(possible_actions)\n",
        "        else:\n",
        "            # Exploit: choose the action with the highest Q-value\n",
        "            q_values = [self.get_q_value(state, action) for action in possible_actions]\n",
        "            max_q = -float('inf')\n",
        "            best_actions = []\n",
        "\n",
        "            for i, q_val in enumerate(q_values):\n",
        "                if q_val > max_q:\n",
        "                    max_q = q_val\n",
        "                    best_actions = [possible_actions[i]]\n",
        "                elif q_val == max_q:\n",
        "                    best_actions.append(possible_actions[i])\n",
        "\n",
        "            return random.choice(best_actions)\n",
        "\n",
        "    def learn(self, old_state_obj, action, reward, new_state_obj, new_board_obj):\n",
        "        hashable_action = tuple(action) if isinstance(action, list) else action\n",
        "\n",
        "        old_state_rep = self._get_state_representation(old_state_obj)\n",
        "        new_state_rep = self._get_state_representation(new_state_obj)\n",
        "\n",
        "        current_q = self.get_q_value(old_state_rep, hashable_action)\n",
        "\n",
        "        # Calculate max Q for the new state (Q(s', a'))\n",
        "        # Note: new_board_obj is passed directly to _get_possible_actions\n",
        "        future_possible_actions = new_board_obj.get_valid_moves(self.player) # Use new_board_obj for new_state's possible actions\n",
        "        if future_possible_actions:\n",
        "            max_future_q = max([self.get_q_value(new_state_rep, a) for a in future_possible_actions])\n",
        "        else:\n",
        "            max_future_q = 0.0 # Terminal state or no actions\n",
        "\n",
        "        # Q-learning update rule\n",
        "        self.q_table[(old_state_rep, hashable_action)] = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)\n",
        "\n",
        "    def evaluate_board(self, board_obj):\n",
        "        score = 0\n",
        "        opponent_player = 3 - self.player # If player is 1, opponent is 2, and vice versa\n",
        "\n",
        "        piece_weight = 1\n",
        "        king_weight = 2\n",
        "\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = board_obj.board[r][c]\n",
        "                owner = board_obj._get_piece_owner(piece)\n",
        "\n",
        "                if owner == self.player:\n",
        "                    if board_obj._is_king(piece):\n",
        "                        score += king_weight\n",
        "                    else:\n",
        "                        score += piece_weight\n",
        "\n",
        "                    # Positional bonus: pieces closer to enemy back rank\n",
        "                    if self.player == 1: # Moving down, higher row is better\n",
        "                        score += r * 0.1\n",
        "                    else: # Moving up, lower row is better\n",
        "                        score += (7 - r) * 0.1\n",
        "\n",
        "                    # Center control bonus (simplified) for rows 2-5 and cols 2-5\n",
        "                    if 2 <= r <= 5 and 2 <= c <= 5:\n",
        "                        score += 0.05\n",
        "\n",
        "                elif owner == opponent_player:\n",
        "                    if board_obj._is_king(piece):\n",
        "                        score -= king_weight\n",
        "                    else:\n",
        "                        score -= piece_weight\n",
        "\n",
        "                    # Positional penalty for opponent\n",
        "                    if self.player == 1: # Opponent moving up, lower row is worse for them\n",
        "                        score -= (7 - r) * 0.1\n",
        "                    else: # Opponent moving down, higher row is worse for them\n",
        "                        score -= r * 0.1\n",
        "\n",
        "                    # Opponent center control penalty\n",
        "                    if 2 <= r <= 5 and 2 <= c <= 5:\n",
        "                        score -= 0.05\n",
        "\n",
        "        return score\n",
        "\n",
        "    def minimax(self, board_obj, depth, alpha, beta, maximizing_player):\n",
        "        current_player_in_minimax = self.player if maximizing_player else (3 - self.player)\n",
        "        is_over, winner = board_obj.check_terminal_state(current_player_in_minimax)\n",
        "\n",
        "        if depth == 0 or is_over:\n",
        "            if is_over:\n",
        "                if winner == self.player: # If our agent wins\n",
        "                    return 1000000000 + depth # Large positive value for win\n",
        "                elif winner == (3 - self.player): # If opponent wins\n",
        "                    return -1000000000 - depth # Large negative value for loss\n",
        "                else: # Draw, very unlikely with current terminal state checks\n",
        "                    return 0\n",
        "            return self.evaluate_board(board_obj)\n",
        "\n",
        "        if maximizing_player:\n",
        "            max_eval = -math.inf\n",
        "            valid_moves = board_obj.get_valid_moves(self.player)\n",
        "            if not valid_moves: # If no moves, consider it a loss for this player's turn\n",
        "                return -1000000000 - depth # Opponent wins\n",
        "            for move in valid_moves:\n",
        "                new_board = board_obj.make_move(move, self.player)\n",
        "                eval = self.minimax(new_board, depth - 1, alpha, beta, False)\n",
        "                max_eval = max(max_eval, eval)\n",
        "                alpha = max(alpha, eval)\n",
        "                if beta <= alpha:\n",
        "                    break # Alpha Cut-off\n",
        "            return max_eval\n",
        "        else:\n",
        "            min_eval = math.inf\n",
        "            opponent_player = 3 - self.player\n",
        "            valid_moves = board_obj.get_valid_moves(opponent_player)\n",
        "            if not valid_moves: # If no moves, consider it a win for this player's turn\n",
        "                return 1000000000 + depth # Current player wins (as opponent can't move)\n",
        "            for move in valid_moves:\n",
        "                new_board = board_obj.make_move(move, opponent_player)\n",
        "                eval = self.minimax(new_board, depth - 1, alpha, beta, True)\n",
        "                min_eval = min(min_eval, eval)\n",
        "                beta = min(beta, eval)\n",
        "                if beta <= alpha:\n",
        "                    break # Beta Cut-off\n",
        "            return min_eval\n",
        "\n",
        "    def choose_action_minimax(self, board_obj, depth=3):\n",
        "        possible_actions = board_obj.get_valid_moves(self.player)\n",
        "\n",
        "        if not possible_actions:\n",
        "            return None # No valid moves\n",
        "\n",
        "        # If only one action, take it without further calculation\n",
        "        if len(possible_actions) == 1:\n",
        "            return possible_actions[0]\n",
        "\n",
        "        best_eval = -math.inf\n",
        "        best_move = None\n",
        "        alpha = -math.inf\n",
        "        beta = math.inf\n",
        "\n",
        "        for move in possible_actions:\n",
        "            new_board = board_obj.make_move(move, self.player)\n",
        "            # Call minimax for the next state, assuming opponent will play optimally (minimizing player)\n",
        "            eval = self.minimax(new_board, depth - 1, alpha, beta, False) # After my move, it's opponent's turn, so False (minimizing)\n",
        "\n",
        "            if eval > best_eval:\n",
        "                best_eval = eval\n",
        "                best_move = move\n",
        "            alpha = max(alpha, eval) # Update alpha based on maximizing player's perspective\n",
        "\n",
        "        return best_move\n",
        "\n",
        "# Reward function (can be a standalone function or part of the agent/game class)\n",
        "def get_reward(board, player, opponent_player):\n",
        "    is_over, winner = board.check_terminal_state(player)\n",
        "\n",
        "    if is_over:\n",
        "        if winner == player:\n",
        "            return 100 # Win\n",
        "        elif winner == opponent_player:\n",
        "            return -100 # Loss\n",
        "        else:\n",
        "            return -10 # Draw or other negative outcome for current player\n",
        "\n",
        "    # Non-terminal state rewards (adjust as needed)\n",
        "    # Example: slightly positive for capturing, slightly negative for losing pieces\n",
        "    return -1 # Small negative reward for each step to encourage faster wins\n",
        "\n",
        "# Example usage:\n",
        "board = CheckersBoard()\n",
        "agent_p1 = CheckersAgent(player=1)\n",
        "agent_p2 = CheckersAgent(player=2)\n",
        "\n",
        "print(\"Initial Board:\")\n",
        "print(board)\n",
        "\n",
        "print(\"\\nPlayer 1 (Minimax) choosing action...\")\n",
        "minimax_move_p1 = agent_p1.choose_action_minimax(board, depth=3)\n",
        "\n",
        "if minimax_move_p1:\n",
        "    print(f\"Player 1 chose move: {minimax_move_p1}\")\n",
        "    new_board_minimax_p1 = board.make_move(minimax_move_p1, 1)\n",
        "    print(\"Board after Player 1's Minimax move:\")\n",
        "    print(new_board_minimax_p1)\n",
        "else:\n",
        "    print(\"Player 1 (Minimax) has no moves.\")\n",
        "\n",
        "# To test Player 2's minimax move, we would need to simulate the game flow\n",
        "# For simplicity, let's show P2 choosing a move on the original board\n",
        "print(\"\\nPlayer 2 (Minimax) choosing action on initial board (for demonstration)...\")\n",
        "minimax_move_p2 = agent_p2.choose_action_minimax(board, depth=3)\n",
        "if minimax_move_p2:\n",
        "    print(f\"Player 2 chose move: {minimax_move_p2}\")\n",
        "    new_board_minimax_p2 = board.make_move(minimax_move_p2, 2)\n",
        "    print(\"Board after Player 2's Minimax move:\")\n",
        "    print(new_board_minimax_p2)\n",
        "else:\n",
        "    print(\"Player 2 (Minimax) has no moves.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Board:\n",
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  x  .  x  .  x  .  x \n",
            "5 | .  .  .  .  .  .  .  . \n",
            "4 | .  .  .  .  .  .  .  . \n",
            "3 | o  .  o  .  o  .  o  . \n",
            "2 | .  o  .  o  .  o  .  o \n",
            "1 | o  .  o  .  o  .  o  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n",
            "\n",
            "Player 1 (Minimax) choosing action...\n",
            "Player 1 chose move: ((2, 1), (3, 2))\n",
            "Board after Player 1's Minimax move:\n",
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  .  .  x  .  x  .  x \n",
            "5 | .  .  x  .  .  .  .  . \n",
            "4 | .  .  .  .  .  .  .  . \n",
            "3 | o  .  o  .  o  .  o  . \n",
            "2 | .  o  .  o  .  o  .  o \n",
            "1 | o  .  o  .  o  .  o  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n",
            "\n",
            "Player 2 (Minimax) choosing action on initial board (for demonstration)...\n",
            "Player 2 chose move: ((5, 0), (4, 1))\n",
            "Board after Player 2's Minimax move:\n",
            "8 | .  x  .  x  .  x  .  x \n",
            "7 | x  .  x  .  x  .  x  . \n",
            "6 | .  x  .  x  .  x  .  x \n",
            "5 | .  .  .  .  .  .  .  . \n",
            "4 | .  o  .  .  .  .  .  . \n",
            "3 | .  .  o  .  o  .  o  . \n",
            "2 | .  o  .  o  .  o  .  o \n",
            "1 | o  .  o  .  o  .  o  . \n",
            "  ------------------------\n",
            "    a  b  c  d  e  f  g  h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90f5b5e0"
      },
      "source": [
        "## Treinamento e Integração dos Agentes\n",
        "\n",
        "### Subtask:\n",
        "Configurar o loop de treinamento onde os dois agentes (cada um utilizando Q-learning e Minimax) jogam um contra o outro. O Q-learning ajustará os Q-valores com base nas experiências de jogo e nas recompensas, enquanto o Minimax guiará a escolha de movimentos com base na heurística aprendida.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b5a9c03"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing the training loop. This step defines the `train_agents` function that orchestrates the game between a Q-learning agent and a Minimax agent over multiple episodes, handling turns, moves, Q-value updates, and exploration-exploitation decay.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88fa14f7",
        "outputId": "e1d6af1f-6418-4100-b12c-434e98b8692e"
      },
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "class CheckersBoard:\n",
        "    def __init__(self, board=None):\n",
        "        if board is None:\n",
        "            self.board = [[0 for _ in range(8)] for _ in range(8)]\n",
        "            # 0: empty, 1: player 1 piece, 2: player 2 piece\n",
        "            # 3: player 1 king, 4: player 2 king\n",
        "\n",
        "            # Setup initial pieces for Player 1 (top of the board, rows 0-2)\n",
        "            for row in range(3):\n",
        "                for col in range(8):\n",
        "                    if (row + col) % 2 != 0:\n",
        "                        self.board[row][col] = 1\n",
        "\n",
        "            # Setup initial pieces for Player 2 (bottom of the board, rows 5-7)\n",
        "            for row in range(5, 8):\n",
        "                for col in range(8):\n",
        "                    if (row + col) % 2 != 0:\n",
        "                        self.board[row][col] = 2\n",
        "        else:\n",
        "            self.board = [row[:] for row in board] # Deep copy of the board\n",
        "\n",
        "    def __str__(self):\n",
        "        board_str = \"\"\n",
        "        for r_idx, row in enumerate(self.board):\n",
        "            board_str += f\"{8 - r_idx} |\"\n",
        "            for piece in row:\n",
        "                if piece == 0:\n",
        "                    board_str += \" . \"\n",
        "                elif piece == 1:\n",
        "                    board_str += \" x \"\n",
        "                elif piece == 2:\n",
        "                    board_str += \" o \"\n",
        "                elif piece == 3:\n",
        "                    board_str += \" X \"\n",
        "                elif piece == 4:\n",
        "                    board_str += \" O \"\n",
        "            board_str += \"\\n\"\n",
        "        board_str += \"  ------------------------\\n\"\n",
        "        board_str += \"    a  b  c  d  e  f  g  h\"\n",
        "        return board_str\n",
        "\n",
        "    def _is_valid_coord(self, r, c):\n",
        "        return 0 <= r < 8 and 0 <= c < 8\n",
        "\n",
        "    def _get_piece_owner(self, piece):\n",
        "        if piece == 1 or piece == 3:\n",
        "            return 1 # Player 1\n",
        "        elif piece == 2 or piece == 4:\n",
        "            return 2 # Player 2\n",
        "        return 0 # Empty or invalid\n",
        "\n",
        "    def _is_king(self, piece):\n",
        "        return piece == 3 or piece == 4\n",
        "\n",
        "    def _get_possible_moves_for_piece(self, r, c, player):\n",
        "        moves = []\n",
        "        piece = self.board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can move in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        for dr, dc in directions:\n",
        "            nr, nc = r + dr, c + dc\n",
        "            if self._is_valid_coord(nr, nc) and self.board[nr][nc] == 0:\n",
        "                moves.append(((r, c), (nr, nc)))\n",
        "        return moves\n",
        "\n",
        "    def _get_possible_captures_for_piece(self, r, c, player, current_board=None, path=None):\n",
        "        if current_board is None:\n",
        "            current_board = [row[:] for row in self.board]\n",
        "        if path is None:\n",
        "            path = [(r, c)]\n",
        "\n",
        "        captures = []\n",
        "        piece = current_board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            capture_directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            capture_directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can capture in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            capture_directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        found_capture_in_this_step = False\n",
        "        for dr, dc in capture_directions:\n",
        "            # Square of the opponent's piece to be jumped over\n",
        "            enemy_r, enemy_c = r + dr, c + dc\n",
        "            # Landing square after the jump\n",
        "            land_r, land_c = r + 2 * dr, c + 2 * dc\n",
        "\n",
        "            if (self._is_valid_coord(land_r, land_c) and\n",
        "                    current_board[land_r][land_c] == 0 and\n",
        "                    self._is_valid_coord(enemy_r, enemy_c) and\n",
        "                    current_board[enemy_r][enemy_c] != 0 and\n",
        "                    self._get_piece_owner(current_board[enemy_r][enemy_c]) != player):\n",
        "\n",
        "                found_capture_in_this_step = True\n",
        "                # Simulate the capture on a temporary board\n",
        "                temp_board = [row[:] for row in current_board]\n",
        "                temp_board[land_r][land_c] = temp_board[r][c]\n",
        "                temp_board[r][c] = 0\n",
        "                temp_board[enemy_r][enemy_c] = 0\n",
        "\n",
        "                # Check for king promotion after landing, only if not king yet and reaching back row\n",
        "                promoted_piece = temp_board[land_r][land_c]\n",
        "                if not self._is_king(promoted_piece): # only promote if not already a king\n",
        "                    if (player == 1 and land_r == 7) or (player == 2 and land_r == 0): # Adjusted promotion rows\n",
        "                        temp_board[land_r][land_c] += 2 # Promote to king (1->3, 2->4)\n",
        "\n",
        "                # Recursively look for more captures from the new position\n",
        "                chained_captures = self._get_possible_captures_for_piece(land_r, land_c, player, temp_board, path + [(land_r, land_c)])\n",
        "\n",
        "                if chained_captures: # If more captures are found, add them\n",
        "                    captures.extend(chained_captures)\n",
        "                else: # If no more captures, this is a terminal capture sequence\n",
        "                    captures.append(path + [(land_r, land_c)])\n",
        "\n",
        "        # If no capture was found in this step, but it's part of a longer path, return empty to signify end of chain\n",
        "        if not found_capture_in_this_step and len(path) > 1:\n",
        "            return []\n",
        "\n",
        "        return captures\n",
        "\n",
        "    def get_valid_moves(self, player):\n",
        "        all_possible_moves = []\n",
        "        all_possible_captures = []\n",
        "\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == player:\n",
        "                    # Get basic moves\n",
        "                    all_possible_moves.extend(self._get_possible_moves_for_piece(r, c, player))\n",
        "                    # Get captures\n",
        "                    captures_from_piece = self._get_possible_captures_for_piece(r, c, player)\n",
        "                    if captures_from_piece:\n",
        "                        all_possible_captures.extend(captures_from_piece)\n",
        "\n",
        "        if all_possible_captures:\n",
        "            # If captures are available, they are mandatory\n",
        "            return all_possible_captures\n",
        "        else:\n",
        "            # Otherwise, return all basic moves\n",
        "            return all_possible_moves\n",
        "\n",
        "    def make_move(self, move, player):\n",
        "        # Create a new board state for the move\n",
        "        new_board = CheckersBoard(board=self.board)\n",
        "\n",
        "        if isinstance(move[0], tuple) and len(move) == 2 and isinstance(move[1], tuple): # It's a simple move ((r1,c1), (r2,c2))\n",
        "            start_r, start_c = move[0]\n",
        "            end_r, end_c = move[1]\n",
        "            piece_to_move = new_board.board[start_r][start_c]\n",
        "\n",
        "            new_board.board[end_r][end_c] = piece_to_move\n",
        "            new_board.board[start_r][start_c] = 0\n",
        "\n",
        "            # Check for king promotion\n",
        "            if not new_board._is_king(piece_to_move): # Only promote if not already a king\n",
        "                if (player == 1 and end_r == 7) or (player == 2 and end_r == 0):\n",
        "                    new_board.board[end_r][end_c] += 2 # Promote to king\n",
        "\n",
        "        elif isinstance(move, list): # It's a capture path [(r1,c1), (r2,c2), ...]\n",
        "            for i in range(len(move) - 1):\n",
        "                start_r, start_c = move[i]\n",
        "                end_r, end_c = move[i+1]\n",
        "                piece_to_move = new_board.board[start_r][start_c]\n",
        "\n",
        "                new_board.board[end_r][end_c] = piece_to_move\n",
        "                new_board.board[start_r][start_c] = 0\n",
        "\n",
        "                # Remove captured piece\n",
        "                # Calculate the position of the captured piece (midpoint)\n",
        "                captured_r = (start_r + end_r) // 2\n",
        "                captured_c = (start_c + end_c) // 2\n",
        "                new_board.board[captured_r][captured_c] = 0\n",
        "\n",
        "            # After the whole capture path, check for king promotion at the final landing spot\n",
        "            final_r, final_c = move[-1]\n",
        "            promoted_piece = new_board.board[final_r][final_c]\n",
        "            if not new_board._is_king(promoted_piece): # Only promote if not already a king\n",
        "                if (player == 1 and final_r == 7) or (player == 2 and final_r == 0):\n",
        "                    new_board.board[final_r][final_c] += 2\n",
        "\n",
        "        return new_board\n",
        "\n",
        "    def is_game_over(self):\n",
        "        player1_pieces = 0\n",
        "        player2_pieces = 0\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == 1:\n",
        "                    player1_pieces += 1\n",
        "                elif self._get_piece_owner(piece) == 2:\n",
        "                    player2_pieces += 1\n",
        "\n",
        "        # Check for no pieces left\n",
        "        if player1_pieces == 0: return True, 2 # Player 1 has no pieces, Player 2 wins\n",
        "        if player2_pieces == 0: return True, 1 # Player 2 has no pieces, Player 1 wins\n",
        "\n",
        "        return False, None\n",
        "\n",
        "    def _has_valid_moves(self, player):\n",
        "        return bool(self.get_valid_moves(player))\n",
        "\n",
        "    def check_terminal_state(self, current_player):\n",
        "        player1_pieces = 0\n",
        "        player2_pieces = 0\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == 1:\n",
        "                    player1_pieces += 1\n",
        "                elif self._get_piece_owner(piece) == 2:\n",
        "                    player2_pieces += 1\n",
        "\n",
        "        # Condition 1: No pieces left\n",
        "        if player1_pieces == 0: return True, 2 # Player 2 wins\n",
        "        if player2_pieces == 0: return True, 1 # Player 1 wins\n",
        "\n",
        "        # Condition 2: Current player has no valid moves\n",
        "        if not self._has_valid_moves(current_player):\n",
        "            if current_player == 1: return True, 2 # Player 1 has no moves, Player 2 wins\n",
        "            if current_player == 2: return True, 1 # Player 2 has no moves, Player 1 wins\n",
        "\n",
        "        # Default: Game is not over\n",
        "        return False, None\n",
        "\n",
        "class CheckersAgent:\n",
        "    def __init__(self, player, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.player = player # 1 for player 1, 2 for player 2\n",
        "        self.alpha = alpha # Learning rate\n",
        "        self.gamma = gamma # Discount factor\n",
        "        self.epsilon = epsilon # Exploration rate\n",
        "        self.q_table = {}\n",
        "\n",
        "    def _get_state_representation(self, board):\n",
        "        # Convert the 2D list board into a tuple of tuples for hashability\n",
        "        return tuple(tuple(row) for row in board.board)\n",
        "\n",
        "    def _get_possible_actions(self, board):\n",
        "        # Actions are the valid moves obtained from the CheckersBoard class\n",
        "        return board.get_valid_moves(self.player)\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        # Get the Q-value for a given state-action pair, default to 0 if not seen\n",
        "        # Actions can be tuples (for simple moves) or lists of tuples (for capture paths)\n",
        "        # Convert action to a hashable format if it's a list\n",
        "        hashable_action = tuple(action) if isinstance(action, list) else action\n",
        "        return self.q_table.get((state, hashable_action), 0.0)\n",
        "\n",
        "    def choose_action(self, board):\n",
        "        state = self._get_state_representation(board)\n",
        "        possible_actions = self._get_possible_actions(board)\n",
        "\n",
        "        if not possible_actions:\n",
        "            return None # No valid moves\n",
        "\n",
        "        # Epsilon-greedy strategy\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            # Explore: choose a random action\n",
        "            return random.choice(possible_actions)\n",
        "        else:\n",
        "            # Exploit: choose the action with the highest Q-value\n",
        "            q_values = [self.get_q_value(state, action) for action in possible_actions]\n",
        "            max_q = -float('inf')\n",
        "            best_actions = []\n",
        "\n",
        "            for i, q_val in enumerate(q_values):\n",
        "                if q_val > max_q:\n",
        "                    max_q = q_val\n",
        "                    best_actions = [possible_actions[i]]\n",
        "                elif q_val == max_q:\n",
        "                    best_actions.append(possible_actions[i])\n",
        "\n",
        "            return random.choice(best_actions)\n",
        "\n",
        "    def learn(self, old_state_obj, action, reward, new_state_obj, new_board_obj):\n",
        "        hashable_action = tuple(action) if isinstance(action, list) else action\n",
        "\n",
        "        old_state_rep = self._get_state_representation(old_state_obj)\n",
        "        new_state_rep = self._get_state_representation(new_state_obj)\n",
        "\n",
        "        current_q = self.get_q_value(old_state_rep, hashable_action)\n",
        "\n",
        "        # Calculate max Q for the new state (Q(s', a'))\n",
        "        # Note: new_board_obj is passed directly to _get_possible_actions\n",
        "        future_possible_actions = new_board_obj.get_valid_moves(self.player) # Use new_board_obj for new_state's possible actions\n",
        "        if future_possible_actions:\n",
        "            max_future_q = max([self.get_q_value(new_state_rep, a) for a in future_possible_actions])\n",
        "        else:\n",
        "            max_future_q = 0.0 # Terminal state or no actions\n",
        "\n",
        "        # Q-learning update rule\n",
        "        self.q_table[(old_state_rep, hashable_action)] = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)\n",
        "\n",
        "    def evaluate_board(self, board_obj):\n",
        "        score = 0\n",
        "        opponent_player = 3 - self.player # If player is 1, opponent is 2, and vice versa\n",
        "\n",
        "        piece_weight = 1\n",
        "        king_weight = 2\n",
        "\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = board_obj.board[r][c]\n",
        "                owner = board_obj._get_piece_owner(piece)\n",
        "\n",
        "                if owner == self.player:\n",
        "                    if board_obj._is_king(piece):\n",
        "                        score += king_weight\n",
        "                    else:\n",
        "                        score += piece_weight\n",
        "\n",
        "                    # Positional bonus: pieces closer to enemy back rank\n",
        "                    if self.player == 1: # Moving down, higher row is better\n",
        "                        score += r * 0.1\n",
        "                    else: # Moving up, lower row is better\n",
        "                        score += (7 - r) * 0.1\n",
        "\n",
        "                    # Center control bonus (simplified) for rows 2-5 and cols 2-5\n",
        "                    if 2 <= r <= 5 and 2 <= c <= 5:\n",
        "                        score += 0.05\n",
        "\n",
        "                elif owner == opponent_player:\n",
        "                    if board_obj._is_king(piece):\n",
        "                        score -= king_weight\n",
        "                    else:\n",
        "                        score -= piece_weight\n",
        "\n",
        "                    # Positional penalty for opponent\n",
        "                    if self.player == 1: # Opponent moving up, lower row is worse for them\n",
        "                        score -= (7 - r) * 0.1\n",
        "                    else: # Opponent moving down, higher row is worse for them\n",
        "                        score -= r * 0.1\n",
        "\n",
        "                    # Opponent center control penalty\n",
        "                    if 2 <= r <= 5 and 2 <= c <= 5:\n",
        "                        score -= 0.05\n",
        "\n",
        "        return score\n",
        "\n",
        "    def minimax(self, board_obj, depth, alpha, beta, maximizing_player):\n",
        "        current_player_in_minimax = self.player if maximizing_player else (3 - self.player)\n",
        "        is_over, winner = board_obj.check_terminal_state(current_player_in_minimax)\n",
        "\n",
        "        if depth == 0 or is_over:\n",
        "            if is_over:\n",
        "                if winner == self.player: # If our agent wins\n",
        "                    return 1000000000 + depth # Large positive value for win\n",
        "                elif winner == (3 - self.player): # If opponent wins\n",
        "                    return -1000000000 - depth # Large negative value for loss\n",
        "                else: # Draw, very unlikely with current terminal state checks\n",
        "                    return 0\n",
        "            return self.evaluate_board(board_obj)\n",
        "\n",
        "        if maximizing_player:\n",
        "            max_eval = -math.inf\n",
        "            valid_moves = board_obj.get_valid_moves(self.player)\n",
        "            if not valid_moves: # If no moves, consider it a loss for this player's turn\n",
        "                return -1000000000 - depth # Opponent wins\n",
        "            for move in valid_moves:\n",
        "                new_board = board_obj.make_move(move, self.player)\n",
        "                eval = self.minimax(new_board, depth - 1, alpha, beta, False)\n",
        "                max_eval = max(max_eval, eval)\n",
        "                alpha = max(alpha, eval)\n",
        "                if beta <= alpha:\n",
        "                    break # Alpha Cut-off\n",
        "            return max_eval\n",
        "        else:\n",
        "            min_eval = math.inf\n",
        "            opponent_player = 3 - self.player\n",
        "            valid_moves = board_obj.get_valid_moves(opponent_player)\n",
        "            if not valid_moves: # If no moves, consider it a win for this player's turn\n",
        "                return 1000000000 + depth # Current player wins (as opponent can't move)\n",
        "            for move in valid_moves:\n",
        "                new_board = board_obj.make_move(move, opponent_player)\n",
        "                eval = self.minimax(new_board, depth - 1, alpha, beta, True)\n",
        "                min_eval = min(min_eval, eval)\n",
        "                beta = min(beta, eval)\n",
        "                if beta <= alpha:\n",
        "                    break # Beta Cut-off\n",
        "            return min_eval\n",
        "\n",
        "    def choose_action_minimax(self, board_obj, depth=3):\n",
        "        possible_actions = board_obj.get_valid_moves(self.player)\n",
        "\n",
        "        if not possible_actions:\n",
        "            return None # No valid moves\n",
        "\n",
        "        # If only one action, take it without further calculation\n",
        "        if len(possible_actions) == 1:\n",
        "            return possible_actions[0]\n",
        "\n",
        "        best_eval = -math.inf\n",
        "        best_move = None\n",
        "        alpha = -math.inf\n",
        "        beta = math.inf\n",
        "\n",
        "        for move in possible_actions:\n",
        "            new_board = board_obj.make_move(move, self.player)\n",
        "            # Call minimax for the next state, assuming opponent will play optimally (minimizing player)\n",
        "            eval = self.minimax(new_board, depth - 1, alpha, beta, False) # After my move, it's opponent's turn, so False (minimizing)\n",
        "\n",
        "            if eval > best_eval:\n",
        "                best_eval = eval\n",
        "                best_move = move\n",
        "            alpha = max(alpha, eval) # Update alpha based on maximizing player's perspective\n",
        "\n",
        "        return best_move\n",
        "\n",
        "# Reward function (can be a standalone function or part of the agent/game class)\n",
        "def get_reward(board, player, opponent_player):\n",
        "    is_over, winner = board.check_terminal_state(player)\n",
        "\n",
        "    if is_over:\n",
        "        if winner == player:\n",
        "            return 100 # Win\n",
        "        elif winner == opponent_player:\n",
        "            return -100 # Loss\n",
        "        else:\n",
        "            # This case covers draws or scenarios where a player cannot move.\n",
        "            # If `check_terminal_state` returns a winner for `opponent_player` because `player` had no moves,\n",
        "            # that's already covered by the -100 case. If it's a true draw, then it's a neutral outcome.\n",
        "            # For now, let's keep it simple; a 'draw' is less bad than a loss.\n",
        "            return -5 # Default for other terminal states (e.g., if neither can move, or stalemate type)\n",
        "\n",
        "    # Non-terminal state rewards (adjust as needed)\n",
        "    # Example: slightly positive for capturing, slightly negative for losing pieces\n",
        "    return -1 # Small negative reward for each step to encourage faster wins\n",
        "\n",
        "def train_agents(num_episodes, q_agent_player, minimax_agent_player, minimax_depth, epsilon_decay_rate, max_moves_per_game):\n",
        "    # Initialize agents\n",
        "    q_agent = CheckersAgent(player=q_agent_player, epsilon=1.0) # Start with high exploration\n",
        "    minimax_agent = CheckersAgent(player=minimax_agent_player, epsilon=0.0) # Minimax is always greedy\n",
        "\n",
        "    # Track wins for analysis\n",
        "    q_wins = 0\n",
        "    minimax_wins = 0\n",
        "    draws = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        board = CheckersBoard() # New board for each episode\n",
        "        current_player = 1 # Start with player 1\n",
        "        game_over = False\n",
        "        move_count = 0\n",
        "\n",
        "        last_q_agent_state = None\n",
        "        last_q_agent_action = None\n",
        "\n",
        "        # Store historical boards to detect draws by repetition (simple check)\n",
        "        board_history = []\n",
        "\n",
        "        while not game_over and move_count < max_moves_per_game:\n",
        "            board_rep = tuple(tuple(row) for row in board.board)\n",
        "            if board_rep in board_history:\n",
        "                # Draw by repetition\n",
        "                game_over = True\n",
        "                draws += 1\n",
        "                # Give Q-agent a negative reward for drawing\n",
        "                if last_q_agent_state and last_q_agent_action:\n",
        "                    q_agent.learn(last_q_agent_state, last_q_agent_action, -10, board, board)\n",
        "                break\n",
        "            board_history.append(board_rep)\n",
        "\n",
        "            old_board_obj = CheckersBoard(board=board.board) # Capture state before move\n",
        "            action = None\n",
        "\n",
        "            if current_player == q_agent_player:\n",
        "                action = q_agent.choose_action(board)\n",
        "                # Store state and action for potential Q-learning update if game ends on opponent's turn\n",
        "                last_q_agent_state = old_board_obj\n",
        "                last_q_agent_action = action\n",
        "\n",
        "            elif current_player == minimax_agent_player:\n",
        "                action = minimax_agent.choose_action_minimax(board, depth=minimax_depth)\n",
        "                # Reset last Q-agent state/action as Minimax took a turn\n",
        "                last_q_agent_state = None\n",
        "                last_q_agent_action = None\n",
        "\n",
        "            if action is None: # Current player has no valid moves\n",
        "                game_over = True\n",
        "                if current_player == q_agent_player:\n",
        "                    # Q-agent loses because it has no moves\n",
        "                    q_agent.learn(last_q_agent_state, last_q_agent_action, -100, board, board) # Final loss penalty\n",
        "                    minimax_wins += 1\n",
        "                else:\n",
        "                    # Minimax loses because it has no moves\n",
        "                    # If the Q-agent made the previous move and now Minimax has no moves, Q-agent wins.\n",
        "                    # We need to ensure the Q-agent's last action is rewarded for opponent's inability to move.\n",
        "                    if last_q_agent_state and last_q_agent_action:\n",
        "                        q_agent.learn(last_q_agent_state, last_q_agent_action, 100, board, board) # Q-agent gets win reward\n",
        "                    q_wins += 1\n",
        "                break\n",
        "\n",
        "            # Make the move on the board\n",
        "            new_board = board.make_move(action, current_player)\n",
        "            board = new_board # Update the current board reference\n",
        "\n",
        "            move_count += 1\n",
        "\n",
        "            # Check for terminal state after the move\n",
        "            is_game_over, winner = board.check_terminal_state(3 - current_player) # Check for next player's turn to determine game over\n",
        "            game_over = is_game_over\n",
        "\n",
        "            if game_over:\n",
        "                reward = get_reward(board, current_player, 3 - current_player)\n",
        "                if current_player == q_agent_player:\n",
        "                    q_agent.learn(old_board_obj, action, reward, board, board)\n",
        "                    if winner == q_agent_player: q_wins += 1\n",
        "                    elif winner == minimax_agent_player: minimax_wins += 1\n",
        "                    else: draws += 1 # Should ideally be covered by win/loss if terminal_state is accurate\n",
        "                elif current_player == minimax_agent_player:\n",
        "                    # If Minimax made the winning move, and Q-agent was the previous player,\n",
        "                    # Q-agent's previous action needs to be negatively reinforced (or opponent's win rewarded).\n",
        "                    # This might require a more sophisticated reward propagation for Q-learning.\n",
        "                    # For now, we only learn for Q-agent's moves.\n",
        "                    if winner == q_agent_player: q_wins += 1\n",
        "                    elif winner == minimax_agent_player: minimax_wins += 1\n",
        "                    else: draws += 1\n",
        "                break # Game is over, break from inner game loop\n",
        "\n",
        "            # If game is not over and it was Q-agent's turn, learn from non-terminal state\n",
        "            if current_player == q_agent_player:\n",
        "                reward = get_reward(board, current_player, 3 - current_player)\n",
        "                q_agent.learn(old_board_obj, action, reward, board, board)\n",
        "\n",
        "            current_player = 3 - current_player # Switch turns\n",
        "\n",
        "        if move_count >= max_moves_per_game and not game_over: # Max moves reached, consider it a draw\n",
        "            draws += 1\n",
        "            if last_q_agent_state and last_q_agent_action:\n",
        "                q_agent.learn(last_q_agent_state, last_q_agent_action, -5, board, board) # Small penalty for draw\n",
        "\n",
        "        # Epsilon decay\n",
        "        q_agent.epsilon = max(0.01, q_agent.epsilon * epsilon_decay_rate)\n",
        "\n",
        "        if (episode + 1) % 1000 == 0:  # Changed to report every 1000 episodes\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} - Q-wins: {q_wins}, Minimax-wins: {minimax_wins}, Draws: {draws}, Epsilon: {q_agent.epsilon:.4f}\")\n",
        "            # Reset for next reporting period or just keep cumulative for overall progress\n",
        "            # q_wins, minimax_wins, draws = 0, 0, 0 # Uncomment to see stats per 100 episodes\n",
        "\n",
        "    print(f\"\\nTraining complete after {num_episodes} episodes.\")\n",
        "    print(f\"Final Results: Q-agent Wins: {q_wins}, Minimax Agent Wins: {minimax_wins}, Draws: {draws}\")\n",
        "    return q_agent, minimax_agent\n",
        "\n",
        "# Example of how to call the training function\n",
        "# num_episodes = 500\n",
        "# minimax_depth = 3\n",
        "# epsilon_decay_rate = 0.995\n",
        "# max_moves_per_game = 100 # To prevent infinite games\n",
        "\n",
        "# print(\"Starting training...\")\n",
        "# trained_q_agent, trained_minimax_agent = train_agents(num_episodes, 1, 2, minimax_depth, epsilon_decay_rate, max_moves_per_game)\n",
        "# print(\"Training finished.\")\n",
        "\n",
        "\n",
        "# Let's run a small test with fewer episodes and moves to avoid long execution in this step\n",
        "print(\"Starting a quick test training run (10000 episodes)...\") # Increased episodes\n",
        "TEST_NUM_EPISODES = 10000 # Increased episodes for better learning\n",
        "TEST_MINIMAX_DEPTH = 2\n",
        "TEST_EPSILON_DECAY_RATE = 0.995 # Slightly less aggressive decay\n",
        "TEST_MAX_MOVES_PER_GAME = 50 # Increased max moves\n",
        "\n",
        "trained_q_agent, trained_minimax_agent = train_agents(TEST_NUM_EPISODES, 1, 2, TEST_MINIMAX_DEPTH, TEST_EPSILON_DECAY_RATE, TEST_MAX_MOVES_PER_GAME)\n",
        "print(\"Quick test training finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting a quick test training run (10000 episodes)...\n",
            "Episode 1000/10000 - Q-wins: 0, Minimax-wins: 14, Draws: 986, Epsilon: 0.0100\n",
            "Episode 2000/10000 - Q-wins: 0, Minimax-wins: 31, Draws: 1969, Epsilon: 0.0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea25b4db"
      },
      "source": [
        "## Cálculo de Métricas de Desempenho\n",
        "\n",
        "### Subtask:\n",
        "Simular um grande número de jogos entre os agentes treinados e calcular as chances de vitória, empate e derrota para avaliar o desempenho de cada agente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545d3712"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Checkers Game Environment Fully Implemented**: The core game logic, including board representation, piece movement rules (simple moves and mandatory multi-captures), piece promotion to kings, and comprehensive terminal state detection (no pieces, no valid moves), has been successfully developed and verified.\n",
        "*   **Q-learning Agent Architecture Established**: A `CheckersAgent` class is implemented with key Q-learning components: state representation (hashable board tuples), action representation (valid moves from the game board), a Q-table for storing state-action values, an epsilon-greedy strategy for balancing exploration and exploitation, and the Q-learning update rule.\n",
        "*   **Heuristic Evaluation Function Integrated**: A `evaluate_board` method within the `CheckersAgent` provides a numerical score for any given board state. This heuristic considers factors such as the count and type of pieces (normal vs. king, with different weights), positional bonuses for pieces advanced on the board, and control of central squares, demonstrating logical scoring in test scenarios.\n",
        "*   **Minimax with Alpha-Beta Pruning Functional**: The `CheckersAgent` successfully integrates a `minimax` algorithm enhanced with Alpha-Beta pruning. This enables the agent to perform a deeper search into potential game states and make more informed, optimal move selections based on the developed heuristic evaluation function.\n",
        "*   **Agent Training Loop Operational**: A `train_agents` function has been set up to orchestrate games between a Q-learning agent and a Minimax agent. This loop handles turn-taking, move execution, Q-value updates, and epsilon decay for the Q-learning agent, along with game termination conditions (win, loss, draw by repetition or max moves). An initial small test run of 10 episodes confirmed the functional integration, with all games resulting in draws, which is an expected outcome for a Q-learning agent in its early stages of exploration.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Extend Training Duration and Optimize Hyperparameters**: The initial training results (10 draws in 10 episodes) indicate that the Q-learning agent is still in its early exploratory phase. Significantly more training episodes and careful tuning of Q-learning hyperparameters ($\\alpha$, $\\gamma$, $\\epsilon\\_decay\\_rate$) are crucial for the Q-agent to learn effective strategies and become competitive against the Minimax agent.\n",
        "*   **Refine Reward Function for Strategic Learning**: Consider enhancing the `get_reward` function to provide more granular feedback beyond just win/loss/draw. Incorporating rewards for strategic achievements, such as successful captures, promoting pieces to kings, or controlling key central squares, could accelerate and improve the Q-learning agent's development of more sophisticated gameplay.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6937f71"
      },
      "source": [
        "# Task\n",
        "Simulate a specified number of games between the trained Q-learning agent and the Minimax agent using the `CheckersBoard` game environment. Implement a function `evaluate_agents` that records the win, loss, and draw outcomes for each agent, calculates their respective percentages, and then prints these performance metrics in a clear, understandable format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac39a6b9"
      },
      "source": [
        "## Implement game simulation for evaluation\n",
        "\n",
        "### Subtask:\n",
        "Create a function `evaluate_agents` that simulates a specified number of games between the trained Q-learning agent and the Minimax agent, recording the outcome (win, loss, draw) for each agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "077e1f66"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires creating an `evaluate_agents` function. I will define this function with the specified parameters and implement the game simulation logic, including turn-taking, move execution, win/loss/draw tracking, epsilon handling for the Q-agent, and draw detection by repetition or max moves.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "682c3336",
        "outputId": "4af9b637-99c6-41f1-a70c-ffd672f867f7"
      },
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "# Re-including the CheckersBoard and CheckersAgent classes for completeness and execution in a single block\n",
        "class CheckersBoard:\n",
        "    def __init__(self, board=None):\n",
        "        if board is None:\n",
        "            self.board = [[0 for _ in range(8)] for _ in range(8)]\n",
        "            # 0: empty, 1: player 1 piece, 2: player 2 piece\n",
        "            # 3: player 1 king, 4: player 2 king\n",
        "\n",
        "            # Setup initial pieces for Player 1 (top of the board, rows 0-2)\n",
        "            for row in range(3):\n",
        "                for col in range(8):\n",
        "                    if (row + col) % 2 != 0:\n",
        "                        self.board[row][col] = 1\n",
        "\n",
        "            # Setup initial pieces for Player 2 (bottom of the board, rows 5-7)\n",
        "            for row in range(5, 8):\n",
        "                for col in range(8):\n",
        "                    if (row + col) % 2 != 0:\n",
        "                        self.board[row][col] = 2\n",
        "        else:\n",
        "            self.board = [row[:] for row in board] # Deep copy of the board\n",
        "\n",
        "    def __str__(self):\n",
        "        board_str = \"\"\n",
        "        for r_idx, row in enumerate(self.board):\n",
        "            board_str += f\"{8 - r_idx} |\"\n",
        "            for piece in row:\n",
        "                if piece == 0:\n",
        "                    board_str += \" . \"\n",
        "                elif piece == 1:\n",
        "                    board_str += \" x \"\n",
        "                elif piece == 2:\n",
        "                    board_str += \" o \"\n",
        "                elif piece == 3:\n",
        "                    board_str += \" X \"\n",
        "                elif piece == 4:\n",
        "                    board_str += \" O \"\n",
        "            board_str += \"\\n\"\n",
        "        board_str += \"  ------------------------\\n\"\n",
        "        board_str += \"    a  b  c  d  e  f  g  h\"\n",
        "        return board_str\n",
        "\n",
        "    def _is_valid_coord(self, r, c):\n",
        "        return 0 <= r < 8 and 0 <= c < 8\n",
        "\n",
        "    def _get_piece_owner(self, piece):\n",
        "        if piece == 1 or piece == 3:\n",
        "            return 1 # Player 1\n",
        "        elif piece == 2 or piece == 4:\n",
        "            return 2 # Player 2\n",
        "        return 0 # Empty or invalid\n",
        "\n",
        "    def _is_king(self, piece):\n",
        "        return piece == 3 or piece == 4\n",
        "\n",
        "    def _get_possible_moves_for_piece(self, r, c, player):\n",
        "        moves = []\n",
        "        piece = self.board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can move in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        for dr, dc in directions:\n",
        "            nr, nc = r + dr, c + dc\n",
        "            if self._is_valid_coord(nr, nc) and self.board[nr][nc] == 0:\n",
        "                moves.append(((r, c), (nr, nc)))\n",
        "        return moves\n",
        "\n",
        "    def _get_possible_captures_for_piece(self, r, c, player, current_board=None, path=None):\n",
        "        if current_board is None:\n",
        "            current_board = [row[:] for row in self.board]\n",
        "        if path is None:\n",
        "            path = [(r, c)]\n",
        "\n",
        "        captures = []\n",
        "        piece = current_board[r][c]\n",
        "        is_king = self._is_king(piece)\n",
        "\n",
        "        # Define directions for non-king pieces\n",
        "        if player == 1: # Player 1 pieces are at rows 0,1,2 and move 'down' (increasing row index)\n",
        "            capture_directions = [(1, -1), (1, 1)]\n",
        "        else: # Player 2 pieces are at rows 5,6,7 and move 'up' (decreasing row index)\n",
        "            capture_directions = [(-1, -1), (-1, 1)]\n",
        "\n",
        "        # Kings can capture in all 4 diagonal directions\n",
        "        if is_king:\n",
        "            capture_directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "        found_capture_in_this_step = False\n",
        "        for dr, dc in capture_directions:\n",
        "            # Square of the opponent's piece to be jumped over\n",
        "            enemy_r, enemy_c = r + dr, c + dc\n",
        "            # Landing square after the jump\n",
        "            land_r, land_c = r + 2 * dr, c + 2 * dc\n",
        "\n",
        "            if (self._is_valid_coord(land_r, land_c) and\n",
        "                    current_board[land_r][land_c] == 0 and\n",
        "                    self._is_valid_coord(enemy_r, enemy_c) and\n",
        "                    current_board[enemy_r][enemy_c] != 0 and\n",
        "                    self._get_piece_owner(current_board[enemy_r][enemy_c]) != player):\n",
        "\n",
        "                found_capture_in_this_step = True\n",
        "                # Simulate the capture on a temporary board\n",
        "                temp_board = [row[:] for row in current_board]\n",
        "                temp_board[land_r][land_c] = temp_board[r][c]\n",
        "                temp_board[r][c] = 0\n",
        "                temp_board[enemy_r][enemy_c] = 0\n",
        "\n",
        "                # Check for king promotion after landing, only if not king yet and reaching back row\n",
        "                promoted_piece = temp_board[land_r][land_c]\n",
        "                if not self._is_king(promoted_piece): # only promote if not already a king\n",
        "                    if (player == 1 and land_r == 7) or (player == 2 and land_r == 0): # Adjusted promotion rows\n",
        "                        temp_board[land_r][land_c] += 2 # Promote to king (1->3, 2->4)\n",
        "\n",
        "                # Recursively look for more captures from the new position\n",
        "                chained_captures = self._get_possible_captures_for_piece(land_r, land_c, player, temp_board, path + [(land_r, land_c)])\n",
        "\n",
        "                if chained_captures: # If more captures are found, add them\n",
        "                    captures.extend(chained_captures)\n",
        "                else: # If no more captures, this is a terminal capture sequence\n",
        "                    captures.append(path + [(land_r, land_c)])\n",
        "\n",
        "        # If no capture was found in this step, but it's part of a longer path, return empty to signify end of chain\n",
        "        if not found_capture_in_this_step and len(path) > 1:\n",
        "            return []\n",
        "\n",
        "        return captures\n",
        "\n",
        "    def get_valid_moves(self, player):\n",
        "        all_possible_moves = []\n",
        "        all_possible_captures = []\n",
        "\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == player:\n",
        "                    # Get basic moves\n",
        "                    all_possible_moves.extend(self._get_possible_moves_for_piece(r, c, player))\n",
        "                    # Get captures\n",
        "                    captures_from_piece = self._get_possible_captures_for_piece(r, c, player)\n",
        "                    if captures_from_piece:\n",
        "                        all_possible_captures.extend(captures_from_piece)\n",
        "\n",
        "        if all_possible_captures:\n",
        "            # If captures are available, they are mandatory\n",
        "            return all_possible_captures\n",
        "        else:\n",
        "            # Otherwise, return all basic moves\n",
        "            return all_possible_moves\n",
        "\n",
        "    def make_move(self, move, player):\n",
        "        # Create a new board state for the move\n",
        "        new_board = CheckersBoard(board=self.board)\n",
        "\n",
        "        if isinstance(move[0], tuple) and len(move) == 2 and isinstance(move[1], tuple): # It's a simple move ((r1,c1), (r2,c2))\n",
        "            start_r, start_c = move[0]\n",
        "            end_r, end_c = move[1]\n",
        "            piece_to_move = new_board.board[start_r][start_c]\n",
        "\n",
        "            new_board.board[end_r][end_c] = piece_to_move\n",
        "            new_board.board[start_r][start_c] = 0\n",
        "\n",
        "            # Check for king promotion\n",
        "            if not new_board._is_king(piece_to_move): # Only promote if not already a king\n",
        "                if (player == 1 and end_r == 7) or (player == 2 and end_r == 0):\n",
        "                    new_board.board[end_r][end_c] += 2 # Promote to king\n",
        "\n",
        "        elif isinstance(move, list): # It's a capture path [(r1,c1), (r2,c2), ...]\n",
        "            for i in range(len(move) - 1):\n",
        "                start_r, start_c = move[i]\n",
        "                end_r, end_c = move[i+1]\n",
        "                piece_to_move = new_board.board[start_r][start_c]\n",
        "\n",
        "                new_board.board[end_r][end_c] = piece_to_move\n",
        "                new_board.board[start_r][start_c] = 0\n",
        "\n",
        "                # Remove captured piece\n",
        "                # Calculate the position of the captured piece (midpoint)\n",
        "                captured_r = (start_r + end_r) // 2\n",
        "                captured_c = (start_c + end_c) // 2\n",
        "                new_board.board[captured_r][captured_c] = 0\n",
        "\n",
        "            # After the whole capture path, check for king promotion at the final landing spot\n",
        "            final_r, final_c = move[-1]\n",
        "            promoted_piece = new_board.board[final_r][final_c]\n",
        "            if not new_board._is_king(promoted_piece): # Only promote if not already a king\n",
        "                if (player == 1 and final_r == 7) or (player == 2 and final_r == 0):\n",
        "                    new_board.board[final_r][final_c] += 2\n",
        "\n",
        "        return new_board\n",
        "\n",
        "    def _has_valid_moves(self, player):\n",
        "        return bool(self.get_valid_moves(player))\n",
        "\n",
        "    def check_terminal_state(self, current_player):\n",
        "        player1_pieces = 0\n",
        "        player2_pieces = 0\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = self.board[r][c]\n",
        "                if self._get_piece_owner(piece) == 1:\n",
        "                    player1_pieces += 1\n",
        "                elif self._get_piece_owner(piece) == 2:\n",
        "                    player2_pieces += 1\n",
        "\n",
        "        # Condition 1: No pieces left\n",
        "        if player1_pieces == 0: return True, 2 # Player 2 wins\n",
        "        if player2_pieces == 0: return True, 1 # Player 1 wins\n",
        "\n",
        "        # Condition 2: Current player has no valid moves\n",
        "        if not self._has_valid_moves(current_player):\n",
        "            if current_player == 1: return True, 2 # Player 1 has no moves, Player 2 wins\n",
        "            if current_player == 2: return True, 1 # Player 2 has no moves, Player 1 wins\n",
        "\n",
        "        # Default: Game is not over\n",
        "        return False, None\n",
        "\n",
        "class CheckersAgent:\n",
        "    def __init__(self, player, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.player = player # 1 for player 1, 2 for player 2\n",
        "        self.alpha = alpha # Learning rate\n",
        "        self.gamma = gamma # Discount factor\n",
        "        self.epsilon = epsilon # Exploration rate\n",
        "        self.q_table = {}\n",
        "\n",
        "    def _get_state_representation(self, board):\n",
        "        return tuple(tuple(row) for row in board.board)\n",
        "\n",
        "    def _get_possible_actions(self, board):\n",
        "        return board.get_valid_moves(self.player)\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        hashable_action = tuple(action) if isinstance(action, list) else action\n",
        "        return self.q_table.get((state, hashable_action), 0.0)\n",
        "\n",
        "    def choose_action(self, board):\n",
        "        state = self._get_state_representation(board)\n",
        "        possible_actions = self._get_possible_actions(board)\n",
        "\n",
        "        if not possible_actions:\n",
        "            return None\n",
        "\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(possible_actions)\n",
        "        else:\n",
        "            q_values = [self.get_q_value(state, action) for action in possible_actions]\n",
        "            max_q = -float('inf')\n",
        "            best_actions = []\n",
        "\n",
        "            for i, q_val in enumerate(q_values):\n",
        "                if q_val > max_q:\n",
        "                    max_q = q_val\n",
        "                    best_actions = [possible_actions[i]]\n",
        "                elif q_val == max_q:\n",
        "                    best_actions.append(possible_actions[i])\n",
        "\n",
        "            return random.choice(best_actions)\n",
        "\n",
        "    def learn(self, old_state_obj, action, reward, new_state_obj, new_board_obj):\n",
        "        hashable_action = tuple(action) if isinstance(action, list) else action\n",
        "\n",
        "        old_state_rep = self._get_state_representation(old_state_obj)\n",
        "        new_state_rep = self._get_state_representation(new_state_obj)\n",
        "\n",
        "        current_q = self.get_q_value(old_state_rep, hashable_action)\n",
        "\n",
        "        future_possible_actions = new_board_obj.get_valid_moves(self.player)\n",
        "        if future_possible_actions:\n",
        "            max_future_q = max([self.get_q_value(new_state_rep, a) for a in future_possible_actions])\n",
        "        else:\n",
        "            max_future_q = 0.0\n",
        "\n",
        "        self.q_table[(old_state_rep, hashable_action)] = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)\n",
        "\n",
        "    def evaluate_board(self, board_obj):\n",
        "        score = 0\n",
        "        opponent_player = 3 - self.player\n",
        "\n",
        "        piece_weight = 1\n",
        "        king_weight = 2\n",
        "\n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                piece = board_obj.board[r][c]\n",
        "                owner = board_obj._get_piece_owner(piece)\n",
        "\n",
        "                if owner == self.player:\n",
        "                    if board_obj._is_king(piece):\n",
        "                        score += king_weight\n",
        "                    else:\n",
        "                        score += piece_weight\n",
        "\n",
        "                    if self.player == 1:\n",
        "                        score += r * 0.1\n",
        "                    else:\n",
        "                        score += (7 - r) * 0.1\n",
        "\n",
        "                    if 2 <= r <= 5 and 2 <= c <= 5:\n",
        "                        score += 0.05\n",
        "\n",
        "                elif owner == opponent_player:\n",
        "                    if board_obj._is_king(piece):\n",
        "                        score -= king_weight\n",
        "                    else:\n",
        "                        score -= piece_weight\n",
        "\n",
        "                    if self.player == 1:\n",
        "                        score -= (7 - r) * 0.1\n",
        "                    else:\n",
        "                        score -= r * 0.1\n",
        "\n",
        "                    if 2 <= r <= 5 and 2 <= c <= 5:\n",
        "                        score -= 0.05\n",
        "\n",
        "        return score\n",
        "\n",
        "    def minimax(self, board_obj, depth, alpha, beta, maximizing_player):\n",
        "        current_player_in_minimax = self.player if maximizing_player else (3 - self.player)\n",
        "        is_over, winner = board_obj.check_terminal_state(current_player_in_minimax)\n",
        "\n",
        "        if depth == 0 or is_over:\n",
        "            if is_over:\n",
        "                if winner == self.player:\n",
        "                    return 1000000000 + depth\n",
        "                elif winner == (3 - self.player):\n",
        "                    return -1000000000 - depth\n",
        "                else:\n",
        "                    return 0\n",
        "            return self.evaluate_board(board_obj)\n",
        "\n",
        "        if maximizing_player:\n",
        "            max_eval = -math.inf\n",
        "            valid_moves = board_obj.get_valid_moves(self.player)\n",
        "            if not valid_moves:\n",
        "                return -1000000000 - depth\n",
        "            for move in valid_moves:\n",
        "                new_board = board_obj.make_move(move, self.player)\n",
        "                eval = self.minimax(new_board, depth - 1, alpha, beta, False)\n",
        "                max_eval = max(max_eval, eval)\n",
        "                alpha = max(alpha, eval)\n",
        "                if beta <= alpha:\n",
        "                    break\n",
        "            return max_eval\n",
        "        else:\n",
        "            min_eval = math.inf\n",
        "            opponent_player = 3 - self.player\n",
        "            valid_moves = board_obj.get_valid_moves(opponent_player)\n",
        "            if not valid_moves:\n",
        "                return 1000000000 + depth\n",
        "            for move in valid_moves:\n",
        "                new_board = board_obj.make_move(move, opponent_player)\n",
        "                eval = self.minimax(new_board, depth - 1, alpha, beta, True)\n",
        "                min_eval = min(min_eval, eval)\n",
        "                beta = min(beta, eval)\n",
        "                if beta <= alpha:\n",
        "                    break\n",
        "            return min_eval\n",
        "\n",
        "    def choose_action_minimax(self, board_obj, depth=3):\n",
        "        possible_actions = board_obj.get_valid_moves(self.player)\n",
        "\n",
        "        if not possible_actions:\n",
        "            return None\n",
        "\n",
        "        if len(possible_actions) == 1:\n",
        "            return possible_actions[0]\n",
        "\n",
        "        best_eval = -math.inf\n",
        "        best_move = None\n",
        "        alpha = -math.inf\n",
        "        beta = math.inf\n",
        "\n",
        "        for move in possible_actions:\n",
        "            new_board = board_obj.make_move(move, self.player)\n",
        "            eval = self.minimax(new_board, depth - 1, alpha, beta, False)\n",
        "\n",
        "            if eval > best_eval:\n",
        "                best_eval = eval\n",
        "                best_move = move\n",
        "            alpha = max(alpha, eval)\n",
        "\n",
        "        return best_move\n",
        "\n",
        "def get_reward(board, player, opponent_player):\n",
        "    is_over, winner = board.check_terminal_state(player)\n",
        "\n",
        "    if is_over:\n",
        "        if winner == player:\n",
        "            return 100\n",
        "        elif winner == opponent_player:\n",
        "            return -100\n",
        "        else:\n",
        "            return -5\n",
        "\n",
        "    return -1\n",
        "\n",
        "def evaluate_agents(q_agent, minimax_agent, num_evaluation_games, minimax_depth, max_moves_per_game):\n",
        "    q_wins = 0\n",
        "    minimax_wins = 0\n",
        "    draws = 0\n",
        "\n",
        "    original_q_epsilon = q_agent.epsilon # Store original epsilon\n",
        "\n",
        "    for game_num in range(num_evaluation_games):\n",
        "        board = CheckersBoard()\n",
        "        current_player = 1 # Start with player 1\n",
        "        game_over = False\n",
        "        move_count = 0\n",
        "        board_history = set() # Use a set for efficient lookup of board states\n",
        "\n",
        "        # Determine which agent is which player\n",
        "        agent_for_p1 = q_agent if q_agent.player == 1 else minimax_agent\n",
        "        agent_for_p2 = q_agent if q_agent.player == 2 else minimax_agent\n",
        "\n",
        "        while not game_over and move_count < max_moves_per_game:\n",
        "            board_rep = tuple(tuple(row) for row in board.board)\n",
        "            if board_rep in board_history:\n",
        "                draws += 1\n",
        "                game_over = True\n",
        "                break\n",
        "            board_history.add(board_rep)\n",
        "\n",
        "            action = None\n",
        "            current_agent = None\n",
        "            if current_player == 1: # Player 1's turn\n",
        "                current_agent = agent_for_p1\n",
        "            else: # Player 2's turn\n",
        "                current_agent = agent_for_p2\n",
        "\n",
        "            if current_agent.player == q_agent.player: # If it's the Q-agent's turn\n",
        "                q_agent.epsilon = 0 # Set to greedy for evaluation\n",
        "                action = q_agent.choose_action(board)\n",
        "                q_agent.epsilon = original_q_epsilon # Restore original epsilon\n",
        "            else: # It's the Minimax agent's turn\n",
        "                action = minimax_agent.choose_action_minimax(board, depth=minimax_depth)\n",
        "\n",
        "            if action is None: # Current player has no valid moves, opponent wins\n",
        "                game_over = True\n",
        "                if current_player == q_agent.player: # Q-agent had no moves, Minimax wins\n",
        "                    minimax_wins += 1\n",
        "                else: # Minimax had no moves, Q-agent wins\n",
        "                    q_wins += 1\n",
        "                break\n",
        "\n",
        "            # Make the move\n",
        "            board = board.make_move(action, current_player)\n",
        "            move_count += 1\n",
        "\n",
        "            # Check terminal state for the *next* player\n",
        "            next_player = 3 - current_player\n",
        "            is_game_over_check, winner = board.check_terminal_state(next_player)\n",
        "            game_over = is_game_over_check\n",
        "\n",
        "            if game_over:\n",
        "                if winner == q_agent.player:\n",
        "                    q_wins += 1\n",
        "                elif winner == minimax_agent.player:\n",
        "                    minimax_wins += 1\n",
        "                else: # This handles cases where check_terminal_state returns true but winner is None/draw\n",
        "                    draws += 1\n",
        "                break\n",
        "\n",
        "            current_player = next_player # Switch turns\n",
        "\n",
        "        if not game_over: # Max moves reached without a clear winner\n",
        "            draws += 1\n",
        "\n",
        "        if (game_num + 1) % 100 == 0:\n",
        "            print(f\"Evaluation Game {game_num + 1}/{num_evaluation_games} - Q-wins: {q_wins}, Minimax-wins: {minimax_wins}, Draws: {draws}\")\n",
        "\n",
        "    print(f\"\\nEvaluation complete after {num_evaluation_games} games.\")\n",
        "    print(f\"Final Evaluation Results: Q-agent Wins: {q_wins}, Minimax Agent Wins: {minimax_wins}, Draws: {draws}\")\n",
        "    return q_wins, minimax_wins, draws\n",
        "\n",
        "# Assuming trained_q_agent and trained_minimax_agent exist from previous training step\n",
        "# For demonstration, let's create mock agents if they are not defined yet after a kernel restart\n",
        "if 'trained_q_agent' not in locals():\n",
        "    print(\"Trained agents not found in current session. Running a quick training session...\")\n",
        "    # Use the training parameters from the previous step\n",
        "    TEST_NUM_EPISODES = 1000\n",
        "    TEST_MINIMAX_DEPTH = 2\n",
        "    TEST_EPSILON_DECAY_RATE = 0.995\n",
        "    TEST_MAX_MOVES_PER_GAME = 50\n",
        "    trained_q_agent, trained_minimax_agent = train_agents(TEST_NUM_EPISODES, 1, 2, TEST_MINIMAX_DEPTH, TEST_EPSILON_DECAY_RATE, TEST_MAX_MOVES_PER_GAME)\n",
        "    print(\"Mock training complete.\")\n",
        "\n",
        "print(\"\\n--- Starting Agent Evaluation ---\")\n",
        "EVAL_NUM_GAMES = 100 # Number of games to simulate for evaluation\n",
        "EVAL_MINIMAX_DEPTH = trained_minimax_agent.minimax_depth if hasattr(trained_minimax_agent, 'minimax_depth') else 3 # Use depth from training or default\n",
        "EVAL_MAX_MOVES_PER_GAME = 100 # Max moves to prevent infinite loops during evaluation\n",
        "\n",
        "\n",
        "q_wins, minimax_wins, draws = evaluate_agents(trained_q_agent, trained_minimax_agent, EVAL_NUM_GAMES, EVAL_MINIMAX_DEPTH, EVAL_MAX_MOVES_PER_GAME)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Agent Evaluation ---\n",
            "Evaluation Game 100/100 - Q-wins: 11, Minimax-wins: 15, Draws: 74\n",
            "\n",
            "Evaluation complete after 100 games.\n",
            "Final Evaluation Results: Q-agent Wins: 11, Minimax Agent Wins: 15, Draws: 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9aab1ee"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully ran the evaluation. Now, I will calculate and print the win, loss, and draw percentages for each agent based on the evaluation results to fully complete the subtask requirement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77194d41",
        "outputId": "75ab573a-79d7-41e9-f39b-8fcffa70a5eb"
      },
      "source": [
        "total_games = q_wins + minimax_wins + draws\n",
        "\n",
        "print(\"\\n--- Performance Metrics ---\")\n",
        "if total_games > 0:\n",
        "    # Q-agent percentages\n",
        "    q_win_percentage = (q_wins / total_games) * 100\n",
        "    q_loss_percentage = (minimax_wins / total_games) * 100 # Q-agent loses when Minimax wins\n",
        "    q_draw_percentage = (draws / total_games) * 100\n",
        "\n",
        "    print(f\"Q-learning Agent (Player {trained_q_agent.player}):\")\n",
        "    print(f\"  Wins: {q_wins} ({q_win_percentage:.2f}%) - (vs Minimax)\")\n",
        "    print(f\"  Losses: {minimax_wins} ({q_loss_percentage:.2f}%) - (vs Minimax)\")\n",
        "    print(f\"  Draws: {draws} ({q_draw_percentage:.2f}%)\")\n",
        "\n",
        "    # Minimax agent percentages\n",
        "    minimax_win_percentage = (minimax_wins / total_games) * 100\n",
        "    minimax_loss_percentage = (q_wins / total_games) * 100 # Minimax loses when Q-agent wins\n",
        "    minimax_draw_percentage = (draws / total_games) * 100\n",
        "\n",
        "    print(f\"\\nMinimax Agent (Player {trained_minimax_agent.player}):\")\n",
        "    print(f\"  Wins: {minimax_wins} ({minimax_win_percentage:.2f}%) - (vs Q-learning)\")\n",
        "    print(f\"  Losses: {q_wins} ({minimax_loss_percentage:.2f}%) - (vs Q-learning)\")\n",
        "    print(f\"  Draws: {draws} ({minimax_draw_percentage:.2f}%)\")\n",
        "else:\n",
        "    print(\"No games were played for evaluation.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Performance Metrics ---\n",
            "Q-learning Agent (Player 1):\n",
            "  Wins: 11 (11.00%) - (vs Minimax)\n",
            "  Losses: 15 (15.00%) - (vs Minimax)\n",
            "  Draws: 74 (74.00%)\n",
            "\n",
            "Minimax Agent (Player 2):\n",
            "  Wins: 15 (15.00%) - (vs Q-learning)\n",
            "  Losses: 11 (11.00%) - (vs Q-learning)\n",
            "  Draws: 74 (74.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6f51c59"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   An `evaluate_agents` function was successfully implemented to simulate games between a Q-learning agent and a Minimax agent, recording win, loss, and draw outcomes, and handling draw conditions due to board state repetition or exceeding the maximum allowed moves per game.\n",
        "*   During evaluation, the Q-learning agent's exploration rate (`epsilon`) was temporarily set to 0 to ensure greedy action selection, reflecting its learned policy.\n",
        "*   The evaluation consisted of 100 games, and the results showed the following distribution:\n",
        "    *   Q-learning Agent Wins: 11 games.\n",
        "    *   Minimax Agent Wins: 15 games.\n",
        "    *   Draws: 74 games.\n",
        "*   The calculated performance percentages are:\n",
        "    *   **Q-learning Agent:** 11.00% wins, 15.00% losses (to Minimax), and 74.00% draws.\n",
        "    *   **Minimax Agent:** 15.00% wins, 11.00% losses (to Q-learning), and 74.00% draws.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The high percentage of draws (74%) suggests that either the game often reaches a stalemate condition within the set `max_moves_per_game`, or both agents are playing cautiously without frequently forcing a decisive win. Future analysis could involve inspecting game logs of drawn games to understand the reasons for stalemates.\n",
        "*   The Minimax agent, with 15% wins, slightly outperforms the Q-learning agent's 11% wins in this evaluation. This indicates that the Minimax strategy, for the given depth, is marginally more effective at securing victories against the Q-learning agent under the current training and evaluation parameters.\n"
      ]
    }
  ]
}