{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjhWOZlCfwxPn/D3ZO2FQk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeniferGoncalvesDaSilvaDev/algo_min_max_tic_tac_toe/blob/main/C%C3%B3pia_de_C%C3%B3pia_de_minmax_algo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install collections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gv7kw0wFrS3",
        "outputId": "8f666dc3-7d01-484e-de60-f622dd35f30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CLmBZCp9--g",
        "outputId": "ba04e3ef-c686-483e-d413-f1964240d757",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Q-learner against fixed minimax opponent (MIN plays optimal).\n",
            "State representation is a truth-table of propositions (see state_propositions()).\n",
            "Episode 300: Win/Tie/Loss = 500/0/0\n",
            "Episode 600: Win/Tie/Loss = 500/0/0\n",
            "Episode 900: Win/Tie/Loss = 500/0/0\n",
            "Episode 1200: Win/Tie/Loss = 500/0/0\n",
            "Episode 1500: Win/Tie/Loss = 500/0/0\n",
            "Episode 1800: Win/Tie/Loss = 500/0/0\n",
            "Episode 2100: Win/Tie/Loss = 500/0/0\n",
            "Episode 2400: Win/Tie/Loss = 500/0/0\n",
            "Episode 2700: Win/Tie/Loss = 500/0/0\n",
            "Episode 3000: Win/Tie/Loss = 500/0/0\n",
            "\n",
            "Final evaluation against minimax opponent:\n",
            "Wins/Ties/Losses: 2000 0 0\n",
            "\n",
            "Initial state propositions key: 00000000001\n",
            "Q-values for initial state:\n",
            "action 0 Q= 0.0\n",
            "action 1 Q= 0.0\n",
            "action 2 Q= 0.0\n",
            "action 3 Q= 0.0\n",
            "\n",
            "Sample board (1, 0, -1, 0)\n",
            "Propositions: {'cell_0is_X': True, 'cell0_is_O': False, 'cell_1is_X': False, 'cell1_is_O': False, 'cell_2is_X': False, 'cell2_is_O': True, 'cell_3is_X': False, 'cell3_is_O': False, 'X_has_threat': True, 'O_has_threat': True, 'center_any_empty': True}\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "\n",
        "# ----------------------------- Game utilities --------------------------------\n",
        "\n",
        "# Board indices: 0 1\n",
        "\n",
        "#                2 3\n",
        "\n",
        "WIN_LINES = [(0,1), (2,3), (0,2), (1,3), (0,3), (1,2)]  # all 2-in-line possibilities\n",
        "\n",
        "EMPTY = 0\n",
        "MAX = 1   # X\n",
        "MIN = -1  # O\n",
        "\n",
        "def is_terminal(board):\n",
        "    \"\"\"Determines if the given board state is terminal and returns the reward.\n",
        "\n",
        "    Args:\n",
        "        board (tuple): A tuple representing the current state of the game board.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - terminal (bool): True if the board is a terminal state (win, loss, or draw), False otherwise.\n",
        "            - reward (int): +1 if MAX wins, -1 if MIN wins, 0 for a draw or non-terminal state.\n",
        "    \"\"\"\n",
        "    # check wins\n",
        "    for (i,j) in WIN_LINES:\n",
        "        if board[i] == board[j] != EMPTY:\n",
        "            return True, (1 if board[i] == MAX else -1)\n",
        "    # draw (all filled)\n",
        "    if all(cell != EMPTY for cell in board):\n",
        "        return True, 0\n",
        "    return False, 0\n",
        "\n",
        "def legal_actions(board):\n",
        "    \"\"\"Returns a list of legal actions (empty cell indices) for the current board state.\n",
        "\n",
        "    Args:\n",
        "        board (tuple): A tuple representing the current state of the game board.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of integers, where each integer is the index of an empty cell.\n",
        "    \"\"\"\n",
        "    return [i for i,v in enumerate(board) if v == EMPTY]\n",
        "\n",
        "def apply_action(board, action, player):\n",
        "    \"\"\"Applies a given action to the board for a specified player.\n",
        "\n",
        "    Args:\n",
        "        board (tuple): The current state of the game board.\n",
        "        action (int): The index of the cell where the player wants to make a move.\n",
        "        player (int): The player making the move (MAX=1 or MIN=-1).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A new tuple representing the board state after the action has been applied.\n",
        "    \"\"\"\n",
        "    new = list(board)\n",
        "    new[action] = player\n",
        "    return tuple(new)\n",
        "\n",
        "# -------------------------- Truth-table mapping --------------------------------\n",
        "\n",
        "def state_propositions(board):\n",
        "    \"\"\"Returns a dictionary of boolean propositions for a given board state.\n",
        "\n",
        "    Example propositions: cell_i_is_X, cell_i_is_O for i in 0..3,\n",
        "    X_two_in_line_threat, O_two_in_line_threat, any_center_empty.\n",
        "\n",
        "    Args:\n",
        "        board (tuple): A tuple representing the current state of the game board.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are proposition names (strings) and values are booleans.\n",
        "    \"\"\"\n",
        "    props = {}\n",
        "    for i in range(4):\n",
        "        props[f'cell_{i}is_X'] = (board[i] == MAX)\n",
        "        props[f'cell{i}_is_O'] = (board[i] == MIN)\n",
        "\n",
        "    def has_threat(player):\n",
        "        \"\"\"Helper function to check for two-in-line threats.\"\"\"\n",
        "        for (a,b) in WIN_LINES:\n",
        "            # X_has_threat: True if the player has one piece on a winning line and the other cell is empty.\n",
        "            if (board[a] == player and board[b] == EMPTY) or (board[b] == player and board[a] == EMPTY):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # X_has_threat: Proposition indicating if MAX (X) has a potential winning line with one piece and one empty cell.\n",
        "    props['X_has_threat'] = has_threat(MAX)\n",
        "    # O_has_threat: Proposition indicating if MIN (O) has a potential winning line with one piece and one empty cell.\n",
        "    props['O_has_threat'] = has_threat(MIN)\n",
        "\n",
        "    # A simple 'center' concept for the tiny board: cells 1 and 2 are considered 'center-ish'\n",
        "    # center_any_empty: Proposition indicating if any of the 'center' cells (1 or 2) are empty.\n",
        "    props['center_any_empty'] = (board[1] == EMPTY or board[2] == EMPTY)\n",
        "\n",
        "    return props\n",
        "\n",
        "def propositions_to_key(props):\n",
        "    \"\"\"Creates a deterministic string key from a dictionary of propositions.\n",
        "    This key represents a unique 'truth-table row' for the state.\n",
        "\n",
        "    Args:\n",
        "        props (dict): A dictionary of boolean propositions.\n",
        "\n",
        "    Returns:\n",
        "        str: A string formed by concatenating '1' for True and '0' for False\n",
        "             values of propositions, sorted by key name for canonical order.\n",
        "    \"\"\"\n",
        "    keys = sorted(props.keys())\n",
        "    bits = ['1' if props[k] else '0' for k in keys]\n",
        "    return ''.join(bits)\n",
        "\n",
        "# ---------------------------- Minimax search ----------------------------------\n",
        "\n",
        "def minimax_value(board, player):\n",
        "    \"\"\"Calculates the minimax value for a given board state for the specified player.\n",
        "\n",
        "    This function recursively explores the game tree to determine the optimal\n",
        "    outcome for the 'player' assuming both players play optimally.\n",
        "\n",
        "    Args:\n",
        "        board (tuple): The current state of the game board.\n",
        "        player (int): The current player whose turn it is (MAX=1 or MIN=-1).\n",
        "\n",
        "    Returns:\n",
        "        int: The optimal value of the state from the current player's perspective\n",
        "             (+1 for win, -1 for loss, 0 for draw).\n",
        "    \"\"\"\n",
        "    term, reward = is_terminal(board)\n",
        "    if term:\n",
        "        return reward\n",
        "\n",
        "    if player == MAX:\n",
        "        best = -math.inf\n",
        "        for a in legal_actions(board):\n",
        "            val = minimax_value(apply_action(board,a,MAX), MIN)\n",
        "            if val > best:\n",
        "                best = val\n",
        "        return best\n",
        "    else:\n",
        "        best = math.inf\n",
        "        for a in legal_actions(board):\n",
        "            val = minimax_value(apply_action(board,a,MIN), MAX)\n",
        "            if val < best:\n",
        "                best = val\n",
        "        return best\n",
        "\n",
        "def minimax_policy(board, player):\n",
        "    \"\"\"Determines the optimal actions for a player using the minimax algorithm.\n",
        "\n",
        "    This function finds all legal moves that lead to the best possible outcome\n",
        "    for the current player, assuming optimal play from both sides.\n",
        "\n",
        "    Args:\n",
        "        board (tuple): The current state of the game board.\n",
        "        player (int): The player for whom to determine the optimal actions (MAX=1 or MIN=-1).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of integers, where each integer is an index representing an optimal action.\n",
        "              Returns an empty list if the board is a terminal state.\n",
        "    \"\"\"\n",
        "    term, _ = is_terminal(board)\n",
        "    if term:\n",
        "        return []\n",
        "    best_actions = []\n",
        "    best_val = -math.inf if player == MAX else math.inf\n",
        "    for a in legal_actions(board):\n",
        "        val = minimax_value(apply_action(board,a,player), -player)\n",
        "        if player == MAX:\n",
        "            if val > best_val:\n",
        "                best_val = val\n",
        "                best_actions = [a]\n",
        "            elif val == best_val:\n",
        "                best_actions.append(a)\n",
        "        else:\n",
        "            if val < best_val:\n",
        "                best_val = val\n",
        "                best_actions = [a]\n",
        "            elif val == best_val:\n",
        "                best_actions.append(a)\n",
        "    return best_actions\n",
        "\n",
        "# -------------------------- Tabular Q-learning ---------------------------------\n",
        "\n",
        "class QLearner:\n",
        "    \"\"\"Implements a Q-learning agent for learning optimal policies in a game.\n",
        "\n",
        "    The Q-learner uses a tabular approach to store Q-values for state-action pairs\n",
        "    and updates them based on rewards received and future expected values.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.5, gamma=0.99, epsilon=0.2):\n",
        "        \"\"\"Initializes the QLearner with learning parameters.\n",
        "\n",
        "        Args:\n",
        "            alpha (float): The learning rate, controlling how much new information overrides old information.\n",
        "            gamma (float): The discount factor, determining the importance of future rewards.\n",
        "            epsilon (float): The exploration-exploitation trade-off parameter.\n",
        "                             (Probability of choosing a random action).\n",
        "        \"\"\"\n",
        "        # Q[state_key][action] = value\n",
        "        self.Q = defaultdict(lambda: defaultdict(float))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def get_Q(self, state_key, action):\n",
        "        \"\"\"Retrieves the Q-value for a given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            state_key (str): The unique string representation of the state.\n",
        "            action (int): The action taken from that state.\n",
        "\n",
        "        Returns:\n",
        "            float: The Q-value associated with the state-action pair.\n",
        "        \"\"\"\n",
        "        return self.Q[state_key][action]\n",
        "\n",
        "    def choose_action(self, state_key, legal_actions_list):\n",
        "        \"\"\"Selects an action using an epsilon-greedy policy.\n",
        "\n",
        "        Args:\n",
        "            state_key (str): The unique string representation of the current state.\n",
        "            legal_actions_list (list): A list of legal actions available in the current state.\n",
        "\n",
        "        Returns:\n",
        "            int: The chosen action (an index).\n",
        "        \"\"\"\n",
        "        # epsilon-greedy: with probability epsilon, choose a random action.\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(legal_actions_list)\n",
        "        # Otherwise, pick the action with the maximum Q-value.\n",
        "        qs = [(self.get_Q(state_key,a),a) for a in legal_actions_list]\n",
        "        maxq = max(qs, key=lambda x: x[0])[0]\n",
        "        best = [a for q,a in qs if q==maxq]\n",
        "        return random.choice(best)\n",
        "\n",
        "    def update(self, s_key, a, r, s_next_key, legal_a_next, opponent_policy):\n",
        "        \"\"\"Updates the Q-value for a state-action pair using the Q-learning update rule.\n",
        "\n",
        "        Args:\n",
        "            s_key (str): The key for the previous state (current state where action 'a' was taken).\n",
        "            a (int): The action taken from state `s_key`.\n",
        "            r (int): The immediate reward received after taking action 'a'.\n",
        "            s_next_key (str or None): The key for the next state. None if `s_next_key` is a terminal state.\n",
        "            legal_a_next (list): A list of legal actions available in the next state `s_next_key`.\n",
        "            opponent_policy (function): The opponent's policy function (e.g., minimax_policy).\n",
        "        \"\"\"\n",
        "        # Since opponent is fixed, the next state's value is expectation under opponent policy\n",
        "        # target = r + gamma * max_a' E_{opponent}[ Q(s', a') ]\n",
        "        if not legal_a_next:  # next is terminal\n",
        "            target = r\n",
        "        else:\n",
        "            # compute expected Q for each candidate next action a' (MAX's action)\n",
        "            best_values = []\n",
        "            for a_prime in legal_a_next:\n",
        "                # opponent will respond according to their policy; here we look up Q(s',a')\n",
        "                # for MAX's potential actions in the next state.\n",
        "                best_values.append(self.get_Q(s_next_key, a_prime))\n",
        "            # The Q-learner (MAX) aims to maximize its expected future reward, so it considers the max Q-value.\n",
        "            target = r + self.gamma * max(best_values)\n",
        "        # TD update: Q(s,a) = Q(s,a) + alpha * (target - Q(s,a))\n",
        "        cur = self.get_Q(s_key, a)\n",
        "        self.Q[s_key][a] = cur + self.alpha * (target - cur)\n",
        "\n",
        "# ------------------------ Environment & Training loop --------------------------\n",
        "\n",
        "def play_episode(qlearner, opponent_policy_func, train=True, verbose=False):\n",
        "    \"\"\"Simulates a single episode of the game between the Q-learner (MAX) and an opponent (MIN).\n",
        "\n",
        "    Args:\n",
        "        qlearner (QLearner): The Q-learning agent for player MAX.\n",
        "        opponent_policy_func (function): A function representing the opponent's policy (e.g., minimax_policy).\n",
        "        train (bool): If True, the Q-learner updates its Q-values during the episode.\n",
        "        verbose (bool): If True, prints game progress.\n",
        "\n",
        "    Returns:\n",
        "        int: The final reward for MAX (+1 for win, -1 for loss, 0 for draw).\n",
        "    \"\"\"\n",
        "    board = (0,0,0,0)\n",
        "    player = MAX\n",
        "    history = []\n",
        "\n",
        "    while True:\n",
        "        term, reward = is_terminal(board)\n",
        "        if term:\n",
        "            # If the game is terminal, return the final reward.\n",
        "            if train:\n",
        "                # Update for the last transition if necessary (e.g., if MAX made the move leading to terminal state)\n",
        "                # For this specific implementation, the update happens when MIN makes a move, so nothing extra here.\n",
        "                pass\n",
        "            if verbose:\n",
        "                print('Terminal:', board, 'reward', reward)\n",
        "            return reward\n",
        "\n",
        "        if player == MAX:\n",
        "            # MAX's turn: choose action using the Q-learner's policy (epsilon-greedy).\n",
        "            props = state_propositions(board)\n",
        "            s_key = propositions_to_key(props)\n",
        "            acts = legal_actions(board)\n",
        "            a = qlearner.choose_action(s_key, acts)\n",
        "            # Apply MAX's chosen action.\n",
        "            board_next = apply_action(board, a, MAX)\n",
        "            # Store MAX's move for potential future Q-value update.\n",
        "            history.append(('MAX', board, s_key, a))\n",
        "            board = board_next\n",
        "            player = MIN\n",
        "\n",
        "        else:\n",
        "            # MIN's turn: opponent plays according to its fixed policy (minimax).\n",
        "            best_actions = opponent_policy_func(board, MIN)\n",
        "            if not best_actions:\n",
        "                # This case handles a rare scenario where MIN has no legal moves (e.g., immediate draw/terminal before MIN's turn logic finishes).\n",
        "                player = MAX\n",
        "                continue\n",
        "            a_op = random.choice(best_actions)\n",
        "            board_next = apply_action(board, a_op, MIN)\n",
        "\n",
        "            # Q-learner update: If MAX just made a move in the previous step, update its Q-value.\n",
        "            if history and history[-1][0] == 'MAX':\n",
        "                _, board_prev, s_key_prev, a_prev = history[-1]\n",
        "                term_now, reward_now = is_terminal(board_next)\n",
        "                # Determine the next state key and legal actions for MAX's perspective for the update.\n",
        "                if term_now:\n",
        "                    s_next_key = None\n",
        "                    legal_next = []\n",
        "                else:\n",
        "                    props_next = state_propositions(board_next)\n",
        "                    s_next_key = propositions_to_key(props_next)\n",
        "                    legal_next = legal_actions(board_next)\n",
        "                if train:\n",
        "                    # Perform the Q-value update based on MAX's previous action and the outcome after MIN's response.\n",
        "                    qlearner.update(s_key_prev, a_prev, reward_now, s_next_key, legal_next, opponent_policy_func)\n",
        "                # Clear history for this MAX move as it has been processed.\n",
        "                history.pop()\n",
        "\n",
        "            board = board_next\n",
        "            player = MAX\n",
        "\n",
        "# ----------------------------- Evaluation -------------------------------------\n",
        "\n",
        "def evaluate(qlearner, opponent_policy_func, episodes=200):\n",
        "    \"\"\"Evaluates the performance of the Q-learner against an opponent over multiple episodes.\n",
        "\n",
        "    Args:\n",
        "        qlearner (QLearner): The Q-learning agent to evaluate.\n",
        "        opponent_policy_func (function): The opponent's policy function (e.g., minimax_policy).\n",
        "        episodes (int): The number of episodes to run for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - wins (int): The number of episodes won by the Q-learner (MAX).\n",
        "            - ties (int): The number of episodes that ended in a draw.\n",
        "            - losses (int): The number of episodes lost by the Q-learner (MAX).\n",
        "    \"\"\"\n",
        "    wins = 0\n",
        "    ties = 0\n",
        "    losses = 0\n",
        "    old_eps = qlearner.epsilon\n",
        "    qlearner.epsilon = 0.0  # Set epsilon to 0 for greedy evaluation\n",
        "    for _ in range(episodes):\n",
        "        res = play_episode(qlearner, opponent_policy_func, train=False, verbose=False)\n",
        "        if res == 1:\n",
        "            wins += 1\n",
        "        elif res == 0:\n",
        "            ties += 1\n",
        "        else:\n",
        "            losses += 1\n",
        "    qlearner.epsilon = old_eps # Restore original epsilon\n",
        "    return wins, ties, losses\n",
        "\n",
        "# ------------------------------- Main -----------------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    random.seed(42)\n",
        "\n",
        "    q = QLearner(alpha=0.7, gamma=0.95, epsilon=0.2)\n",
        "\n",
        "    episodes = 3000\n",
        "    eval_every = 300\n",
        "\n",
        "    # Prints an introductory message about the training process.\n",
        "    print('Training Q-learner against fixed minimax opponent (MIN plays optimal).')\n",
        "    # Explains the state representation used by the Q-learner.\n",
        "    print('State representation is a truth-table of propositions (see state_propositions()).')\n",
        "\n",
        "    for ep in range(1, episodes+1):\n",
        "        play_episode(q, minimax_policy, train=True)\n",
        "\n",
        "        if ep % eval_every == 0:\n",
        "            w,t,l = evaluate(q, minimax_policy, episodes=500)\n",
        "            # Reports the Q-learner's performance (Wins/Ties/Losses) against the minimax opponent\n",
        "            # at regular evaluation intervals during training.\n",
        "            print(f'Episode {ep}: Win/Tie/Loss = {w}/{t}/{l}')\n",
        "\n",
        "    # Prints a newline for better readability before the final evaluation.\n",
        "    print('\\nFinal evaluation against minimax opponent:')\n",
        "    w,t,l = evaluate(q, minimax_policy, episodes=2000)\n",
        "    # Displays the final Win/Tie/Loss record of the Q-learner after all training episodes\n",
        "    # against the fixed minimax opponent over a larger number of evaluation episodes.\n",
        "    print('Wins/Ties/Losses:', w, t, l)\n",
        "\n",
        "    # Inspect learned Q for initial empty board\n",
        "    init_props = state_propositions((0,0,0,0))\n",
        "    init_key = propositions_to_key(init_props)\n",
        "    # Prints the canonical key representation for the initial empty board state based on its propositions.\n",
        "    print('\\nInitial state propositions key:', init_key)\n",
        "    # Indicates that the following output will list the Q-values for possible actions from the initial state.\n",
        "    print('Q-values for initial state:')\n",
        "    for a in legal_actions((0,0,0,0)):\n",
        "        # Shows the learned Q-value for each legal action from the initial empty board state.\n",
        "        print('action', a, 'Q=', q.get_Q(init_key,a))\n",
        "\n",
        "    # Example: show propositions mapping for a sample board\n",
        "    sample = (1,0,-1,0)\n",
        "    # Prints a sample board configuration to demonstrate the state representation.\n",
        "    print('\\nSample board', sample)\n",
        "    # Displays the boolean propositions generated for the `sample` board, illustrating\n",
        "    # how a board state is translated into features for the Q-learner.\n",
        "    print('Propositions:', state_propositions(sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c6af978"
      },
      "source": [
        "# Task\n",
        "Add comments to the main execution block (if __name__ == '__main__':) in the provided code that explain what each print statement in the output signifies, line by line, to provide a clear understanding of the training and evaluation process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cc3d26f"
      },
      "source": [
        "## Explain code output\n",
        "\n",
        "### Subtask:\n",
        "Add comments to the main execution block (if __name__ == '__main__':) that explain what each print statement in the output signifies, line by line, to provide a clear understanding of the training and evaluation process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc36e7f0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "- The main execution block (`if __name__ == '__main__':`) of the code has been thoroughly commented.\n",
        "- Each print statement in the training and evaluation process output now includes a line-by-line explanation clarifying its significance.\n",
        "- The comments elucidate the sequential steps, from model initialization through epoch-specific training progress and loss reporting, to the final evaluation metrics.\n",
        "\n",
        "### Insights or Next Steps\n",
        "- The added comments provide a clear and explicit understanding of the model's operational flow and performance reporting, enhancing code readability and maintainability.\n",
        "- This detailed explanation is beneficial for anyone reviewing the code output, allowing for quick comprehension of the training and evaluation phases without needing deep code inspection.\n"
      ]
    }
  ]
}