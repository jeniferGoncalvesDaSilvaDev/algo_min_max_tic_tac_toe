{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0VRpGaqmw4gqZA9lH9Lpq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeniferGoncalvesDaSilvaDev/algo_min_max_tic_tac_toe/blob/main/C%C3%B3pia_de_C%C3%B3pia_de_minmax_algo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install collections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gv7kw0wFrS3",
        "outputId": "8f666dc3-7d01-484e-de60-f622dd35f30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bzhfegDHeMyY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CLmBZCp9--g",
        "outputId": "ba04e3ef-c686-483e-d413-f1964240d757",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Q-learner against fixed minimax opponent (MIN plays optimal).\n",
            "State representation is a truth-table of propositions (see state_propositions()).\n",
            "Episode 300: Win/Tie/Loss = 500/0/0\n",
            "Episode 600: Win/Tie/Loss = 500/0/0\n",
            "Episode 900: Win/Tie/Loss = 500/0/0\n",
            "Episode 1200: Win/Tie/Loss = 500/0/0\n",
            "Episode 1500: Win/Tie/Loss = 500/0/0\n",
            "Episode 1800: Win/Tie/Loss = 500/0/0\n",
            "Episode 2100: Win/Tie/Loss = 500/0/0\n",
            "Episode 2400: Win/Tie/Loss = 500/0/0\n",
            "Episode 2700: Win/Tie/Loss = 500/0/0\n",
            "Episode 3000: Win/Tie/Loss = 500/0/0\n",
            "\n",
            "Final evaluation against minimax opponent:\n",
            "Wins/Ties/Losses: 2000 0 0\n",
            "\n",
            "Initial state propositions key: 00000000001\n",
            "Q-values for initial state:\n",
            "action 0 Q= 0.0\n",
            "action 1 Q= 0.0\n",
            "action 2 Q= 0.0\n",
            "action 3 Q= 0.0\n",
            "\n",
            "Sample board (1, 0, -1, 0)\n",
            "Propositions: {'cell_0is_X': True, 'cell0_is_O': False, 'cell_1is_X': False, 'cell1_is_O': False, 'cell_2is_X': False, 'cell2_is_O': True, 'cell_3is_X': False, 'cell3_is_O': False, 'X_has_threat': True, 'O_has_threat': True, 'center_any_empty': True}\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "\n",
        "# ----------------------------- Game utilities --------------------------------\n",
        "\n",
        "# Board indices: 0 1\n",
        "\n",
        "#                2 3\n",
        "\n",
        "WIN_LINES = [(0,1), (2,3), (0,2), (1,3), (0,3), (1,2)]  # all 2-in-line possibilities\n",
        "\n",
        "EMPTY = 0\n",
        "MAX = 1   # X\n",
        "MIN = -1  # O\n",
        "\n",
        "def is_terminal(board):\n",
        "    \"\"\"Determines if the given board state is terminal and returns the reward.\n",
        "\n",
        "    Args:\n",
        "        board (tuple): A tuple representing the current state of the game board.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - terminal (bool): True if the board is a terminal state (win, loss, or draw), False otherwise.\n",
        "            - reward (int): +1 if MAX wins, -1 if MIN wins, 0 for a draw or non-terminal state.\n",
        "    \"\"\"\n",
        "    # check wins\n",
        "    for (i,j) in WIN_LINES:\n",
        "        if board[i] == board[j] != EMPTY:\n",
        "            return True, (1 if board[i] == MAX else -1)\n",
        "    # draw (all filled)\n",
        "    if all(cell != EMPTY for cell in board):\n",
        "        return True, 0\n",
        "    return False, 0\n",
        "\n",
        "def legal_actions(board):\n",
        "    \"\"\"Returns a list of legal actions (empty cell indices) for the current board state.\n",
        "\n",
        "    Args:\n",
        "        board (tuple): A tuple representing the current state of the game board.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of integers, where each integer is the index of an empty cell.\n",
        "    \"\"\"\n",
        "    return [i for i,v in enumerate(board) if v == EMPTY]\n",
        "\n",
        "def apply_action(board, action, player):\n",
        "    \"\"\"Applies a given action to the board for a specified player.\n",
        "\n",
        "    Args:\n",
        "        board (tuple): The current state of the game board.\n",
        "        action (int): The index of the cell where the player wants to make a move.\n",
        "        player (int): The player making the move (MAX=1 or MIN=-1).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A new tuple representing the board state after the action has been applied.\n",
        "    \"\"\"\n",
        "    new = list(board)\n",
        "    new[action] = player\n",
        "    return tuple(new)\n",
        "\n",
        "# -------------------------- Truth-table mapping --------------------------------\n",
        "\n",
        "def state_propositions(board):\n",
        "    \"\"\"Returns a dictionary of boolean propositions for a given board state.\n",
        "\n",
        "    Example propositions: cell_i_is_X, cell_i_is_O for i in 0..3,\n",
        "    X_two_in_line_threat, O_two_in_line_threat, any_center_empty.\n",
        "\n",
        "    Args:\n",
        "        board (tuple): A tuple representing the current state of the game board.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are proposition names (strings) and values are booleans.\n",
        "    \"\"\"\n",
        "    props = {}\n",
        "    for i in range(4):\n",
        "        props[f'cell_{i}is_X'] = (board[i] == MAX)\n",
        "        props[f'cell{i}_is_O'] = (board[i] == MIN)\n",
        "\n",
        "    def has_threat(player):\n",
        "        \"\"\"Helper function to check for two-in-line threats.\"\"\"\n",
        "        for (a,b) in WIN_LINES:\n",
        "            # X_has_threat: True if the player has one piece on a winning line and the other cell is empty.\n",
        "            if (board[a] == player and board[b] == EMPTY) or (board[b] == player and board[a] == EMPTY):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # X_has_threat: Proposition indicating if MAX (X) has a potential winning line with one piece and one empty cell.\n",
        "    props['X_has_threat'] = has_threat(MAX)\n",
        "    # O_has_threat: Proposition indicating if MIN (O) has a potential winning line with one piece and one empty cell.\n",
        "    props['O_has_threat'] = has_threat(MIN)\n",
        "\n",
        "    # A simple 'center' concept for the tiny board: cells 1 and 2 are considered 'center-ish'\n",
        "    # center_any_empty: Proposition indicating if any of the 'center' cells (1 or 2) are empty.\n",
        "    props['center_any_empty'] = (board[1] == EMPTY or board[2] == EMPTY)\n",
        "\n",
        "    return props\n",
        "\n",
        "def propositions_to_key(props):\n",
        "    \"\"\"Creates a deterministic string key from a dictionary of propositions.\n",
        "    This key represents a unique 'truth-table row' for the state.\n",
        "\n",
        "    Args:\n",
        "        props (dict): A dictionary of boolean propositions.\n",
        "\n",
        "    Returns:\n",
        "        str: A string formed by concatenating '1' for True and '0' for False\n",
        "             values of propositions, sorted by key name for canonical order.\n",
        "    \"\"\"\n",
        "    keys = sorted(props.keys())\n",
        "    bits = ['1' if props[k] else '0' for k in keys]\n",
        "    return ''.join(bits)\n",
        "\n",
        "# ---------------------------- Minimax search ----------------------------------\n",
        "\n",
        "def minimax_value(board, player):\n",
        "    \"\"\"Calculates the minimax value for a given board state for the specified player.\n",
        "\n",
        "    This function recursively explores the game tree to determine the optimal\n",
        "    outcome for the 'player' assuming both players play optimally.\n",
        "\n",
        "    Args:\n",
        "        board (tuple): The current state of the game board.\n",
        "        player (int): The current player whose turn it is (MAX=1 or MIN=-1).\n",
        "\n",
        "    Returns:\n",
        "        int: The optimal value of the state from the current player's perspective\n",
        "             (+1 for win, -1 for loss, 0 for draw).\n",
        "    \"\"\"\n",
        "    term, reward = is_terminal(board)\n",
        "    if term:\n",
        "        return reward\n",
        "\n",
        "    if player == MAX:\n",
        "        best = -math.inf\n",
        "        for a in legal_actions(board):\n",
        "            val = minimax_value(apply_action(board,a,MAX), MIN)\n",
        "            if val > best:\n",
        "                best = val\n",
        "        return best\n",
        "    else:\n",
        "        best = math.inf\n",
        "        for a in legal_actions(board):\n",
        "            val = minimax_value(apply_action(board,a,MIN), MAX)\n",
        "            if val < best:\n",
        "                best = val\n",
        "        return best\n",
        "\n",
        "def minimax_policy(board, player):\n",
        "    \"\"\"Determines the optimal actions for a player using the minimax algorithm.\n",
        "\n",
        "    This function finds all legal moves that lead to the best possible outcome\n",
        "    for the current player, assuming optimal play from both sides.\n",
        "\n",
        "    Args:\n",
        "        board (tuple): The current state of the game board.\n",
        "        player (int): The player for whom to determine the optimal actions (MAX=1 or MIN=-1).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of integers, where each integer is an index representing an optimal action.\n",
        "              Returns an empty list if the board is a terminal state.\n",
        "    \"\"\"\n",
        "    term, _ = is_terminal(board)\n",
        "    if term:\n",
        "        return []\n",
        "    best_actions = []\n",
        "    best_val = -math.inf if player == MAX else math.inf\n",
        "    for a in legal_actions(board):\n",
        "        val = minimax_value(apply_action(board,a,player), -player)\n",
        "        if player == MAX:\n",
        "            if val > best_val:\n",
        "                best_val = val\n",
        "                best_actions = [a]\n",
        "            elif val == best_val:\n",
        "                best_actions.append(a)\n",
        "        else:\n",
        "            if val < best_val:\n",
        "                best_val = val\n",
        "                best_actions = [a]\n",
        "            elif val == best_val:\n",
        "                best_actions.append(a)\n",
        "    return best_actions\n",
        "\n",
        "# -------------------------- Tabular Q-learning ---------------------------------\n",
        "\n",
        "class QLearner:\n",
        "    \"\"\"Implements a Q-learning agent for learning optimal policies in a game.\n",
        "\n",
        "    The Q-learner uses a tabular approach to store Q-values for state-action pairs\n",
        "    and updates them based on rewards received and future expected values.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.5, gamma=0.99, epsilon=0.2):\n",
        "        \"\"\"Initializes the QLearner with learning parameters.\n",
        "\n",
        "        Args:\n",
        "            alpha (float): The learning rate, controlling how much new information overrides old information.\n",
        "            gamma (float): The discount factor, determining the importance of future rewards.\n",
        "            epsilon (float): The exploration-exploitation trade-off parameter.\n",
        "                             (Probability of choosing a random action).\n",
        "        \"\"\"\n",
        "        # Q[state_key][action] = value\n",
        "        self.Q = defaultdict(lambda: defaultdict(float))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def get_Q(self, state_key, action):\n",
        "        \"\"\"Retrieves the Q-value for a given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            state_key (str): The unique string representation of the state.\n",
        "            action (int): The action taken from that state.\n",
        "\n",
        "        Returns:\n",
        "            float: The Q-value associated with the state-action pair.\n",
        "        \"\"\"\n",
        "        return self.Q[state_key][action]\n",
        "\n",
        "    def choose_action(self, state_key, legal_actions_list):\n",
        "        \"\"\"Selects an action using an epsilon-greedy policy.\n",
        "\n",
        "        Args:\n",
        "            state_key (str): The unique string representation of the current state.\n",
        "            legal_actions_list (list): A list of legal actions available in the current state.\n",
        "\n",
        "        Returns:\n",
        "            int: The chosen action (an index).\n",
        "        \"\"\"\n",
        "        # epsilon-greedy: with probability epsilon, choose a random action.\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(legal_actions_list)\n",
        "        # Otherwise, pick the action with the maximum Q-value.\n",
        "        qs = [(self.get_Q(state_key,a),a) for a in legal_actions_list]\n",
        "        maxq = max(qs, key=lambda x: x[0])[0]\n",
        "        best = [a for q,a in qs if q==maxq]\n",
        "        return random.choice(best)\n",
        "\n",
        "    def update(self, s_key, a, r, s_next_key, legal_a_next, opponent_policy):\n",
        "        \"\"\"Updates the Q-value for a state-action pair using the Q-learning update rule.\n",
        "\n",
        "        Args:\n",
        "            s_key (str): The key for the previous state (current state where action 'a' was taken).\n",
        "            a (int): The action taken from state `s_key`.\n",
        "            r (int): The immediate reward received after taking action 'a'.\n",
        "            s_next_key (str or None): The key for the next state. None if `s_next_key` is a terminal state.\n",
        "            legal_a_next (list): A list of legal actions available in the next state `s_next_key`.\n",
        "            opponent_policy (function): The opponent's policy function (e.g., minimax_policy).\n",
        "        \"\"\"\n",
        "        # Since opponent is fixed, the next state's value is expectation under opponent policy\n",
        "        # target = r + gamma * max_a' E_{opponent}[ Q(s', a') ]\n",
        "        if not legal_a_next:  # next is terminal\n",
        "            target = r\n",
        "        else:\n",
        "            # compute expected Q for each candidate next action a' (MAX's action)\n",
        "            best_values = []\n",
        "            for a_prime in legal_a_next:\n",
        "                # opponent will respond according to their policy; here we look up Q(s',a')\n",
        "                # for MAX's potential actions in the next state.\n",
        "                best_values.append(self.get_Q(s_next_key, a_prime))\n",
        "            # The Q-learner (MAX) aims to maximize its expected future reward, so it considers the max Q-value.\n",
        "            target = r + self.gamma * max(best_values)\n",
        "        # TD update: Q(s,a) = Q(s,a) + alpha * (target - Q(s,a))\n",
        "        cur = self.get_Q(s_key, a)\n",
        "        self.Q[s_key][a] = cur + self.alpha * (target - cur)\n",
        "\n",
        "# ------------------------ Environment & Training loop --------------------------\n",
        "\n",
        "def play_episode(qlearner, opponent_policy_func, train=True, verbose=False):\n",
        "    \"\"\"Simulates a single episode of the game between the Q-learner (MAX) and an opponent (MIN).\n",
        "\n",
        "    Args:\n",
        "        qlearner (QLearner): The Q-learning agent for player MAX.\n",
        "        opponent_policy_func (function): A function representing the opponent's policy (e.g., minimax_policy).\n",
        "        train (bool): If True, the Q-learner updates its Q-values during the episode.\n",
        "        verbose (bool): If True, prints game progress.\n",
        "\n",
        "    Returns:\n",
        "        int: The final reward for MAX (+1 for win, -1 for loss, 0 for draw).\n",
        "    \"\"\"\n",
        "    board = (0,0,0,0)\n",
        "    player = MAX\n",
        "    history = []\n",
        "\n",
        "    while True:\n",
        "        term, reward = is_terminal(board)\n",
        "        if term:\n",
        "            # If the game is terminal, return the final reward.\n",
        "            if train:\n",
        "                # Update for the last transition if necessary (e.g., if MAX made the move leading to terminal state)\n",
        "                # For this specific implementation, the update happens when MIN makes a move, so nothing extra here.\n",
        "                pass\n",
        "            if verbose:\n",
        "                print('Terminal:', board, 'reward', reward)\n",
        "            return reward\n",
        "\n",
        "        if player == MAX:\n",
        "            # MAX's turn: choose action using the Q-learner's policy (epsilon-greedy).\n",
        "            props = state_propositions(board)\n",
        "            s_key = propositions_to_key(props)\n",
        "            acts = legal_actions(board)\n",
        "            a = qlearner.choose_action(s_key, acts)\n",
        "            # Apply MAX's chosen action.\n",
        "            board_next = apply_action(board, a, MAX)\n",
        "            # Store MAX's move for potential future Q-value update.\n",
        "            history.append(('MAX', board, s_key, a))\n",
        "            board = board_next\n",
        "            player = MIN\n",
        "\n",
        "        else:\n",
        "            # MIN's turn: opponent plays according to its fixed policy (minimax).\n",
        "            best_actions = opponent_policy_func(board, MIN)\n",
        "            if not best_actions:\n",
        "                # This case handles a rare scenario where MIN has no legal moves (e.g., immediate draw/terminal before MIN's turn logic finishes).\n",
        "                player = MAX\n",
        "                continue\n",
        "            a_op = random.choice(best_actions)\n",
        "            board_next = apply_action(board, a_op, MIN)\n",
        "\n",
        "            # Q-learner update: If MAX just made a move in the previous step, update its Q-value.\n",
        "            if history and history[-1][0] == 'MAX':\n",
        "                _, board_prev, s_key_prev, a_prev = history[-1]\n",
        "                term_now, reward_now = is_terminal(board_next)\n",
        "                # Determine the next state key and legal actions for MAX's perspective for the update.\n",
        "                if term_now:\n",
        "                    s_next_key = None\n",
        "                    legal_next = []\n",
        "                else:\n",
        "                    props_next = state_propositions(board_next)\n",
        "                    s_next_key = propositions_to_key(props_next)\n",
        "                    legal_next = legal_actions(board_next)\n",
        "                if train:\n",
        "                    # Perform the Q-value update based on MAX's previous action and the outcome after MIN's response.\n",
        "                    qlearner.update(s_key_prev, a_prev, reward_now, s_next_key, legal_next, opponent_policy_func)\n",
        "                # Clear history for this MAX move as it has been processed.\n",
        "                history.pop()\n",
        "\n",
        "            board = board_next\n",
        "            player = MAX\n",
        "\n",
        "# ----------------------------- Evaluation -------------------------------------\n",
        "\n",
        "def evaluate(qlearner, opponent_policy_func, episodes=200):\n",
        "    \"\"\"Evaluates the performance of the Q-learner against an opponent over multiple episodes.\n",
        "\n",
        "    Args:\n",
        "        qlearner (QLearner): The Q-learning agent to evaluate.\n",
        "        opponent_policy_func (function): The opponent's policy function (e.g., minimax_policy).\n",
        "        episodes (int): The number of episodes to run for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - wins (int): The number of episodes won by the Q-learner (MAX).\n",
        "            - ties (int): The number of episodes that ended in a draw.\n",
        "            - losses (int): The number of episodes lost by the Q-learner (MAX).\n",
        "    \"\"\"\n",
        "    wins = 0\n",
        "    ties = 0\n",
        "    losses = 0\n",
        "    old_eps = qlearner.epsilon\n",
        "    qlearner.epsilon = 0.0  # Set epsilon to 0 for greedy evaluation\n",
        "    for _ in range(episodes):\n",
        "        res = play_episode(qlearner, opponent_policy_func, train=False, verbose=False)\n",
        "        if res == 1:\n",
        "            wins += 1\n",
        "        elif res == 0:\n",
        "            ties += 1\n",
        "        else:\n",
        "            losses += 1\n",
        "    qlearner.epsilon = old_eps # Restore original epsilon\n",
        "    return wins, ties, losses\n",
        "\n",
        "# ------------------------------- Main -----------------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    random.seed(42)\n",
        "\n",
        "    q = QLearner(alpha=0.7, gamma=0.95, epsilon=0.2)\n",
        "\n",
        "    episodes = 3000\n",
        "    eval_every = 300\n",
        "\n",
        "    # Prints an introductory message about the training process.\n",
        "    print('Training Q-learner against fixed minimax opponent (MIN plays optimal).')\n",
        "    # Explains the state representation used by the Q-learner.\n",
        "    print('State representation is a truth-table of propositions (see state_propositions()).')\n",
        "\n",
        "    for ep in range(1, episodes+1):\n",
        "        play_episode(q, minimax_policy, train=True)\n",
        "\n",
        "        if ep % eval_every == 0:\n",
        "            w,t,l = evaluate(q, minimax_policy, episodes=500)\n",
        "            # Reports the Q-learner's performance (Wins/Ties/Losses) against the minimax opponent\n",
        "            # at regular evaluation intervals during training.\n",
        "            print(f'Episode {ep}: Win/Tie/Loss = {w}/{t}/{l}')\n",
        "\n",
        "    # Prints a newline for better readability before the final evaluation.\n",
        "    print('\\nFinal evaluation against minimax opponent:')\n",
        "    w,t,l = evaluate(q, minimax_policy, episodes=2000)\n",
        "    # Displays the final Win/Tie/Loss record of the Q-learner after all training episodes\n",
        "    # against the fixed minimax opponent over a larger number of evaluation episodes.\n",
        "    print('Wins/Ties/Losses:', w, t, l)\n",
        "\n",
        "    # Inspect learned Q for initial empty board\n",
        "    init_props = state_propositions((0,0,0,0))\n",
        "    init_key = propositions_to_key(init_props)\n",
        "    # Prints the canonical key representation for the initial empty board state based on its propositions.\n",
        "    print('\\nInitial state propositions key:', init_key)\n",
        "    # Indicates that the following output will list the Q-values for possible actions from the initial state.\n",
        "    print('Q-values for initial state:')\n",
        "    for a in legal_actions((0,0,0,0)):\n",
        "        # Shows the learned Q-value for each legal action from the initial empty board state.\n",
        "        print('action', a, 'Q=', q.get_Q(init_key,a))\n",
        "\n",
        "    # Example: show propositions mapping for a sample board\n",
        "    sample = (1,0,-1,0)\n",
        "    # Prints a sample board configuration to demonstrate the state representation.\n",
        "    print('\\nSample board', sample)\n",
        "    # Displays the boolean propositions generated for the `sample` board, illustrating\n",
        "    # how a board state is translated into features for the Q-learner.\n",
        "    print('Propositions:', state_propositions(sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c6af978"
      },
      "source": [
        "# Task\n",
        "Add comments to the main execution block (if __name__ == '__main__':) in the provided code that explain what each print statement in the output signifies, line by line, to provide a clear understanding of the training and evaluation process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cc3d26f"
      },
      "source": [
        "## Explain code output\n",
        "\n",
        "### Subtask:\n",
        "Add comments to the main execution block (if __name__ == '__main__':) that explain what each print statement in the output signifies, line by line, to provide a clear understanding of the training and evaluation process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc36e7f0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "- The main execution block (`if __name__ == '__main__':`) of the code has been thoroughly commented.\n",
        "- Each print statement in the training and evaluation process output now includes a line-by-line explanation clarifying its significance.\n",
        "- The comments elucidate the sequential steps, from model initialization through epoch-specific training progress and loss reporting, to the final evaluation metrics.\n",
        "\n",
        "### Insights or Next Steps\n",
        "- The added comments provide a clear and explicit understanding of the model's operational flow and performance reporting, enhancing code readability and maintainability.\n",
        "- This detailed explanation is beneficial for anyone reviewing the code output, allowing for quick comprehension of the training and evaluation phases without needing deep code inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "from copy import deepcopy\n",
        "\n",
        "# ----------------------------- Game utilities --------------------------------\n",
        "\n",
        "# Board indices: 0 1\n",
        "\n",
        "#                2 3\n",
        "\n",
        "WIN_LINES = [(0,1), (2,3), (0,2), (1,3), (0,3), (1,2)]  # all 2-in-line possibilities\n",
        "\n",
        "EMPTY = 0\n",
        "\n",
        "# ----------------------- Multi-agent environment -----------------------------\n",
        "\n",
        "class MultiAgentGame:\n",
        "    def __init__(self, n_players=4):\n",
        "        assert 2 <= n_players <= 4, \"n_players must be between 2 and 4\"\n",
        "        self.n = n_players\n",
        "\n",
        "    def init_board(self):\n",
        "        # Empty board represented by tuple of length 4 with values 0 (empty) or player id (1..n)\n",
        "        return tuple([EMPTY]*4)\n",
        "\n",
        "    def legal_actions(self, board):\n",
        "        return [i for i,v in enumerate(board) if v == EMPTY]\n",
        "\n",
        "    def apply_action(self, board, action, player):\n",
        "        new = list(board)\n",
        "        new[action] = player\n",
        "        return tuple(new)\n",
        "\n",
        "    def is_terminal(self, board):\n",
        "        # Check for any winner: player who has both cells in any WIN_LINES\n",
        "        for (i,j) in WIN_LINES:\n",
        "            if board[i] != EMPTY and board[i] == board[j]:\n",
        "                winner = board[i]\n",
        "                return True, winner  # return winner id\n",
        "        # draw if full\n",
        "        if all(cell != EMPTY for cell in board):\n",
        "            return True, None\n",
        "        return False, None\n",
        "\n",
        "    def reward_vector(self, winner_id):\n",
        "        # reward 1 to winner, 0 to others; if draw, all 0\n",
        "        if winner_id is None:\n",
        "            return [0.0]*self.n\n",
        "        else:\n",
        "            return [1.0 if (i+1) == winner_id else 0.0 for i in range(self.n)]\n",
        "\n",
        "# -------------------------- Truth-table mapping --------------------------------\n",
        "\n",
        "def state_propositions_multi(board, n_players):\n",
        "    \"\"\"Return dict of boolean propositions for a given board and number of players.\n",
        "    Propositions include: cell_i_is_pj for each cell 0..3 and player 1..n\n",
        "    Additional features: for each player, 'has_threat' (one piece on a winning line and one empty)\n",
        "    \"\"\"\n",
        "    props = {}\n",
        "    for i in range(4):\n",
        "        for p in range(1, n_players+1):\n",
        "            props[f'cell_{i}_is_p{p}'] = (board[i] == p)\n",
        "\n",
        "    def has_threat(player):\n",
        "        for (a,b) in WIN_LINES:\n",
        "            if (board[a] == player and board[b] == EMPTY) or (board[b] == player and board[a] == EMPTY):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    for p in range(1, n_players+1):\n",
        "        props[f'P{p}_has_threat'] = has_threat(p)\n",
        "\n",
        "    # simple center concept\n",
        "    props['center_any_empty'] = (board[1] == EMPTY or board[2] == EMPTY)\n",
        "    return props\n",
        "\n",
        "def propositions_to_key(props):\n",
        "    keys = sorted(props.keys())\n",
        "    bits = ['1' if props[k] else '0' for k in keys]\n",
        "    return ''.join(bits)\n",
        "\n",
        "# ------------------------ Vector Q-table & utilities ---------------------------\n",
        "\n",
        "class VectorQLearner:\n",
        "    \"\"\"Tabular vector-valued Q: Q[state_key][action] -> list of length n_players\"\"\"\n",
        "    def __init__(self, n_players, alpha=0.5, gamma=0.95, epsilon=0.2):\n",
        "        self.n = n_players\n",
        "        self.Q = defaultdict(lambda: defaultdict(self._zero_action))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def _zero_action(self):\n",
        "        # returns a fresh zero-vector for action values\n",
        "        return [0.0]*self.n\n",
        "\n",
        "    def get_Q_vector(self, state_key, action):\n",
        "        return self.Q[state_key][action]\n",
        "\n",
        "    def get_component(self, state_key, action, player_idx):\n",
        "        return self.get_Q_vector(state_key, action)[player_idx]\n",
        "\n",
        "    def set_Q_vector(self, state_key, action, vec):\n",
        "        self.Q[state_key][action] = vec\n",
        "\n",
        "    def choose_action(self, state_key, legal_actions_list, player_idx, coop_threshold=0.0):\n",
        "        \"\"\"Choose action for player index (0-based). Implements epsilon-greedy and a simple cooperative heuristic.\n",
        "        Coop heuristic: if for some opponent j, max_a' Q_j(s,a') - max_a' Q_i(s,a') > coop_threshold,\n",
        "        then consider cooperation with that opponent by maximizing sum of components (i+j) instead of only i.\n",
        "        \"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(legal_actions_list)\n",
        "\n",
        "        # Evaluate own-component values\n",
        "        qs = [(self.get_component(state_key,a,player_idx), a) for a in legal_actions_list]\n",
        "        max_own = max(qs, key=lambda x: x[0])[0]\n",
        "\n",
        "        # find if any opponent has strictly larger best value\n",
        "        best_opponent = None\n",
        "        best_delta = 0.0\n",
        "        for j in range(self.n):\n",
        "            if j == player_idx: continue\n",
        "            opp_qs = [self.get_component(state_key,a,j) for a in legal_actions_list]\n",
        "            if not opp_qs: continue\n",
        "            max_opp = max(opp_qs)\n",
        "            delta = max_opp - max_own\n",
        "            if delta > best_delta:\n",
        "                best_delta = delta\n",
        "                best_opponent = j\n",
        "\n",
        "        # If cooperative advantage exists above threshold, compute action maximizing sum of components\n",
        "        if best_opponent is not None and best_delta > coop_threshold:\n",
        "            # maximize Q_i + Q_j\n",
        "            summed = [ (self.get_component(state_key,a,player_idx) + self.get_component(state_key,a,best_opponent), a)\n",
        "                      for a in legal_actions_list]\n",
        "            maxsum = max(summed, key=lambda x: x[0])[0]\n",
        "            best_actions = [a for val,a in summed if val == maxsum]\n",
        "            return random.choice(best_actions)\n",
        "\n",
        "        # otherwise act greedily on own component\n",
        "        best = [a for val,a in qs if val == max_own]\n",
        "        return random.choice(best)\n",
        "\n",
        "    def update(self, s_key, a, reward_vector, s_next_key, legal_a_next):\n",
        "        \"\"\"Update only the component(s) of players who acted recently.\n",
        "        Here we will update all components that have their turn in the next state? Simpler: update the acting player's component(s) passed via caller.\n",
        "        For clarity we assume caller updates components for specific players. This method implements generic vector TD for all players using max over their components.\n",
        "        \"\"\"\n",
        "        # current vector\n",
        "        cur = self.get_Q_vector(s_key, a)\n",
        "        target = [0.0]*self.n\n",
        "        if not legal_a_next:\n",
        "            # terminal: target = reward\n",
        "            target = reward_vector\n",
        "        else:\n",
        "            # non-terminal: target_i = r_i + gamma * max_{a'} Q_i(s',a')\n",
        "            for i in range(self.n):\n",
        "                # find max over next actions for component i\n",
        "                next_vals = [self.get_component(s_next_key, a2, i) for a2 in legal_a_next]\n",
        "                best_next = max(next_vals) if next_vals else 0.0\n",
        "                target[i] = reward_vector[i] + self.gamma * best_next\n",
        "\n",
        "        # TD update for vector\n",
        "        newvec = [0.0]*self.n\n",
        "        for i in range(self.n):\n",
        "            newvec[i] = cur[i] + self.alpha * (target[i] - cur[i])\n",
        "        self.set_Q_vector(s_key, a, newvec)\n",
        "\n",
        "# ------------------------ Training & Episode logic ----------------------------\n",
        "\n",
        "def play_episode_multi(game, qlearner, coop_threshold=0.0, train=True, verbose=False):\n",
        "    board = game.init_board()\n",
        "    player_turn = 1  # players numbered 1..n\n",
        "    history = []  # store (player, s_key, action) for updates\n",
        "\n",
        "    while True:\n",
        "        term, winner = game.is_terminal(board)\n",
        "        if term:\n",
        "            rewards = game.reward_vector(winner)\n",
        "            # update for last actions in history\n",
        "            if train:\n",
        "                # update all stored actions with resulting reward and terminal next-state\n",
        "                while history:\n",
        "                    pl, s_key, a = history.pop()\n",
        "                    # terminal next: no legal next actions\n",
        "                    qlearner.update(s_key, a, rewards, None, [])\n",
        "            if verbose:\n",
        "                print('Terminal board', board, 'winner', winner, 'rewards', rewards)\n",
        "            return winner, rewards\n",
        "\n",
        "        # current state key\n",
        "        props = state_propositions_multi(board, game.n)\n",
        "        s_key = propositions_to_key(props)\n",
        "        legal = game.legal_actions(board)\n",
        "\n",
        "        # player chooses action\n",
        "        a = qlearner.choose_action(s_key, legal, player_idx=player_turn-1, coop_threshold=coop_threshold)\n",
        "\n",
        "        # record history for later update (we'll update after seeing next state / later terminal)\n",
        "        history.append((player_turn, s_key, a))\n",
        "\n",
        "        # apply action\n",
        "        board = game.apply_action(board, a, player_turn)\n",
        "\n",
        "        # next player's turn\n",
        "        player_turn = player_turn + 1\n",
        "        if player_turn > game.n:\n",
        "            player_turn = 1\n",
        "\n",
        "        # if training with intermediate updates, optionally do partial updates here (omitted for clarity)\n",
        "\n",
        "# ------------------------------- Evaluation ----------------------------------\n",
        "\n",
        "def evaluate_multi(game, qlearner, episodes=200):\n",
        "    win_counts = {i+1:0 for i in range(game.n)}\n",
        "    draws = 0\n",
        "    old_eps = qlearner.epsilon\n",
        "    qlearner.epsilon = 0.0\n",
        "    for _ in range(episodes):\n",
        "        winner, _ = play_episode_multi(game, qlearner, coop_threshold=0.0, train=False, verbose=False)\n",
        "        if winner is None:\n",
        "            draws += 1\n",
        "        else:\n",
        "            win_counts[winner] += 1\n",
        "    qlearner.epsilon = old_eps\n",
        "    return win_counts, draws\n",
        "\n",
        "# --------------------------------- Main --------------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    random.seed(1)\n",
        "\n",
        "    # Example: 3-player match\n",
        "    n_players = 3\n",
        "    game = MultiAgentGame(n_players=n_players)\n",
        "    q = VectorQLearner(n_players=n_players, alpha=0.6, gamma=0.95, epsilon=0.3)\n",
        "\n",
        "    episodes = 2000\n",
        "    eval_every = 200\n",
        "\n",
        "    print('Training multi-agent vector-Q (Max^N-like decisions)')\n",
        "    print('Players:', n_players)\n",
        "\n",
        "    for ep in range(1, episodes+1):\n",
        "        play_episode_multi(game, q, coop_threshold=0.1, train=True)\n",
        "        if ep % eval_every == 0:\n",
        "            wins, draws = evaluate_multi(game, q, episodes=500)\n",
        "            print(f'Episode {ep}: wins per player {wins}, draws {draws}')\n",
        "\n",
        "    print('\\nFinal evaluation:')\n",
        "    wins, draws = evaluate_multi(game, q, episodes=2000)\n",
        "    print('Wins:', wins, 'Draws:', draws)\n",
        "\n",
        "    # Inspect Q for initial empty board\n",
        "    init_key = propositions_to_key(state_propositions_multi(game.init_board(), n_players))\n",
        "    print('\\nInitial state key:', init_key)\n",
        "    print('Q-vectors for initial state:')\n",
        "    for a in game.legal_actions(game.init_board()):\n",
        "        print('action', a, 'Q=', q.get_Q_vector(init_key, a))\n",
        "\n",
        "    print('\\nExample: show propositions for a sample board')\n",
        "    sample = (1,0,2,0) if n_players>=2 else (1,0,0,0)\n",
        "    print('sample board', sample)\n",
        "    print('propositions', state_propositions_multi(sample, n_players))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6FMiDSAeNLt",
        "outputId": "07b55b27-d7f7-49e8-d04a-cf06082f5fb9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training multi-agent vector-Q (Max^N-like decisions)\n",
            "Players: 3\n",
            "Episode 200: wins per player {1: 500, 2: 0, 3: 0}, draws 0\n",
            "Episode 400: wins per player {1: 500, 2: 0, 3: 0}, draws 0\n",
            "Episode 600: wins per player {1: 500, 2: 0, 3: 0}, draws 0\n",
            "Episode 800: wins per player {1: 500, 2: 0, 3: 0}, draws 0\n",
            "Episode 1000: wins per player {1: 500, 2: 0, 3: 0}, draws 0\n",
            "Episode 1200: wins per player {1: 500, 2: 0, 3: 0}, draws 0\n",
            "Episode 1400: wins per player {1: 500, 2: 0, 3: 0}, draws 0\n",
            "Episode 1600: wins per player {1: 500, 2: 0, 3: 0}, draws 0\n",
            "Episode 1800: wins per player {1: 500, 2: 0, 3: 0}, draws 0\n",
            "Episode 2000: wins per player {1: 500, 2: 0, 3: 0}, draws 0\n",
            "\n",
            "Final evaluation:\n",
            "Wins: {1: 2000, 2: 0, 3: 0} Draws: 0\n",
            "\n",
            "Initial state key: 0000000000000001\n",
            "Q-vectors for initial state:\n",
            "action 0 Q= [1.0, 0.0, 0.0]\n",
            "action 1 Q= [1.0, 0.0, 0.0]\n",
            "action 2 Q= [1.0, 0.0, 0.0]\n",
            "action 3 Q= [1.0, 0.0, 0.0]\n",
            "\n",
            "Example: show propositions for a sample board\n",
            "sample board (1, 0, 2, 0)\n",
            "propositions {'cell_0_is_p1': True, 'cell_0_is_p2': False, 'cell_0_is_p3': False, 'cell_1_is_p1': False, 'cell_1_is_p2': False, 'cell_1_is_p3': False, 'cell_2_is_p1': False, 'cell_2_is_p2': True, 'cell_2_is_p3': False, 'cell_3_is_p1': False, 'cell_3_is_p2': False, 'cell_3_is_p3': False, 'P1_has_threat': True, 'P2_has_threat': True, 'P3_has_threat': False, 'center_any_empty': True}\n"
          ]
        }
      ]
    }
  ]
}